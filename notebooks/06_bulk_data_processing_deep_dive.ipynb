{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5974588b-0809-468d-b6e3-e2df73afaacd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# ensure that all columns are shown and that colum content is not cut\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option('display.width',1000)\n",
    "pd.set_option('display.max_rows', 500) # ensure that all rows are shown"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "333f3746-2750-4526-a4d9-b97ca6a1e167",
   "metadata": {},
   "source": [
    "# Bulk Data Processing Deep Dive\n",
    "The main advantage of this library is that the all data is downloaded down to your computer and therefore makes it easy to analyze all the data at once. \n",
    "\n",
    "For instance, if you want to implement your own screener.\n",
    "\n",
    "Just on the file system, the size of all parquet files is more than 5 GB and growing with every quarter of new data. Since the parquet format is also storage optimized, loading all the data into memory would need significantly more memory than a standard computer/laptop usually provides.\n",
    "\n",
    "Hence it is important to filter the data during the loading process, so that you only load the data into memory that is really needed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b55f7fa2-9c40-4e7e-8f0f-9f3b00f41044",
   "metadata": {},
   "source": [
    "**NOTE**: with the new automation feature (introduced with version 1.8.0), it is possible to add processing steps directly to the normal update process. That means, whenever a new file is detected on SEC.gov, not only downloading, transforming into parquet, and indexing it can be executed, but also customized additional processing steps - like the steps described in this notebook which filters the data and produces files containing all the data. For an example, that directly can be used and shows how it works, have a look at the 08_00_automation_bascis notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "901ec057-a8b4-4be9-bbf5-3495424d879a",
   "metadata": {
    "tags": []
   },
   "source": [
    "<span style=\"color: #FF8C00;\">==========================================================</span>\n",
    "\n",
    "**If you find this tool useful, a sponsorship would be greatly appreciated!**\n",
    "\n",
    "**https://github.com/sponsors/HansjoergW**\n",
    "\n",
    "How to get in touch\n",
    "\n",
    "* Found a bug: https://github.com/HansjoergW/sec-fincancial-statement-data-set/issues\n",
    "* Have a remark: https://github.com/HansjoergW/sec-fincancial-statement-data-set/discussions/categories/general\n",
    "* Have an idea: https://github.com/HansjoergW/sec-fincancial-statement-data-set/discussions/categories/ideas\n",
    "* Have a question: https://github.com/HansjoergW/sec-fincancial-statement-data-set/discussions/categories/q-a\n",
    "* Have something to show: https://github.com/HansjoergW/sec-fincancial-statement-data-set/discussions/categories/show-and-tell\n",
    "\n",
    "<span style=\"color: #FF8C00;\">==========================================================</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ba379f1-5124-4bc4-bd2d-3cfc292ba03f",
   "metadata": {},
   "source": [
    "## Prepare Datasets\n",
    "In the first part of this notebook, we will create four different datasets for all the balance sheet datapoints, the cashflow datapoints, the income statement datapoints, and the cover page datapoints.\n",
    "These datasets will be stored in their own directories, so that they can be easily loaded afterwards. Moreover, we will store the raw version (where the num_df and the pre_df are not joined) and the joined version, where num_df and pre_df are joined. Depending on what you want to do/analyze, you can use either one.\n",
    "\n",
    "**Note:** The code that is explained here is also available in the modul `bulk_loading` which is inside the `secfsdstools.u_usecase` package.\n",
    "\n",
    "This notebook will show two approaches. The first one is loading all the data in parallel, which you can do if you have enough resources in your computer. The second is doing it sequentially, which is slower, but needs less memory. In the end, you will create these datasets once and extend it when new quarterly zip files arrive, or you will recreate them once every quarter. So in the end it doesn't really matter if the process takes 15 minutes or an hour.\n",
    "\n",
    "We will also apply different filters:\n",
    "\n",
    "* only filter 10-K and 10-Q reports during loading\n",
    "* `ReportPeriodRawFilter`: since we are only interested in datapoints that belong to the period of the report\n",
    "* `MainCoregRawFilter`: since we don't want to see datapoints of a subsidiary\n",
    "* `OfficialTagsOnlyRawFilter`: since we want to be able to compare the content and therefore don't want to read tags that or not in the standard sec xbrl definition\n",
    "* `USDOnlyRawFilter`: since we are not interested in money datapoints that are not in USD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1904d65-28df-47e1-9385-9b60098afc6e",
   "metadata": {},
   "source": [
    "### Basics\n",
    "First, we will defines some basic stuff that is used by both approaches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d0d01c1f-8479-4360-aab6-164068424126",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-03 06:20:18,881 [INFO] configmgt  reading configuration from C:\\Users\\hansj\\.secfsdstools.cfg\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from secfsdstools.d_container.databagmodel import RawDataBag, JoinedDataBag\n",
    "from secfsdstools.e_collector.zipcollecting import ZipCollector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6e87306-9242-4523-9b81-0293069ddebe",
   "metadata": {},
   "source": [
    "The following list defines which statements we want to load."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "594116ef-3645-4dcc-9942-201bfe343e07",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "statements_to_load = [\"BS\", \"CF\", \"IS\", \"CP\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "133b535c-9ad4-4b00-9d39-bdf3dad43663",
   "metadata": {},
   "source": [
    "Next, we define a filter function, that defines the whole chain. As mentioned in the 04_collector_deep_dive.ipynt notebook, we have to define the imports inside the function itself, if we want to use it in jupyter together with parallization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6d908348-8e48-461f-a26c-ec0d0f2bbfd0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def postloadfilter(databag: RawDataBag) -> RawDataBag:\n",
    "    from secfsdstools.e_filter.rawfiltering import ReportPeriodRawFilter, MainCoregRawFilter, OfficialTagsOnlyRawFilter, USDOnlyRawFilter\n",
    "\n",
    "    return databag[ReportPeriodRawFilter()][MainCoregRawFilter()][OfficialTagsOnlyRawFilter()][USDOnlyRawFilter()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0ff096f-b7e8-4e4a-a661-2a95e4b922f5",
   "metadata": {},
   "source": [
    "Next is a simple function that takes a raw databag and creates the joined databag. Both, the rawdatabag and the joined databag are then stored in a specific folder.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9395fe8a-06c3-4ca4-9953-fc22fa58b9f4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def save_databag(databag: RawDataBag, financial_statement: str, base_path: str) -> JoinedDataBag:\n",
    "    target_path_raw = os.path.join(base_path, financial_statement, 'raw')\n",
    "    print(f\"store rawdatabag under {target_path_raw}\")\n",
    "    os.makedirs(target_path_raw, exist_ok=True)\n",
    "    databag.save(target_path_raw)\n",
    "    \n",
    "    target_path_joined = os.path.join(base_path, financial_statement, 'joined')\n",
    "    os.makedirs(target_path_joined, exist_ok=True)\n",
    "    print(\"create joined databag\")\n",
    "    joined_databag = databag.join()\n",
    "    \n",
    "    print(f\"store joineddatabag under {target_path_joined}\")\n",
    "    joined_databag.save(target_path_joined)\n",
    "    return joined_databag"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c564ba5e-3334-47a9-94c7-53ba5df7a9c5",
   "metadata": {},
   "source": [
    "### Parallel Data Loading\n",
    "As stated above, we want to load all available 10-K and 10-Q reports. Therefore, we can use the `ZipCollector`which provides an option to load data from all available zip files. \n",
    "\n",
    "Moreover, the implementation of the ziploader uses all your cores in order to load data from your disk into memory. So you don't have to implement the parallization yourself. There are 50+ zip files that have to be loaded, so if you have 4 cores, you will load 4 at one time.\n",
    "\n",
    "Also, the `ZipCollector` provides parameters for filtering the report type (10-K and 10-Q) amd the financial statement type (Balance Sheet, Casch Flow, or Income Statement). These filters are directly applied during loading, since the data is stored in Parquet format. This will already reduce that amount of data that is being loaded into memory significantly.\n",
    "\n",
    "Moreover, it also provides the post_load_filter which we can use to apply the other filters, defined in the postloadfilter function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d18ac72e-0020-4ea5-9cd8-373e2f143fde",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_all_financial_statements_parallel(financial_statement: str) -> RawDataBag:\n",
    "    \"\"\" \n",
    "    financial_statement: either \"BS\", \"CF\", or \"IS\"\n",
    "    \"\"\"\n",
    "\n",
    "    collector: ZipCollector = ZipCollector.get_all_zips(forms_filter=[\"10-K\", \"10-Q\"],\n",
    "                                                        stmt_filter=[financial_statement],\n",
    "                                                        post_load_filter=postloadfilter)\n",
    "    return collector.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db367d8c-bda3-4ab1-a555-0e31c6f6bd52",
   "metadata": {},
   "source": [
    "We loop over the statements that we want to load and collect their datapoints into a specific dataset.\n",
    "\n",
    "This process will take several minutes. On my laptop the execution time was approximately 30 minutes (32GB Ram / 4/8 Cores)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b158ec71-ce9d-49ea-9a23-6f111fcc8811",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-02 12:57:51,516 [INFO] configmgt  reading configuration from C:\\Users\\hansj\\.secfsdstools.cfg\n",
      "2025-02-02 12:57:51,521 [INFO] parallelexecution      items to process: 63\n",
      "2025-02-02 13:03:12,884 [INFO] parallelexecution      commited chunk: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "store rawdatabag under ./set/parallel/BS\\raw\n",
      "create joined databag\n",
      "store joineddatabag under ./set/parallel/BS\\joined\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-02 13:05:41,117 [INFO] configmgt  reading configuration from C:\\Users\\hansj\\.secfsdstools.cfg\n",
      "2025-02-02 13:05:41,125 [INFO] parallelexecution      items to process: 63\n",
      "2025-02-02 13:11:07,236 [INFO] parallelexecution      commited chunk: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "store rawdatabag under ./set/parallel/CF\\raw\n",
      "create joined databag\n",
      "store joineddatabag under ./set/parallel/CF\\joined\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-02 13:13:25,329 [INFO] configmgt  reading configuration from C:\\Users\\hansj\\.secfsdstools.cfg\n",
      "2025-02-02 13:13:25,342 [INFO] parallelexecution      items to process: 63\n",
      "2025-02-02 13:18:26,399 [INFO] parallelexecution      commited chunk: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "store rawdatabag under ./set/parallel/IS\\raw\n",
      "create joined databag\n",
      "store joineddatabag under ./set/parallel/IS\\joined\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-02 13:20:40,704 [INFO] configmgt  reading configuration from C:\\Users\\hansj\\.secfsdstools.cfg\n",
      "2025-02-02 13:20:40,712 [INFO] parallelexecution      items to process: 63\n",
      "2025-02-02 13:25:13,782 [INFO] parallelexecution      commited chunk: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "store rawdatabag under ./set/parallel/CP\\raw\n",
      "create joined databag\n",
      "store joineddatabag under ./set/parallel/CP\\joined\n"
     ]
    }
   ],
   "source": [
    "for statement_to_load in statements_to_load:\n",
    "    rawdatabag = load_all_financial_statements_parallel(financial_statement=statement_to_load)\n",
    "    save_databag(databag=rawdatabag, financial_statement=statement_to_load, base_path=\"./set/parallel/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c1b0d7d-683d-40fd-937c-7242c5eb84ce",
   "metadata": {},
   "source": [
    "After processing, you have the following structure and sizes (with data up to 2024 Q4):\n",
    "<pre>\n",
    "- set/parallel\n",
    "  - BS\n",
    "    - raw     : 1.44 GB\n",
    "    - joined  :  650 MB\n",
    "  - CF\n",
    "    - raw     : 1.43 GB\n",
    "    - joined  :  426 MB\n",
    "  - IS\n",
    "    - raw     : 1.36 GB\n",
    "    - joined  :  483 MB\n",
    "  - CP\n",
    "    - raw     : 1.26 GB\n",
    "    - joined  :   19 MB    \n",
    "</pre>\n",
    "\n",
    "Especially the joined databags have a size that can be easily loaded. Moreover, loading them just takes a few seconds. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "686cf90c-fe3e-4f28-a987-42d5961695fa",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded BS databag:  (19657047, 17)\n",
      "loaded CF databag:  (12883947, 17)\n",
      "loaded IS databag:  (17412439, 17)\n",
      "loaded CP databag:  (76, 17)\n"
     ]
    }
   ],
   "source": [
    "#load BS joined data\n",
    "joinedBS = JoinedDataBag.load(\"./set/parallel/BS/joined\")\n",
    "print(\"loaded BS databag: \", joinedBS.pre_num_df.shape)\n",
    "joinedCF = JoinedDataBag.load(\"./set/parallel/CF/joined\")\n",
    "print(\"loaded CF databag: \", joinedCF.pre_num_df.shape)\n",
    "joinedIS = JoinedDataBag.load(\"./set/parallel/IS/joined\")\n",
    "print(\"loaded IS databag: \", joinedIS.pre_num_df.shape)\n",
    "joinedCP = JoinedDataBag.load(\"./set/parallel/CP/joined\")\n",
    "print(\"loaded CP databag: \", joinedCP.pre_num_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c15bf60-8266-405b-bf17-f1470c48933b",
   "metadata": {},
   "source": [
    "### Serial Data Loading\n",
    "As mentioned above, parallel loading requires some minimal ressources on your laptop/computer. However, using a serial process, you still can create the databags for all balance sheet, cash flow, and income statments. Of course, we need more code and we will also save intermediate results on disk."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b26d7a4d-1e01-4349-b400-fcadfea40fb5",
   "metadata": {},
   "source": [
    "The first thing which we need, is a list of all available zip-files. Actually, we just can copy the code from `ZipCollector.get_all_zips()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "96fdc620-20d7-4d3b-9294-9b1b696241e6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from secfsdstools.a_config.configmgt import ConfigurationManager\n",
    "from secfsdstools.c_index.indexdataaccess import ParquetDBIndexingAccessor\n",
    "\n",
    "def read_all_zip_names() -> List[str]:\n",
    "    configuration = ConfigurationManager.read_config_file()\n",
    "    dbaccessor = ParquetDBIndexingAccessor(db_dir=configuration.db_dir)\n",
    "\n",
    "    # exclude 2009q1.zip, since this is empty and causes an error when it is read with a filter\n",
    "    filenames = [x.fileName for x in dbaccessor.read_all_indexfileprocessing() if not x.fullPath.endswith(\"2009q1.zip\")]\n",
    "    return filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f2f566bf-0422-40dc-8d2f-b2bce2a842c6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-02 14:22:59,558 [INFO] configmgt  reading configuration from C:\\Users\\hansj\\.secfsdstools.cfg\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63\n",
      "['2012q4.zip', '2013q3.zip', '2018q2.zip', '2024q1.zip', '2015q1.zip', '2018q4.zip', '2019q1.zip', '2014q2.zip', '2012q1.zip', '2022q3.zip', '2016q3.zip', '2023q1.zip', '2024q3.zip', '2015q2.zip', '2015q3.zip', '2014q3.zip', '2013q2.zip', '2018q1.zip', '2014q1.zip', '2009q4.zip', '2019q2.zip', '2022q4.zip', '2011q1.zip', '2010q2.zip', '2017q1.zip', '2020q1.zip', '2016q1.zip', '2020q2.zip', '2024q4.zip', '2009q2.zip', '2021q3.zip', '2017q3.zip', '2021q4.zip', '2020q4.zip', '2011q2.zip', '2010q3.zip', '2014q4.zip', '2021q2.zip', '2022q2.zip', '2016q4.zip', '2019q3.zip', '2013q4.zip', '2010q1.zip', '2024q2.zip', '2022q1.zip', '2023q3.zip', '2009q3.zip', '2010q4.zip', '2018q3.zip', '2023q2.zip', '2017q2.zip', '2012q3.zip', '2011q3.zip', '2016q2.zip', '2012q2.zip', '2023q4.zip', '2019q4.zip', '2015q4.zip', '2021q1.zip', '2011q4.zip', '2013q1.zip', '2020q3.zip', '2017q4.zip']\n"
     ]
    }
   ],
   "source": [
    "all_zip_names = read_all_zip_names()\n",
    "print(len(all_zip_names))\n",
    "print(all_zip_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8ab160d-2b3c-439b-8fa1-ec87950138ce",
   "metadata": {},
   "source": [
    "**Prepare the temporary dataset**\n",
    "Next, prepare the data for every single zip-file. So for every zip-file, we collect the datapoints for BS, CF, and IS and apply the aove defined filters. The following functions takes care of that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4d0d98e6-e6c5-418a-b775-320245dfecda",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def build_tmp_set(financial_statement: str, file_names: List[str], target_path: str = \"set/tmp/\"):\n",
    "    \"\"\" This function reads the data in sequence from the provided list of zip file names. It filters according to the \n",
    "        defined financial_statement and stores the data in specific subfolders.\n",
    "        \n",
    "        the folder structure will look like\n",
    "        <target_path>/<file_name>/<financial_statement>/raw\n",
    "        <target_path>/<file_name>/<financial_statement>/joined                                       \n",
    "        \"\"\"\n",
    "    \n",
    "    for file_name in file_names:\n",
    "        collector = ZipCollector.get_zip_by_name(name=file_name,\n",
    "                                 forms_filter=[\"10-K\", \"10-Q\"],\n",
    "                                 stmt_filter=[financial_statement],\n",
    "                                 post_load_filter=postloadfilter)\n",
    "\n",
    "        rawdatabag = collector.collect()\n",
    "\n",
    "        base_path = os.path.join(target_path, file_name)\n",
    "        # saving the raw databag, joining and saving the joined databag\n",
    "        save_databag(databag=rawdatabag, financial_statement=financial_statement, base_path=base_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93d62cba-893c-4f41-9b06-ae99f8b87244",
   "metadata": {
    "tags": []
   },
   "source": [
    "We call the function for every statement (BS, CF, IS, and CP).\n",
    "As a reference, running all four cells took about 15 minutes on my laptop (32GB Ram / 4/8 Cores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56ae6366-922c-4f32-b63c-e0cc1eed53a5",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "build_tmp_set(financial_statement=\"BS\", file_names=all_zip_names, target_path=\"set/tmp/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c01cfcac-5260-41de-a9dd-7ce7f890859f",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "build_tmp_set(financial_statement=\"CF\", file_names=all_zip_names, target_path=\"set/tmp/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24659372-b22b-42a2-b002-b28abdba7231",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "build_tmp_set(financial_statement=\"IS\", file_names=all_zip_names, target_path=\"set/tmp/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62d8b73f-1cb1-4b7a-8bb4-a69061012633",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "build_tmp_set(financial_statement=\"CP\", file_names=all_zip_names, target_path=\"set/tmp/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b78e0305-9380-439d-8b60-bc26d22f4cd8",
   "metadata": {},
   "source": [
    "We know have subfolders for BS, CF, IS, and CP for every quarterly zipfile with the corresponding datapoints."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12720d14-6be6-4eb9-9b11-6c6c79df7773",
   "metadata": {},
   "source": [
    "**Create the rawdatabags**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0b3a9574-8895-4fe3-a9f0-1e48ef717bb0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "\n",
    "def create_rawdatabag(financial_statement: str, target_path: str):\n",
    "    raw_files = glob(f\"./set/tmp/*/{financial_statement}/raw/\", recursive = True)    \n",
    "    raw_databags = [RawDataBag.load(file) for file in raw_files]\n",
    "    raw_databag = RawDataBag.concat(raw_databags)\n",
    "    target_path_raw = os.path.join(target_path, financial_statement, 'raw')\n",
    "    print(f\"store rawdatabag under {target_path_raw}\")\n",
    "    os.makedirs(target_path_raw, exist_ok=True)\n",
    "    raw_databag.save(target_path_raw)      "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0b6ce36-6b06-400b-9200-e45182d4b86c",
   "metadata": {},
   "source": [
    "Next, concatenate the raw datasets together. Again, as a reference it took about 6-7 minutes to create all four rawdatabags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "097a4940-2f44-434c-be19-b48cae0d1016",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "store rawdatabag under set/serial/BS\\raw\n"
     ]
    }
   ],
   "source": [
    "create_rawdatabag(financial_statement=\"BS\", target_path=\"set/serial/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d036f268-d664-4e18-96cb-5f50deb50257",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "store rawdatabag under set/serial/CF\\raw\n"
     ]
    }
   ],
   "source": [
    "create_rawdatabag(financial_statement=\"CF\", target_path=\"set/serial/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "27464709-7766-4ca3-a95f-3d8fd668d866",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "store rawdatabag under set/serial/IS\\raw\n"
     ]
    }
   ],
   "source": [
    "create_rawdatabag(financial_statement=\"IS\", target_path=\"set/serial/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fc0a9310-3047-4c4b-b2ed-a629e9467e7d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "store rawdatabag under set/serial/CP\\raw\n"
     ]
    }
   ],
   "source": [
    "create_rawdatabag(financial_statement=\"CP\", target_path=\"set/serial/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d429003-84a6-4e66-89d3-974d7629a442",
   "metadata": {},
   "source": [
    "**Create the joined databags**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6c513474-b29e-4034-a6a4-afefe0a2070c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "\n",
    "def create_joineddatabag(financial_statement: str, target_path: str):\n",
    "    joined_files = glob(f\"./set/tmp/*/{financial_statement}/joined/\", recursive = True)\n",
    "    joined_databags = [JoinedDataBag.load(file) for file in joined_files]\n",
    "    joined_databag = JoinedDataBag.concat(joined_databags)\n",
    "    target_path_joined = os.path.join(target_path, financial_statement, 'joined')\n",
    "    print(f\"store joineddatabag under {target_path_joined}\")\n",
    "    os.makedirs(target_path_joined, exist_ok=True)\n",
    "    joined_databag.save(target_path_joined)   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffa8b6e8-4b20-4462-a931-d64e09ac5d33",
   "metadata": {},
   "source": [
    "Finally, create the joined databags. To create all four datasets, it took about 90 seconds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6fb50096-a09b-4b26-b2cc-9a5374820d3b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "store joineddatabag under set/serial/BS\\joined\n"
     ]
    }
   ],
   "source": [
    "create_joineddatabag(financial_statement=\"BS\", target_path=\"set/serial/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c8f9c719-17f6-4a85-aadb-4608bf685c3f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "store joineddatabag under set/serial/CF\\joined\n"
     ]
    }
   ],
   "source": [
    "create_joineddatabag(financial_statement=\"CF\", target_path=\"set/serial/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "117528e8-c577-47b0-b5b2-876b44a2af6d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "store joineddatabag under set/serial/IS\\joined\n"
     ]
    }
   ],
   "source": [
    "create_joineddatabag(financial_statement=\"IS\", target_path=\"set/serial/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5088cac7-ebee-4dff-adf8-af33311abe21",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "store joineddatabag under set/serial/CP\\joined\n"
     ]
    }
   ],
   "source": [
    "create_joineddatabag(financial_statement=\"CP\", target_path=\"set/serial/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e3f5417-9726-4584-84a5-b284274681cb",
   "metadata": {
    "tags": []
   },
   "source": [
    "Now we can read back all four prepared joined datasets. This only takes a few seconds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8b7f8a8d-3354-4e6b-8280-53c145338bd2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded BS databag:  (19657047, 17)\n",
      "loaded CF databag:  (12883947, 17)\n",
      "loaded IS databag:  (17412439, 17)\n",
      "loaded CP databag:  (76, 17)\n"
     ]
    }
   ],
   "source": [
    "#load BS joined data\n",
    "joinedBS = JoinedDataBag.load(\"./set/serial/BS/joined\")\n",
    "print(\"loaded BS databag: \", joinedBS.pre_num_df.shape)\n",
    "joinedCF = JoinedDataBag.load(\"./set/serial/CF/joined\")\n",
    "print(\"loaded CF databag: \", joinedCF.pre_num_df.shape)\n",
    "joinedIS = JoinedDataBag.load(\"./set/serial/IS/joined\")\n",
    "print(\"loaded IS databag: \", joinedIS.pre_num_df.shape)\n",
    "joinedCP = JoinedDataBag.load(\"./set/serial/CP/joined\")\n",
    "print(\"loaded CP databag: \", joinedCP.pre_num_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c47bbfa-69e6-468d-8b8e-17cf1d8e40bf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (base)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
