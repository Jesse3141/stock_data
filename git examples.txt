# example of using sec api:
#from: https://github.com/AdamGetbags/secAPI/blob/main/secFilingScraper.py
# -*- coding: utf-8 -*-
"""

SEC Filing Scraper
@author: AdamGetbags

"""

# import modules
import requests
import pandas as pd

# create request header
headers = {'User-Agent': "email@address.com"}

# get all companies data
companyTickers = requests.get(
    "https://www.sec.gov/files/company_tickers.json",
    headers=headers
    )

# review response / keys
print(companyTickers.json().keys())

# format response to dictionary and get first key/value
firstEntry = companyTickers.json()['0']

# parse CIK // without leading zeros
directCik = companyTickers.json()['0']['cik_str']

# dictionary to dataframe
companyData = pd.DataFrame.from_dict(companyTickers.json(),
                                     orient='index')

# add leading zeros to CIK
companyData['cik_str'] = companyData['cik_str'].astype(
                           str).str.zfill(10)

# review data
print(companyData[:1])

cik = companyData[0:1].cik_str[0]

# get company specific filing metadata
filingMetadata = requests.get(
    f'https://data.sec.gov/submissions/CIK{cik}.json',
    headers=headers
    )

# review json 
print(filingMetadata.json().keys())
filingMetadata.json()['filings']
filingMetadata.json()['filings'].keys()
filingMetadata.json()['filings']['recent']
filingMetadata.json()['filings']['recent'].keys()

# dictionary to dataframe
allForms = pd.DataFrame.from_dict(
             filingMetadata.json()['filings']['recent']
             )

# review columns
allForms.columns
allForms[['accessionNumber', 'reportDate', 'form']].head(50)

# 10-Q metadata
allForms.iloc[11]

# get company facts data
companyFacts = requests.get(
    f'https://data.sec.gov/api/xbrl/companyfacts/CIK{cik}.json',
    headers=headers
    )

#review data
companyFacts.json().keys()
companyFacts.json()['facts']
companyFacts.json()['facts'].keys()

# filing metadata
companyFacts.json()['facts']['dei'][
    'EntityCommonStockSharesOutstanding']
companyFacts.json()['facts']['dei'][
    'EntityCommonStockSharesOutstanding'].keys()
companyFacts.json()['facts']['dei'][
    'EntityCommonStockSharesOutstanding']['units']
companyFacts.json()['facts']['dei'][
    'EntityCommonStockSharesOutstanding']['units']['shares']
companyFacts.json()['facts']['dei'][
    'EntityCommonStockSharesOutstanding']['units']['shares'][0]

# concept data // financial statement line items
companyFacts.json()['facts']['us-gaap']
companyFacts.json()['facts']['us-gaap'].keys()

# different amounts of data available per concept
companyFacts.json()['facts']['us-gaap']['AccountsPayable']
companyFacts.json()['facts']['us-gaap']['Revenues']
companyFacts.json()['facts']['us-gaap']['Assets']

# get company concept data
companyConcept = requests.get(
    (
    f'https://data.sec.gov/api/xbrl/companyconcept/CIK{cik}'
     f'/us-gaap/Assets.json'
    ),
    headers=headers
    )

# review data
companyConcept.json().keys()
companyConcept.json()['units']
companyConcept.json()['units'].keys()
companyConcept.json()['units']['USD']
companyConcept.json()['units']['USD'][0]

# parse assets from single filing
companyConcept.json()['units']['USD'][0]['val']

# get all filings data 
assetsData = pd.DataFrame.from_dict((
               companyConcept.json()['units']['USD']))

# review data
assetsData.columns
assetsData.form

# get assets from 10Q forms and reset index
assets10Q = assetsData[assetsData.form == '10-Q']
assets10Q = assets10Q.reset_index(drop=True)

# plot 
assets10Q.plot(x='end', y='val')


# end example


#open xbrl 
"""
Convert JSON files from Edgar into CSV.
Using the companyfacts.zip (all XBRL accounting data in SEC filings),
parse to find the accounting terms we want.
"""
import os
import json
import datetime 


class AccountingParser() :

    _MAPPED_SUFFIX = "_map_from"

    def __init__(self, workingdir : str ) :
        self._workingdir = workingdir

        # import field mappings, assume in JSON file in same dir as code
        jsonfile = os.path.join( os.path.dirname(__file__), 'concept_handling.json') 
        with open(jsonfile, "r") as f:
            self._concept_handling = json.load(f)

        self._output_params = list()
        for m in self._concept_handling.keys():
            self._output_params.append( m )
            self._output_params.append( f"{m}{AccountingParser._MAPPED_SUFFIX}")

        self._filing_id_params =  [     'FY_year'
                                   ,    'FY_quarter'
                                   ,    'form'      
                                   ,    'FY_date'   
                                   ,    'CY_date'   
                                   ,    'CY_filing_date' 
                                  ]


    def parse_file( self, filename : str ) -> dict :
        """
        Parse Edgar JSON file w/ accounting data
        Parameters:
            filename : the JSON file to read
        Returns :
            dict with CIK and other corp parameters
            and SEC filings in ['filings']
        """
        filename = os.path.join( self._workingdir, filename )
        with open(filename, "r") as f:
            data = json.load(f)

        entries = data['facts'].get('us-gaap')
        if( entries == None ) :
            entries = data['facts'].get('ifrs-full')
            if( entries == None ) :
                print( '[ERROR] CIK: {cik} does not have US-GAAP or IFRS-FULL data.')
                return None

        # STEP 1: Transform JSON to dict w/ filings
        filings = dict()

        cik         = data['cik']
        entityName  = data['entityName']

        # Add from facts.dei 
        entry = data['facts'].get('dei')
        if( entry != None ) :
            entry = data['facts']['dei'].get('EntityPublicFloat')
            if( entry != None ) :
                entries['EntityPublicFloat'] = entry

        for accounting_parameter in entries:
            
            entry = entries.get(accounting_parameter)

            # TBD: Check for non-USD currency
            #  Also, some units are 'shares'; we ignore those
            if( entry['units'].get('USD') == None ) :
                continue
            units = entry['units']['USD']
            for item in units :
                form = item['form']
                # restrict to 10-X forms --> 10-Q, 10-K, 10-K/A
                if( form[0:2] != '10' ) :
                    continue

                fy_year = item['fy']
                # Remove bad data (there are some year=0 and other nonsense)
                if( (fy_year == None) or (fy_year == '') ) :
                    continue
                else :
                    fy_year = int(fy_year)
                if( (fy_year < 2009) or (fy_year > 2025) ) :
                    continue 
                
                fy_quarter = item['fp']
                if(   (fy_quarter == 'FY') or (fy_quarter == 'CY') 
                   or (fy_quarter == None) or (fy_quarter == ''  ) ) :
                    fy_quarter = 0
                else :
                    fy_quarter = int(fy_quarter[1:2])

                # Assume all entries have same filing date
                filing_date = item['filed']

                # Calendar Year framing may not exist, if so impute 
                cy_frame = item.get('frame')
                if( (cy_frame == None) or (cy_frame == '') ) :
                    # impute from 'end' (end of period)
                    # back up date a bit (just in case reporting date goes a little past end-of-quarter)
                    d        = datetime.datetime.strptime(item['end'], '%Y-%m-%d') - datetime.timedelta(days=8)
                    cy_frame = f"{d.year}Q{(d.month - 1) // 3 + 1}"
                else :  # format is CYyyyyQqI
                    cy_frame = cy_frame[2:8]
                    if( (len(cy_frame)==5) or (len(cy_frame)==4) ) :
                        # frame is missing Q; try to fix
                        cy_year = int(cy_frame[0:5])
                        d = datetime.datetime.strptime(item['end'], '%Y-%m-%d') - datetime.timedelta(days=8)
                        if( cy_year == d.year ):
                            cy_frame = f"{d.year}Q{(d.month - 1) // 3 + 1}"
                        else :
                            if( item.get('start') != None ) :
                                d = datetime.datetime.strptime(item['start'], '%Y-%m-%d') 
                                if( cy_year == d.year ) :
                                    cy_frame = f"{d.year}Q4"
                            if( len(cy_frame) < 6 ) :
                                print (f"[Warning] CIK:{cik} Form:{form} Year:{fy_year} Quarter:{fy_quarter} Tag:{accounting_parameter} has CY_frame mismatch to end-of-period.")

                index = (fy_year,fy_quarter,form)
                value = item['val']
                
                filing = filings.get(index)
                if( filing == None ) :
                    # Create new filing record
                    new_q = fy_quarter
                    if( new_q == 0 ) :
                        new_q = 4       # Put yearly results in Q4
                    filings[index] = {  
                                        'FY_year'            : fy_year
                                     ,  'FY_quarter'         : fy_quarter 
                                     ,  'form'               : form 
                                     ,  'FY_date'            : f"{fy_year}Q{new_q}"
                                     ,  'CY_date'            : cy_frame
                                     ,  'CY_filing_date'     : filing_date
                                     }
                    filing = filings[index]

                # Add the parameter to the filing record
                filing[accounting_parameter] = value
        # end for 

        # STEP 2: 
        # Go through form by form and calculate the accounting concepts we want
        
        VALUE_NaN           = "NaN"
        VALUE_NotAvailable  = "NA"

        # Mapping between accounting concepts and fields
        #   Code will try to go down the list for each concept until it finds one
        concept_handling = self._concept_handling

        for ndx in filings.keys() :
            row = filings[ndx]

            for entry_name in concept_handling.keys() :
                row[entry_name] = None
                for field_name in concept_handling[entry_name]['map'] :
                    if( row.get(field_name) != None ) :
                        row[entry_name] = row[field_name]
                        row[f"{entry_name}{AccountingParser._MAPPED_SUFFIX}"] = field_name
                        break
                if( row[entry_name] == None ) :
                    row[entry_name] = concept_handling[entry_name]['on_missing']
                    row[f"{entry_name}{AccountingParser._MAPPED_SUFFIX}"] = "Not_Found"
                    if( row[entry_name] == VALUE_NaN ) :
                        print (f"[Warning] CIK:{cik} Form:{row['form']} Year:{row['FY_year']} Quarter:{row['FY_quarter']} is missing {entry_name}")

            # Public float
            entry_name = '_EntityPublicFloat'
            if( row.get('EntityPublicFloat') != None ) :
                row[entry_name] = row['EntityPublicFloat']
            elif( row['form'].startswith("10-Q") ) :
                # 10-Q's don't have this value
                row[entry_name] = VALUE_NotAvailable
            else :
                row[entry_name] = VALUE_NaN
                print (f"[Warning] CIK:{cik} Form:{row['form']} Year:{row['FY_year']} is missing EntityPublicFloat")
       
        # end loop on accounting concepts

        # STEP 3: Impute certain quarterly data
        for ndx in filings.keys() :
            row = filings[ndx]

            # We modify the 10-K rows (these are the FY 4Q)
            if( not row['form'].startswith('10-K') ) :
                continue 
        
            fy_year = row[ 'FY_year' ]
            for series_name in [ '_Profits' ] :
                year_value = row[ series_name ]
                if( year_value == VALUE_NaN ) :
                    continue
                
                # Keep old values as _annual
                row[ series_name + '_annual' ] = row[series_name]

                # impute quarterly value on this FY 10-K
                #  by subtracting the FY 10-Q values from the year's
                for q in range(1,4) :
                    q_row = filings.get((fy_year, q, '10-Q'))
                    if( q_row == None ) :
                        # try another form
                        q_row = filings.get((fy_year, q, '10-Q/A'))
                    if( (q_row == None) or ( q_row[series_name] == VALUE_NaN ) ) :
                        print( f"[Warning] Missing 10-Q form for CIK:{cik} FY_year:{fy_year} for quarter: {q}. Skipping year's {series_name}")
                        year_value = VALUE_NaN
                        break
                    else :
                        year_value -= q_row[series_name]

                # Overwrite w/ imputed value
                row[series_name] = year_value
        # end loop on imputation
                
        all = dict()
        all['CIK']          = cik
        all['entityName']   = entityName
        all['filings']      = filings
        return all
    # end parse_file


    def _csv_header(self) -> str :
        """
        CSV header fields
        """
        column_names = self._filing_id_params + self._output_params
        s = '"CIK","entityName"'
        for param in column_names:
            s += f',"{param}"'
        return s
    
    def _csv_filing(self, cik : int, entityName : str, filing_fields : dict ) -> str :
        """
        Takes output of parse_file() and returns CSV row
        """
        cols = self._filing_id_params + self._output_params
        s    = f'{cik},"{entityName}"'
        for c in cols :
            param = filing_fields[c]
            if( type(param) == str ) :
                s += f',"{param}"'
            else :
                s += f',{param}'
        return s


    def parse_files_to_csv( self, ciks : list, outputfile : str ) -> None :
        """
        Read files of CIKs in list, call .parse_file() on each one, and produce CSV
        Parameters: 
            ciks : list of CIKS to read. Filename in format 'CIK{cik:010}.json'
            outputfile : path (folder and filename) to write CSV to
        """
        progress_update_at = max(1, int(len(ciks)/50))
        cik_progress       = 0

        column_names = self._filing_id_params + self._output_params
        with open(outputfile, "w") as f:
            print(self._csv_header(), file=f)  #header

            num_rows = 0
            for cik in ciks :
                x = self.parse_file( f"CIK{cik:010}.json" )
                for ndx in x['filings'].keys() :
                    filing_fields = x['filings'][ndx]
                    s = self._csv_filing( cik, x['entityName'], filing_fields)                
                    print(s, file=f)
                    num_rows += 1
                cik_progress += 1
                # Show progress
                if( cik_progress % progress_update_at == 0 ) :
                    print( f"Parsed {cik_progress} out of {len(ciks)}")

        print( f"Finished parsing. Total filings: {num_rows} ")



    def get_CIK_list( self, min_size : int = 0 ) -> list :
        """
        Read <working_dir>, find all CIK files, extract CIKs for big-enough files
        Parameters: 
            min_size       : minimum size (in KB) of files
        """
        min_size   = min_size * 1024

        # Get list of all files only in the given directory
        func       = lambda x : os.path.isfile(os.path.join(self._workingdir, x))
        files_list = filter(func, os.listdir(self._workingdir))

        cik_list = list()
        for f in files_list :
            sz = os.stat(os.path.join(self._workingdir, f)).st_size
            if( sz > min_size ) :
                cik = int(f[3:13])
                cik_list.append(cik)
        
        return cik_list
    

# end AccountingParser

"""
Downloader of SEC Edgar data
"""
import os
import requests
import json
import pandas
import time 


class Downloader( ) :

  def get_companyfacts( self, cik : int, workingdir : str ) -> str :
    """
    Downloads a single companyfacts JSON file to the working dir
    Parameters: 
      cik         : CIK id for company
      workingdir  : path to folder to which do download the file
    Returns:
      name of JSON file downloaded
    """
    filename  = f"CIK{cik:010}.json" 
    url       = f"https://data.sec.gov/api/xbrl/companyfacts/{filename}"
    content   = self._load_url(url)

    if( not os.path.exists(workingdir) ) :
      os.makedirs(workingdir, exist_ok=True)

    outfile   = os.path.join(workingdir, filename)
    with open(outfile, "wb") as f:
        f.write(content)
    return filename


  def get_urls( self, ciks, forms, from_date, to_date ) -> pandas.DataFrame :
    """
    Returns DataFrame with URLs to Edgar data based on query criteria
    Parameters:
      ciks  : list of CIK numbers of firms
      forms : list of SEC forms 
      from_date : start of period in 'yyyy-mm-dd' format
      to_date   : end of period  in 'yyyy-mm-dd' format
    Returns :
      pandas.DataFrame with columns CIK, filing_date, form, URL
    """
    sec_data = dict()
    
    for cik in ciks :
      cik = int(cik)  
      url = f"https://data.sec.gov/submissions/CIK{cik:010}.json"

      data = json.loads(self._load_url(url))

      # Convert JSON to dataframe for easier use
      recents = pandas.DataFrame(data['filings']['recent'])
      recents['reportDate'] = pandas.to_datetime(recents['reportDate'])
      recents['filingDate'] = pandas.to_datetime(recents['filingDate'])

      for form in forms :

        df = recents[(recents['form'] == form) &
            (recents['filingDate'] >= from_date) &
            (recents['filingDate'] <= to_date  ) ]

        for i in range(len(df)) :
          row = df.iloc[i]
          accessionNumber = row['accessionNumber'].replace("-", "")
          filename        = f"{row['accessionNumber']}.txt"
          url = f"https://www.sec.gov/Archives/edgar/data/{cik:010}/{accessionNumber}/{filename}"

          sec_data[ (cik, form, row['filingDate']) ] = { 'URL': url, 'filename' : filename }

    # Convert dict to DataFrame
    df = pandas.DataFrame.from_dict( sec_data, orient='index' )
    xf = pandas.DataFrame.from_records( df.index, columns=['CIK', 'form', 'filing_date'])
    df.reset_index(inplace=True)
    df = df.join(xf)
    return df[['CIK', 'form', 'filing_date', 'URL', 'filename']]
    # end get_urls()


  def get_files( self, ciks, forms, from_date, to_date, workingdir : str ) -> None :
    """
    Downloads and saves Edgar data to working dir.
    Parameters:
      ciks  : list of CIK numbers of firms
      forms : list of SEC forms to download
      from_date : start of period to download in 'yyyy-mm-dd' format
      to_date   : end of period to download   in 'yyyy-mm-dd' format
      workingdir : folder to download files to
    """
    os.makedirs( workingdir, exist_ok=True)

    num_files = 0 

    df = self.get_urls( ciks, forms, from_date, to_date )
    for ndx in range(0,len(df)) :
      row = df.iloc[ndx]

      filename = row['filename']
      url      = row['URL']
      cik      = row['CIK']
      form     = row['form']
      content = self._load_url(url)

      with open(os.path.join(workingdir, filename), "wb") as f:
        f.write(content)
      
      num_files += 1
      print(f"Downloaded CIK:{cik} Form:{form} to {filename}")
    print( f"Download request finished. {num_files} files fetched.")
  # end get_files()

        
  def _load_url(self, url : str) :
    """
    Helper function to download URLs from EDGAR
    """
    # Wait a bit as per SEC Edgar rate use requirements
    time.sleep(0.11)
  
    response = requests.get(url,headers={"User-Agent": "Mozilla/5.0"})
    if response.status_code != 200:
      raise Exception(f"Failed to fetch data from URL: {url}")
    return response.content
  # end _load_url()
           
# end Downloader()
{ 
    "_Liabilities"      : 
    {
        "map"           : ["Liabilities", "LiabilitiesAndStockholdersEquity", "LiabilitiesCurrent", "LiabilitiesNoncurrent", "OtherLiabilities", "OtherLiabilitiesCurrent", "OtherLiabilitiesNoncurrent"]
    ,   "on_missing"    : "NaN"
    }
,   "_Assets"           :
    {
        "map"           : ["Assets", "AssetsCurrent", "AssetsNoncurrent", "OtherAssetsCurrent", "OtherAssetsNoncurrent", "OtherAssets", "IntangibleAssetsGrossExcludingGoodwill", "IntangibleAssetsNetExcludingGoodwill"]
    ,   "on_missing"    : "NaN"
    }
,   "_Profits"          :
    {
        "map"           : ["GrossProfit", "ProfitLoss", "OperatingIncomeLoss", "NetIncomeLoss", "OperatingIncomeLoss", "ComprehensiveIncomeNetOfTax", "NetIncomeLossIncludingPortionAttributableToNonredeemableNoncontrollingInterest"]
    ,   "on_missing"    : "NaN"
    }
,   "_Revenues"             : 
    {
        "map"           : ["RevenueFromContractWithCustomerExcludingAssessedTax", "Revenues", "SalesRevenueNet", "SalesRevenueServicesGross", "SalesRevenueGoodsNet", "SalesRevenueServicesNet", "OtherSalesRevenueNet", "RevenueFromContractWithCustomerIncludingAssessedTax"]
    ,   "on_missing"    : "NaN"
    }
,   "_Costs"                : 
    {
        "map"           : ["CostOfGoodsSold", "CostOfRevenue", "CostsAndExpenses", "CostOfGoodsAndServicesSold", "CostOfServices"]
    ,   "on_missing"    : "NaN"
    }
,   "_Dividends"            : 
    {
        "map"           : ["DividendsCommonStock", "DividendsCommonStockCash", "Dividends", "DividendsCash", "CashDividends", "PaymentsOfDividends", "PaymentsOfDividendsCommonStock"]
    ,   "on_missing"    : 0
    }
}

"""
Main access point for OpenXBRL api
"""
from openxbrl import AccountingParser, Downloader
import os

class OpenXBRL() :

    def __init__(self, base_dir : str ) :
        """
        Initialization requires pointing to a base directory.
        This folder will hold filings data.
        """
        self._base_dir = base_dir

    
    def get_filing_accounting( self, cik : int, fiscal_year : int, fiscal_quarter : int ) -> dict :
        """
        Return dict with accounting fields parsed from a corporate filing.
        Parameters: 
            cik             : CIK identifier of the company
            fiscal_year     : Fiscal year of the filing
            fiscal_quarter  : Fiscal quarter of the filing. 
                            NOTE: Use fiscal_quarter = 0 to get the annual (10-K) filing
        """
        workingdir = os.path.join(self._base_dir, 'companyfacts/')
        downloader = Downloader()
        parser     = AccountingParser( workingdir )
        filename   = downloader.get_companyfacts(cik=cik, workingdir=workingdir)
        
        result  = parser.parse_file( filename )
        filings = result['filings']

        # Look for filing. Rem index=(fy_year,fy_quarter,form)
        if( fiscal_quarter == 0 ) :
            form = '10-K'
        else :
            form = '10-Q'
        filing = filings.get((fiscal_year, fiscal_quarter, form))
        if( filing == None ) :
            form += '/A'    # look for amended
            filing = filings.get((fiscal_year, fiscal_quarter, form))
        
        if( filing != None ) :
            filing['CIK'] = cik
            filing['entityName'] = result['entityName']

        return filing

# end open xbrl project



#delta api edgar scrapper

from datetime import datetime, timedelta
import pandas as pd
import json
from .connector import APIconnector


def fillTo10D(cell):
    while len(cell) != 10:
        cell = '0' + cell
    return cell


def generate_CIK_TICKER(filename = 'ticker-SEC.csv'):
    jsonSEClist = APIconnector('https://www.sec.gov/files/company_tickers.json').get_request()
    recentFilings = pd.DataFrame.from_dict(jsonSEClist.json()).T
    recentFilings['cik10D'] = recentFilings['cik_str'].astype(str).apply(lambda x: fillTo10D(x))
    #recentFilings.to_csv(filename, index=False)
    
    return recentFilings

def rename_cols_in_df(df):
    renameDic = {}
    for column in df.columns:
        newName = column.strip().replace(' ', '_').replace('-', '_')
        renameDic[column] = newName
        
    df.rename(columns=renameDic, inplace=True)
    return df

def monthWindow(df):    
    daysDiff = df['diffDate']
    months = 0
    while daysDiff >= 30:
        daysDiff -=30
        months +=1
        
    return months


def unpactUnitsJson(index,jsonDataframe):
    key = list(jsonDataframe['units'].iloc[index].keys())[0]
    valuesInTable = pd.DataFrame(jsonDataframe['units'].iloc[index][key])
        
    if 'start' in valuesInTable.columns:
        valuesInTable['startFormat'] = pd.to_datetime(valuesInTable['start'])
    
    if 'end' in valuesInTable.columns:
        valuesInTable['endFormat'] = pd.to_datetime(valuesInTable['end'])
        
    if 'end' in valuesInTable.columns and 'start' in valuesInTable.columns:
        valuesInTable['diffDate'] =  (valuesInTable['endFormat'] - valuesInTable['startFormat']).dt.days
        valuesInTable['monthWindow'] = valuesInTable.apply(monthWindow, axis=1)
        
    #valuesInTable['time'] = valuesInTable['endFormat'].dt.year.astype(str).str[2:] + valuesInTable['endFormat'].dt.month.map("{:02}".format)

    
    return valuesInTable


def unpackSECjson(cik):
    scr = APIconnector(f'https://data.sec.gov/api/xbrl/companyfacts/{cik}.json')
    
    jsonRequest = scr.get_request().json()
    r = jsonRequest['facts']['us-gaap']
    r = json.dumps(r)
    jsonDataframe = pd.read_json(r).T
    jsonDataframe.reset_index(inplace=True, names='finType')

    valuesDF = pd.DataFrame()
    for index, row in jsonDataframe.iterrows():
        unit = unpactUnitsJson(index, jsonDataframe)
        unit['finType'] = jsonDataframe.iloc[index]['finType']
        valuesDF = pd.concat([unit,valuesDF])
    
    mergedDF = jsonDataframe.merge(valuesDF, on='finType')
    mergedDF = mergedDF[['finType', 'val', 'accn', 'fy', #'label', 'description'
       'fp', 'form', 'filed', 'frame', 'endFormat', 'time', 'startFormat', 'monthWindow']]
    
    return mergedDF


def get_spy500_formWiki():
    table = pd.read_html('https://en.wikipedia.org/wiki/List_of_S%26P_500_companies')[0]
    table['CIK'] = table['CIK'].astype(str).apply(lambda x: fillTo10D(x))
    table = rename_cols_in_df(table)
    return table


def get_rangeOfDates(yearOffset):
    now = datetime.now()
    dateto = int(now.timestamp())
    dt = timedelta(days=366*yearOffset)
    datefrom = int((now - dt).timestamp())
    
    return dateto, datefrom

def get_StockPrices(ticker, startSelect = None, endSelect = None, interval = '1d'):
    if startSelect:
        end = int(datetime.strptime(endSelect,"%Y-%m-%d").timestamp())
        start = int(datetime.strptime(startSelect,"%Y-%m-%d").timestamp())
    else:
        end, start = get_rangeOfDates(24)
    
    stockPrice = APIconnector(f'https://query2.finance.yahoo.com/v8/finance/chart/{ticker}?period1={start}&period2={end}&interval={interval}')
    
    adjClose = stockPrice.get_request().json()['chart']['result'][0]['indicators']['adjclose'][0]['adjclose']
    timestap = stockPrice.get_request().json()['chart']['result'][0]['timestamp']
    priceDF = pd.DataFrame({'adjClose':adjClose, 'time':timestap})
    priceDF['date'] =  pd.to_datetime(priceDF['time'].apply(lambda x: datetime.fromtimestamp(x))).dt.strftime('%Y-%m-%d')
    priceDF['ticker'] = ticker
    priceDF.drop('time', inplace=True, axis=1)
    
    priceDF["Quarter"] = "Q" + pd.to_datetime(priceDF['date']).dt.quarter.astype(str) + "-" \
                        + pd.to_datetime(priceDF['date']).dt.year.astype(str)
    groupByQ = priceDF[["Quarter", "adjClose"]].groupby("Quarter").mean().reset_index().rename(columns={'adjClose': 'quarter_meanADJclose'})
    mergeMean = pd.merge(priceDF, groupByQ, on = 'Quarter')
    
    mergeMean["Month"] = "Month" + pd.to_datetime(mergeMean['date']).dt.month.astype(str) + "-" \
                        + pd.to_datetime(mergeMean['date']).dt.year.astype(str)
    groupByQ = mergeMean[["Month", "adjClose"]].groupby("Month").mean().reset_index().rename(columns={'adjClose': 'month_meanADJclose'})
    mergeMean = pd.merge(mergeMean, groupByQ, on = 'Month')

    mergeMean = mergeMean.apply(lambda x: addDateKey(x, 'date', 'date'), axis=1)
    mergeMean.drop(['Quarter', 'date', 'Month'], axis=1, inplace=True)
    
    return mergeMean

def cleaned_companyfacts(jsonDataframe):
    valuesDF = pd.DataFrame()
    for index, row in jsonDataframe.iterrows():
        unit = unpactUnitsJson(index, jsonDataframe)
        unit['finType'] = jsonDataframe.iloc[index]['finType']
        valuesDF = pd.concat([unit,valuesDF])
        
        
    mergedDF = jsonDataframe.merge(valuesDF, on='finType')
    mergedDF.drop(['units'], axis = 1, inplace=True)
    #mergedDF.fillna("null", inplace = True)
    
    return mergedDF

def get_companyfacts(cik, ticker):

    if "CIK" not in cik:
        cik = "CIK" + cik
    
    baseUrl = f'https://data.sec.gov/api/xbrl/companyfacts/{cik}.json'
    scr = APIconnector(baseUrl)
    jsonRequest = scr.get_request().json()
    r = jsonRequest['facts']['us-gaap']
    r = json.dumps(r)
    jsonDataframe = pd.read_json(r).T

    jsonDataframe.reset_index(inplace=True, names='finType')
    
    mergedDF = cleaned_companyfacts(jsonDataframe)
    
    mergedDF = mergedDF.apply(lambda x: addDateKey(x, 'end', 'start', ticker), axis=1)
    mergedDF['ticker'] = ticker
    desired_order = [
        'finType', 'label', 'description', 'end', 'val', 'accn', 'fy', 'fp', 
        'form', 'filed', 'frame', 'endFormat', 'start', 'startFormat', 
        'diffDate', 'monthWindow', 'yearMonthDay', 'ticker']

    mergedDF = mergedDF[desired_order]
    mergedDF.drop(["fy", "endFormat", "startFormat", "diffDate", 'frame'], inplace=True,axis=1)
    
    return mergedDF

def get_CIK_by_Ticker(ticker, filename = 'ticker-SEC.csv', fill0 = True):
    
    recentFilings = pd.read_csv(filename)
    selectedTicker = recentFilings[recentFilings['ticker'] == ticker]

    if not fill0:
        return str(selectedTicker.cik_str.values[0])
    
    return fillTo10D(str(selectedTicker.cik_str.values[0]))


def addDateKey(row, col1, col2, ticker = None):
    pd.options.mode.copy_on_write = True
    try:
        row['yearMonthDay'] = pd.to_datetime(row[col1]).strftime("%Y%m%d")
    except ValueError:
        row['yearMonthDay'] = pd.to_datetime(row[col2]).strftime("%Y%m%d")
    except:
        row['yearMonthDay'] = ''
    
    if ticker:
        row['ticker'] = ticker
    
    return row


def get_SEC_filings(cik, ticker):
    pd.options.mode.copy_on_write = True
    if "CIK" not in cik:
        cik = "CIK" + cik

    clearCik = cik.replace('CIK', '')
    reqURL = F'https://data.sec.gov/submissions/CIK{clearCik}.json'
    scr = APIconnector(reqURL)

    scr.URL = reqURL
    res = scr.get_request()
    JSONresponse = res.json()

    #https://data.sec.gov/api/xbrl/companyfacts/CIK0000320193.json
    filings = pd.DataFrame.from_dict(JSONresponse['filings']['recent'])

    filings['accessionNumberCLEAN'] = filings['accessionNumber'].apply(lambda x: x.replace('-', ''))
    filings['fileURL'] = 'https://www.sec.gov/Archives/edgar/data/' + clearCik + "/" + \
        filings['accessionNumberCLEAN'] + "/"+ filings['primaryDocument']
    
    filings.drop('accessionNumberCLEAN', inplace = True, axis = 1)
    filings = filings.apply(lambda x: addDateKey(x, 'reportDate', 'filingDate', ticker), axis=1)
    #filings['ticker'] = ticker
    desired_order = [
        'accessionNumber', 'filingDate', 'reportDate', 'acceptanceDateTime',
        'act', 'form', 'fileNumber', 'filmNumber', 'items', 'size',
        'isXBRL', 'isInlineXBRL', 'primaryDocument', 'primaryDocDescription',
        'fileURL', 'yearMonthDay', 'ticker']
    filings = filings[desired_order]
    
    
    return filings

if __name__=="main":
    pass

# end delta api edgar scrapper



#quants

import pandas as pd
import requests
headers = {"User-Agent": "ian.ye.fu@gmail.com"} 

data_folder_download = './datasets/download/'
data_folder_generate = './datasets/generate/'

def cik_matching_ticker(ticker, headers=headers):
    ticker = ticker.upper().replace(".", "-")
    ticker_json = requests.get(
        "https://www.sec.gov/files/company_tickers.json", headers=headers
    ).json()

    for company in ticker_json.values():
        if company["ticker"] == ticker:
            cik = str(company["cik_str"]).zfill(10)
            return cik
    raise ValueError(f"Ticker {ticker} not found in SEC database")


def get_fiscal_YE(sp500_cik_list):
    """
    Get_fiscal_year_end for S&P companies by passing the cik list.
    """
    fiscal_YE = []
    
    for cik in sp500_cik_list:
        url = f"https://data.sec.gov/submissions/CIK{cik}.json"
        
        try:
            # Make the API request
            company_json = requests.get(url, headers=headers).json()
            
            # Check if 'fiscalYearEnd' exists in the JSON response
            fiscal_year_end = company_json.get('fiscalYearEnd', 'N/A')
            
            # Append the result
            fiscal_YE.append(fiscal_year_end)
            
        except Exception as e:
            # In case of error, append 'N/A' or handle accordingly
            print(f"Error fetching data for CIK {cik}: {e}")
            fiscal_YE.append('N/A')
            
    return fiscal_YE



def get_facts(ticker, sp500_df, headers):
    """
    Load company_facts data from SEC API
    """
    try:
        cik = f'CIK{sp500_df.loc[ticker, "CIK"]}'
        url = f"https://data.sec.gov/api/xbrl/companyfacts/{cik}.json"
        response = requests.get(url, headers=headers)
        
        # Check if the response status code is 200 (success)
        if response.status_code == 200:
            company_facts = response.json()
            return company_facts
        else:
            print(f"Error: {response.status_code} for ticker {ticker}. Skipping.")
            return None
    except requests.exceptions.RequestException as e:
        print(f"An error occurred: {e}. Skipping ticker {ticker}.")
        return None



def facts_DF(ticker, sp500_df, headers=headers):
    """
    Convert the company_facts dict to dataframes
    """
    facts = get_facts(ticker, sp500_df, headers)  # get company facts data
    
    # Check if facts is None before processing
    if facts is None:
        print(f"No data available for {ticker}. Skipping...")
        return pd.DataFrame()
    
    # Process us_gaap data
    us_gaap_data = facts.get("facts", {}).get("us-gaap", {})
    df_data = []
    
    for fact, details in us_gaap_data.items():
        for unit in details.get("units", {}).keys():
            for item in details["units"][unit]:
                row = item.copy()  # keep the original data intact
                row["fact"] = fact
                row["label"] = details["label"]
                df_data.append(row)
    
    # Create a DataFrame from the collected data
    df = pd.DataFrame(df_data)
    
    df["end"] = pd.to_datetime(df["end"])
    df["start"] = pd.to_datetime(df["start"])
    
    df = df.drop_duplicates(subset=["fact", "end", "val"])
    df.set_index("end", inplace=True)
    
    labels_dict = {fact: details["label"] for fact, details in us_gaap_data.items()}
    
    return df



def download_financial_data_from_SEC(sp500_tickers, sp500_df, data_category, headers = headers):
    """
    download all the financial metrics in the data_category from SEC for the sp500 companies 
    """
    sp500_financial_data = {} # outer dict
    
    for ticker in sp500_tickers:
        
        df = facts_DF(ticker, sp500_df, headers)  # Correct data extraction for the ticker
        financial_data= {} # inner dict
        
        # Loop over all categories in the data_category list
        for category in data_category:
            
            x = df.query('fact == @category')
            x = x[(x['val']!= 0) & (x['val'].notna())]
            # remove the duplicated rows based on 'end' index and keep the last record
            cleaned_data = x[~x.index.duplicated(keep='last')].sort_index(ascending = True)
            # slice data from only 2013 onwards
            financial_data[category] = cleaned_data.loc['2013':]  
            
        # Assign the financial data for each ticker
        sp500_financial_data[ticker] = financial_data
        
    return  sp500_financial_data


def convert_annual_to_quarter(sp500_financial_data, updated_data_category):
    """
    This function updates financial data by subtracting the annual rows by 
    the sum of the previous three quarters for a given list of tickers and data categories
    """
    
    for ticker in sp500_financial_data.keys(): 

        for category in sp500_financial_data[ticker].keys(): 
            
            new_df = sp500_financial_data[ticker][category].reset_index().copy() 

            if category == "EarningsPerShareDiluted" or category == 'CommonStockDividendsPerShareDeclared' or category == 'NetIncomeLoss':

                # identify the annual rows
                index_list = new_df[(new_df['end'] - new_df['start']).dt.days > 130].index.tolist()
    
                # subtract the annual rows by the sum of the previous three quarters.
                for i in index_list: 
                    new_df.loc[i,'val'] = new_df.loc[i,'val'] - new_df.loc[i-3: i-1, 'val'].sum()
    
            sp500_financial_data[ticker][category] = new_df.set_index('end')
        
    return sp500_financial_data


def weighting(metric_ranking_df, monthly_availability_df): 
    """
    Calculate the weighting among selected stocks to construct the portfolio for each month
    Select the top 20% and bottom 20% stocks based on ranking and assign equal weights between the stocks. 
    """
    # filter the PB ranking df based on the monthly availability of the stocks.
    filtered_metric_ranking_df = metric_ranking_df*monthly_availability_df

    # Calculating the top and bottom 20% of the companies for each month: 
    top_20_threshold = filtered_metric_ranking_df.quantile(0.8, axis=1)
    bottom_20_threshold = filtered_metric_ranking_df.quantile(0.2, axis=1)

    # Create masks for top 20% and bottom 20%
    top_20_mask = filtered_metric_ranking_df.ge(top_20_threshold, axis=0)
    bottom_20_mask = filtered_metric_ranking_df.le(bottom_20_threshold, axis=0)

    # Filter dataframes for top 20% and bottom 20%
    top_20_df = filtered_metric_ranking_df[top_20_mask]
    bottom_20_df = filtered_metric_ranking_df[bottom_20_mask]

    # Calculate equal weight for each stock in top 20% and bottom 20%
    top_20_weights = 1 / top_20_df.count(axis=1)
    bottom_20_weights = 1 / bottom_20_df.count(axis=1)

    # Apply the weights across the dataframe. 
    top_20_df_weights = top_20_df.map(lambda x: 1 if pd.notna(x) else np.NaN).multiply(top_20_weights, axis=0)
    bottom_20_df_weights = bottom_20_df.map(lambda x: 1 if pd.notna(x) else np.NaN).multiply(bottom_20_weights, axis=0)

    return top_20_df_weights, bottom_20_df_weights


def cal_cum_rets(top_20_df_weights, bottom_20_df_weights, monthly_returns_df): 
    
    value_20130331 = 100
    # Multiply the monthly returns by the equal weights and aggregate monthly returns. 
    top_20_weighted_monthly_returns = ((top_20_df_weights.shift(1) * monthly_returns_df).sum(axis = 1)).iloc[1:]
    bottom_20_weighted_monthly_returns = ((bottom_20_df_weights.shift(1) * monthly_returns_df).sum(axis = 1)).iloc[1:]
    long_short_monthly_returns =  bottom_20_weighted_monthly_returns - top_20_weighted_monthly_returns
    
    top_20_weighted_cum_returns = value_20130331*(1 + top_20_weighted_monthly_returns).cumprod() - 1
    bottom_20_weighted_cum_returns = value_20130331*(1 + bottom_20_weighted_monthly_returns).cumprod() - 1
    long_short_cum_returns = value_20130331*(1 + long_short_monthly_returns).cumprod() - 1

    return  top_20_weighted_cum_returns, bottom_20_weighted_cum_returns, long_short_cum_returns


def cal_index_cum_rets(index_name, data_folder_download): 
    
    value_20130331 = 100
    index_price = pd.read_csv(data_folder_download + index_name + '.csv', index_col = "Date", parse_dates = True)
    index_price = index_price.resample('ME').last()
    
    index_monthly_returns = index_price / index_price.shift(1) - 1
    index_monthly_returns = index_monthly_returns.iloc[3: ] 
    index_cum_returns = value_20130331*(1 + index_monthly_returns).cumprod() - 1

    return index_cum_returns

def calc_rolling_12mon_return_n_vol(monthly_return):
    """
    calculate the rolling 12month returns and volatilities of a given monthly_return series
    """
    rolling_12mon_return = (1 + monthly_return).rolling(window = 12).apply(lambda x: x.prod()) - 1
    rolling_12mon_std = rolling_12mon_return.rolling(window = 12).std()*np.sqrt(12)
    return rolling_12mon_return, rolling_12mon_std


def calc_index_monthly_returns(ticker, folder):
    index_price = pd.read_csv(folder + ticker + '.csv', index_col = "Date", parse_dates = True)
    index_price = index_price.resample('ME').last()
    index_monthly_returns = index_price / index_price.shift(1) - 1 
    return index_monthly_returns


def get_10Y_bond_yields(folder, file, syear, smonth, sday, eyear, emonth, eday): 
    """
    Get the US 10Y government bond yield
    """
    # read the file into a dataframe
    df = pd.read_csv(data_folder_download + file + '.csv', index_col = 'DATE', parse_dates = True).rename(columns = {'DGS10': '10YR'})
    df['10YR'] = pd.to_numeric(df['10YR'], errors = 'coerce')
    df['10YR'] = df['10YR']/100
    # convert datetime
    start = f"{syear}-{smonth}-{sday}"
    start = pd.to_datetime(start) 
    end = f"{eyear}-{emonth}-{eday}"
    end = pd.to_datetime(end)
    # select the timespan
    df = df.loc[start:end]
    return df

def sharpe_ratio(r, riskfree_rate, periods_per_year):
    """
    Compute the annualized sharpe ratio of a set of returns
    """
    length = r.shape[0]
    riskfree_rate = pd.Series(riskfree_rate, index = r.index)
    rf_per_period = (1+riskfree_rate)**(1/periods_per_year)-1
    rf_per_period = rf_per_period.iloc[3:]
    excess_ret = r - rf_per_period
    annualized_ret = (excess_ret + 1).prod() ** (periods_per_year/length) - 1
    annualized_vol = excess_ret.std()*(periods_per_year**0.5)
    
    return annualized_ret/annualized_vol


def drawdown(return_series: pd.Series):
    """
       Takes a time series of asset returns.
       returns a DataFrame with columns for
       the wealth index, 
       the previous peaks, and 
       the percentage drawdown
    """
    wealth_index = 1000*(1+return_series).cumprod()
    previous_peaks = wealth_index.cummax()
    drawdowns = (wealth_index - previous_peaks)/previous_peaks
    return pd.DataFrame({"Wealth": wealth_index, 
                         "Previous Peak": previous_peaks, 
                         "Drawdown": drawdowns})

def skewness(r):
    """
    Alternative to scipy.stats.skew()
    Computes the skewness of the supplied Series or DataFrame
    Returns a float or a Series
    """
    demeaned_r = r - r.mean()
    # use the population standard deviation, so set dof=0
    sigma_r = r.std(ddof=0)
    exp = (demeaned_r**3).mean()
    return exp/sigma_r**3

def kurtosis(r):
    """
    Alternative to scipy.stats.kurtosis()
    Computes the kurtosis of the supplied Series or DataFrame
    Returns a float or a Series
    """
    demeaned_r = r - r.mean()
    # use the population standard deviation, so set dof=0
    sigma_r = r.std(ddof=0)
    exp = (demeaned_r**4).mean()
    return exp/sigma_r**4

def var_historic(r, level):
    if isinstance(r, pd.DataFrame):
        return r.aggregate(lambda x: abs(np.percentile(x, level)), axis = 0)
    elif isinstance(r, pd.Series): 
        return abs(np.percentile(r, level))  # note here I only want the scaler without index for further comparison or masking. 
    else:
        raise TypeError("Expected r to be Series or DataFrame")


from scipy.stats import norm
def var_gaussian(r, level=5, modified=False):
    """
    Returns the Parametric Gauusian VaR of a Series or DataFrame
    If "modified" is True, then the modified VaR is returned,
    using the Cornish-Fisher modification
    """
    # compute the Z score assuming it was Gaussian
    z = norm.ppf(level/100)
    if modified:
        # modify the Z score based on observed skewness and kurtosis
        s = skewness(r)
        k = kurtosis(r)
        z = (z +
                (z**2 - 1)*s/6 +
                (z**3 -3*z)*(k-3)/24 -
                (2*z**3 - 5*z)*(s**2)/36
            )
    return -(r.mean() + z*r.std(ddof=0))

def cvar_historic(r, level=5):
    """
    Computes the Conditional VaR of Series or DataFrame
    """
    if isinstance(r, pd.Series):
        var_value = var_historic(r, level=level)
        is_beyond = r<= -var_value
        return -r[is_beyond].mean()
    elif isinstance(r, pd.DataFrame):
        return r.aggregate(cvar_historic, level = level)
    else:
        raise TypeError("Expected r to be a Series or DataFrame")






#investor helper

import requests
import json
from dotenv import load_dotenv
import os
import pandas as pd
from datetime import datetime

from functions.data_wrangle import historical_10Q_dw, historical_10Q_merge

def get_ticker_cik_mapping(ticker_list):
    try:
        with open('functions/data/cik_mapping.json', 'r') as f:
            cik_mapping = json.load(f)
            f.close()
    except:
        print("Error in loading cik_mapping.json, check directory")
        return {"ticker": None}

    result = {"ticker": "CIK" + cik_mapping[ticker][0] for ticker in ticker_list if ticker in cik_mapping.keys()}
    if result == {}:
        return {"ticker": None}
    return result

def load_env(api: str = None, cik = None) -> dict:
    try:
        load_dotenv('functions/.env')

        user_agent : str = os.getenv('USER_AGENT')
        headers = {
            'User-Agent': user_agent
        }
        
        url = f"{api}{cik}.json"
        print("Load .env variables -- SUCCESS")
        return {"url": url, "headers": headers}
    except:
        print("Error in loading environment variables, check .env file")
        return {"url": None, "headers": None}

def get_company_info_by_CIK(cik) -> dict:
    
    env_var = load_env("https://data.sec.gov/submissions/", cik = cik)
    if env_var["url"] is None or env_var["headers"] is None:
        return {"client_info": None, "contact_info": None}
    
    attempt = 0
    max_attempt = 1
    while attempt < max_attempt:
        try:
            res = requests.get(env_var["url"], headers=env_var["headers"]).json()
            client_info = {key: res[key] for key in [
                "cik", "name", "sic", "sicDescription", "ownerOrg"
            ]}
            for key in ["tickers", "exchanges"]:
                client_info[key] = ", ".join(list(set(res[key])))
            contact_info = {
                "mailing_address" : res['addresses']['mailing'],
                "phone" : res['phone']
            }
            with open('functions/data/countrycode_mapping.json', 'r') as f:
                countrycode = json.load(f)
                f.close()
            contact_info["mailing_address"]["Country_Region"] = countrycode[contact_info["mailing_address"]["stateOrCountry"]]
            
            return {"client_info": client_info, "contact_info": contact_info}
        except:
            attempt += 1
            if attempt == max_attempt:
                print("Error in SEC API call, check API validity")
                return {"client_info": None, "contact_info": None}
    
def get_company_info_by_ticker(ticker) -> dict:
    cik_mapping = get_ticker_cik_mapping([ticker])
    if cik_mapping["ticker"] is None:
        return {"client_info": None, "contact_info": None}
    return get_company_info_by_CIK(cik_mapping["ticker"])

def get_report_by_CIK(cik, reportType=["10-Q"]) -> dict:
    env_var = load_env(api = "https://data.sec.gov/api/xbrl/companyfacts/", cik = cik)
    if env_var["url"] is None or env_var["headers"] is None:
        print("Error in loading environment variables, check .env file")
        return {}
    
    try:
        res = requests.get(env_var["url"], headers=env_var["headers"]).json()
        data = res['facts']['us-gaap']
        metadata = {
            key: {
                "label": data[key]["label"],
                "description": data[key]["description"]
            }
            for key in data.keys()
        }
        result = {"metadata": metadata, "data": {}, "fileDate": {}}
        for key in metadata.keys():
            for k in data[key]['units'].keys():
                record_list = [{'val': item['val'], 'fileDate': item['filed'], 'fyfp': str(item['fy']) + item['fp'][1], 'endDate': item['end']} for item in data[key]['units'][k] if item['form'] in reportType]
            record_list, file_date_list = historical_10Q_dw(pd.DataFrame(record_list))
            if len(record_list) < 4:
                continue
            result["data"][key] = record_list
            result["fileDate"][key] = file_date_list
        print(f"{reportType} report loaded -- SUCCESS")
        result['data'], result['time'] = historical_10Q_merge(result['data'], result['fileDate'])
        result['metadata']['fileDate'] = {"label": "Filed Date", "description": "The last release date of the filing."}
        
        print(f"{reportType} report loaded -- SUCCESS")
        return result
    
    except:
        print("Error in SEC API call, check API validity")
        return {}

def get_report_by_ticker(ticker, reportType="10-Q") -> dict:
    cik_mapping = get_ticker_cik_mapping([ticker])
    if cik_mapping["ticker"] is None:
        return {}
    return get_report_by_CIK(cik_mapping["ticker"], reportType=reportType)


import pandas as pd
import json
import os

def historical_10Q_dw(df):
    if df.empty:
        return {}, {}
    df[['fileDate', 'endDate']] = df[['fileDate', 'endDate']].apply(pd.to_datetime)
    df = df.sort_values(by=['fileDate', 'endDate'])
    df = df.drop_duplicates(subset='fileDate', keep='last')
    df.index = df['fyfp']
    return df['val'].to_dict(), df['fileDate'].to_dict()

def historical_10Q_merge(data, fileDate):
    col_name = data.keys()
    data = (pd.concat([pd.Series(data[key]) for key in data.keys()], axis=1))
    data = data.fillna('--')
    data.columns = col_name
    data = data.sort_index()
    idx = data.index.astype(str)
    data.index = idx.str[:-1] + ' Q' + idx.str[-1]
    
    fileDate = pd.concat([pd.Series(fileDate[key]) for key in fileDate.keys()]).sort_values().drop_duplicates(keep='last').astype(str).to_frame(name='fileDate')
    fileDateIdx = fileDate.index.astype(str)
    fileDate.index = fileDateIdx.str[:-1] + ' Q' + fileDateIdx.str[-1]
    data = pd.merge(data, fileDate, left_index=True, right_index=True, how='inner')
    
    return data.to_dict(), list(data.index)

def read_json(filepath):
    if os.path.isfile(filepath):
        with open(filepath, 'r') as f:
            data = json.load(f)
            f.close()
        return data
    
    return [{}]

def get_sector_options_results(filter_sector = "--", filepath = "functions/data/sector_mapping.json"):
    if not os.path.isfile(filepath):
        return {"opt" : [], "res" : [], "res_display" : "none", "ini_alert" : "inline-block"}
    
    try:
        filter_sector = filter_sector[0:3]
        data = pd.read_json(filepath).drop_duplicates()
        data['sic_'] = data['sic'].str[:-1]
        d_opt = data.groupby('sic_')['sicDescription'].apply(lambda x: ' | '.join(x.unique())).drop(['000',''])
        d_opt = pd.DataFrame([d_opt.index,d_opt]).T
        d_opt.columns = ['sic','sicDescription']
        
        return {
            "opt" : (d_opt['sic'] + '0 : ' + d_opt['sicDescription']).to_list(),
            "res" : data[data['sic_'] == filter_sector][["tickers", "name", "cik", 'sic', 'sicDescription']].drop_duplicates().sort_values('sic').values.tolist(),
            "res_display" : "inline-block",
            "ini_alert" : "none"
            }
    except:
        print("Error in loading sectors.")
        return []

from functions.tenQ_report import get_ticker_cik_mapping, get_company_info_by_CIK, get_company_info_by_ticker, get_report_by_CIK, get_report_by_ticker
from functions.cik_mapping import get_mapping
from functions.data_wrangle import read_json, get_sector_options_results
from functions.data_load import sample_data_load

def initialize_api():
    print("Initializing data...")
    get_mapping()
    print("Data initialization -- SUCCESS")

def search_cik_api(ticker):
    cik_mapping = get_ticker_cik_mapping([ticker])
    if cik_mapping["ticker"] is None:
        return {str(ticker):"The above ticker is invalid."}
    return cik_mapping

def search_company_api(company, search_method):
    if search_method == "CIK number":
        company_info = get_company_info_by_CIK(company)
    else:
        company_info = get_company_info_by_ticker(company)
    return company_info

def historical_10Q_api(company, search_method):
    if search_method == "CIK number":
        historical_10Q = get_report_by_CIK(company)
    else:
        historical_10Q = get_report_by_ticker(company)
    return historical_10Q

def search_sector_api(filter_sector = "--"):
    return get_sector_options_results(filter_sector=filter_sector)

def sample_data_api():
    return sample_data_load()



# gap keys
{
    "0": {
        "0": "AcceleratedShareRepurchaseProgramAdjustment",
        "1": "AcceleratedShareRepurchasesFinalPricePaidPerShare",
        "2": "AcceleratedShareRepurchasesSettlementPaymentOrReceipt",
        "3": "AccountsPayableCurrent",
        "4": "AccountsReceivableNetCurrent",
        "5": "AccruedIncomeTaxesNoncurrent",
        "6": "AccruedIncomeTaxesPayable",
        "7": "AccruedLiabilitiesCurrent",
        "8": "AccruedProfessionalFeesCurrentAndNoncurrent",
        "9": "AccruedRentCurrent",
        "10": "AccruedRentNoncurrent",
        "11": "AccruedRoyaltiesCurrent",
        "12": "AccumulatedDepreciationDepletionAndAmortizationPropertyPlantAndEquipment",
        "13": "AccumulatedOtherComprehensiveIncomeLossCumulativeChangesInNetGainLossFromCashFlowHedgesEffectNetOfTax",
        "14": "AccumulatedOtherComprehensiveIncomeLossNetOfTax",
        "15": "AdditionalPaidInCapital",
        "16": "AdditionalPaidInCapitalCommonStock",
        "17": "AdjustmentsNoncashItemsToReconcileNetIncomeLossToCashProvidedByUsedInOperatingActivitiesOther",
        "18": "AdjustmentsRelatedToTaxWithholdingForShareBasedCompensation",
        "19": "AdjustmentsToAdditionalPaidInCapitalConvertibleDebtWithConversionFeature",
        "20": "AdjustmentsToAdditionalPaidInCapitalEquityComponentOfConvertibleDebtSubsequentAdjustments",
        "21": "AdjustmentsToAdditionalPaidInCapitalSharebasedCompensationRequisiteServicePeriodRecognitionValue",
        "22": "AdjustmentsToAdditionalPaidInCapitalTaxEffectFromShareBasedCompensation",
        "23": "AdvertisingExpense",
        "24": "AllocatedShareBasedCompensationExpense",
        "25": "AllowanceForDoubtfulAccountsReceivableCurrent",
        "26": "AmortizationOfAcquiredIntangibleAssets",
        "27": "AmortizationOfDebtDiscountPremium",
        "28": "AmortizationOfFinancingCosts",
        "29": "AmortizationOfFinancingCostsAndDiscounts",
        "30": "AmortizationOfIntangibleAssets",
        "31": "AntidilutiveSecuritiesExcludedFromComputationOfEarningsPerShareAmount",
        "32": "AssetRetirementObligation",
        "33": "AssetRetirementObligationsNoncurrent",
        "34": "Assets",
        "35": "AssetsCurrent",
        "36": "AssetsHeldForSaleCurrent",
        "37": "AvailableForSaleDebtSecuritiesAccumulatedGrossUnrealizedGainBeforeTax",
        "38": "AvailableForSaleDebtSecuritiesAccumulatedGrossUnrealizedLossBeforeTax",
        "39": "AvailableForSaleDebtSecuritiesAmortizedCostBasis",
        "40": "AvailableForSaleSecurities",
        "41": "AvailableForSaleSecuritiesAccumulatedGrossUnrealizedGainBeforeTax",
        "42": "AvailableForSaleSecuritiesAmortizedCost",
        "43": "AvailableForSaleSecuritiesChangeInNetUnrealizedHoldingGainLossNetOfTax",
        "44": "AvailableForSaleSecuritiesContinuousUnrealizedLossPosition12MonthsOrLongerAccumulatedLoss",
        "45": "AvailableForSaleSecuritiesContinuousUnrealizedLossPosition12MonthsOrLongerAggregateLosses",
        "46": "AvailableForSaleSecuritiesContinuousUnrealizedLossPositionAccumulatedLoss",
        "47": "AvailableForSaleSecuritiesContinuousUnrealizedLossPositionAggregateLosses",
        "48": "AvailableForSaleSecuritiesContinuousUnrealizedLossPositionFairValue",
        "49": "AvailableForSaleSecuritiesContinuousUnrealizedLossPositionLessThan12MonthsAccumulatedLoss",
        "50": "AvailableForSaleSecuritiesContinuousUnrealizedLossPositionLessThan12MonthsAggregateLosses",
        "51": "AvailableForSaleSecuritiesContinuousUnrealizedLossPositionLessThanTwelveMonthsFairValue",
        "52": "AvailableForSaleSecuritiesContinuousUnrealizedLossPositionTwelveMonthsOrLongerFairValue",
        "53": "AvailableForSaleSecuritiesDebtMaturitiesAfterOneThroughFiveYearsAmortizedCost",
        "54": "AvailableForSaleSecuritiesDebtMaturitiesAfterOneThroughFiveYearsFairValue",
        "55": "AvailableForSaleSecuritiesDebtMaturitiesAmortizedCost",
        "56": "AvailableForSaleSecuritiesDebtMaturitiesFairValue",
        "57": "AvailableForSaleSecuritiesDebtMaturitiesWithinOneYearAmortizedCost",
        "58": "AvailableForSaleSecuritiesDebtMaturitiesWithinOneYearFairValue",
        "59": "AvailableForSaleSecuritiesDebtMaturitiesWithoutSingleMaturityDateAmortizedCost",
        "60": "AvailableForSaleSecuritiesDebtMaturitiesWithoutSingleMaturityDateFairValue",
        "61": "AvailableForSaleSecuritiesDebtSecurities",
        "62": "AvailableForSaleSecuritiesFairValueDisclosure",
        "63": "AvailableForSaleSecuritiesGrossRealizedGainLossNet",
        "64": "AvailableForSaleSecuritiesGrossUnrealizedGains",
        "65": "AvailableForSaleSecuritiesGrossUnrealizedLoss",
        "66": "AvailableForSaleSecuritiesGrossUnrealizedLosses1",
        "67": "AvailableForSaleSecuritiesInUnrealizedLossPositionsQualitativeDisclosureNumberOfPositions",
        "68": "AvailableforsaleSecuritiesContinuousUnrealizedLossPosition12MonthsOrLongerAggregateLosses1",
        "69": "AvailableforsaleSecuritiesContinuousUnrealizedLossPosition12MonthsOrLongerAggregateLosses2",
        "70": "AvailableforsaleSecuritiesContinuousUnrealizedLossPositionAggregateLosses1",
        "71": "AvailableforsaleSecuritiesContinuousUnrealizedLossPositionAggregateLosses2",
        "72": "AvailableforsaleSecuritiesContinuousUnrealizedLossPositionLessThan12MonthsAggregateLosses1",
        "73": "AvailableforsaleSecuritiesContinuousUnrealizedLossPositionLessThan12MonthsAggregateLosses2",
        "74": "AvailableforsaleSecuritiesGrossUnrealizedGain",
        "75": "AvailableforsaleSecuritiesGrossUnrealizedGainLoss1",
        "76": "AvailableforsaleSecuritiesInUnrealizedLossPositionsQualitativeDisclosureNumberOfPositions1",
        "77": "BusinessAcquisitionContingentConsiderationPotentialCashPayment",
        "78": "BusinessAcquisitionCostOfAcquiredEntityCashPaid",
        "79": "BusinessAcquisitionCostOfAcquiredEntityTransactionCosts",
        "80": "BusinessAcquisitionProFormaEarningsPerShareBasic",
        "81": "BusinessAcquisitionProFormaEarningsPerShareDiluted",
        "82": "BusinessAcquisitionProFormaNetIncomeLoss",
        "83": "BusinessAcquisitionProFormaRevenue",
        "84": "BusinessAcquisitionPurchasePriceAllocationAmortizableIntangibleAssets",
        "85": "BusinessAcquisitionPurchasePriceAllocationAssetsAcquired",
        "86": "BusinessAcquisitionPurchasePriceAllocationAssetsAcquiredLiabilitiesAssumedNet",
        "87": "BusinessAcquisitionPurchasePriceAllocationCurrentAssetsInventory",
        "88": "BusinessAcquisitionPurchasePriceAllocationCurrentAssetsPrepaidExpenseAndOtherAssets",
        "89": "BusinessAcquisitionPurchasePriceAllocationCurrentAssetsReceivables",
        "90": "BusinessAcquisitionPurchasePriceAllocationCurrentLiabilitiesAccountsPayable",
        "91": "BusinessAcquisitionPurchasePriceAllocationCurrentLiabilitiesAccruedLiabilities",
        "92": "BusinessAcquisitionPurchasePriceAllocationGoodwillAmount",
        "93": "BusinessAcquisitionPurchasePriceAllocationNoncurrentAssets",
        "94": "BusinessAcquisitionPurchasePriceAllocationNotesPayableAndLongTermDebt",
        "95": "BusinessAcquisitionPurchasePriceAllocationOtherNoncurrentAssets",
        "96": "BusinessAcquisitionPurchasePriceAllocationPropertyPlantAndEquipment",
        "97": "BusinessAcquisitionsProFormaNetIncomeLoss",
        "98": "BusinessAcquisitionsProFormaRevenue",
        "99": "BusinessCombinationAcquisitionRelatedCosts",
        "100": "CapitalLeaseObligationsCurrent",
        "101": "CapitalLeaseObligationsNoncurrent",
        "102": "CapitalLeasesFutureMinimumPaymentsDue",
        "103": "CapitalLeasesFutureMinimumPaymentsDueCurrent",
        "104": "CapitalLeasesFutureMinimumPaymentsDueInFiveYears",
        "105": "CapitalLeasesFutureMinimumPaymentsDueInFourYears",
        "106": "CapitalLeasesFutureMinimumPaymentsDueInThreeYears",
        "107": "CapitalLeasesFutureMinimumPaymentsDueInTwoYears",
        "108": "CapitalLeasesFutureMinimumPaymentsDueThereafter",
        "109": "CapitalLeasesFutureMinimumPaymentsPresentValueOfNetMinimumPayments",
        "110": "CashAndCashEquivalentsAtCarryingValue",
        "111": "CashAndCashEquivalentsPeriodIncreaseDecrease",
        "112": "CashCashEquivalentsRestrictedCashAndRestrictedCashEquivalents",
        "113": "CashCashEquivalentsRestrictedCashAndRestrictedCashEquivalentsPeriodIncreaseDecreaseIncludingExchangeRateEffect",
        "114": "CashEquivalentsAtCarryingValue",
        "115": "ClassOfWarrantOrRightNumberOfSecuritiesCalledByEachWarrantOrRight",
        "116": "ClassOfWarrantOrRightNumberOfSecuritiesCalledByWarrantsOrRights",
        "117": "ClassOfWarrantOrRightOutstanding",
        "118": "CommercialPaper",
        "119": "CommitmentsAndContingencies",
        "120": "CommonStockCapitalSharesReservedForFutureIssuance",
        "121": "CommonStockDividendsPerShareCashPaid",
        "122": "CommonStockDividendsPerShareDeclared",
        "123": "CommonStockParOrStatedValuePerShare",
        "124": "CommonStockSharesAuthorized",
        "125": "CommonStockSharesIssued",
        "126": "CommonStockSharesOutstanding",
        "127": "CommonStockValue",
        "128": "ComprehensiveIncomeNetOfTax",
        "129": "ComprehensiveIncomeNetOfTaxIncludingPortionAttributableToNoncontrollingInterest",
        "130": "ContractWithCustomerLiability",
        "131": "ContractWithCustomerLiabilityCurrent",
        "132": "ContractWithCustomerLiabilityIncreaseDecreaseForContractAcquiredInBusinessCombination",
        "133": "ContractWithCustomerLiabilityNoncurrent",
        "134": "ContractWithCustomerLiabilityRevenueRecognized",
        "135": "ConvertibleDebt",
        "136": "ConvertibleDebtCurrent",
        "137": "ConvertibleDebtFairValueDisclosures",
        "138": "ConvertibleDebtNoncurrent",
        "139": "CostMethodInvestments",
        "140": "CostOfGoodsAndServicesSold",
        "141": "CostOfGoodsSoldAmortization",
        "142": "CostOfRevenue",
        "143": "CumulativeEffectOfNewAccountingPrincipleInPeriodOfAdoption",
        "144": "CurrentFederalTaxExpenseBenefit",
        "145": "CurrentForeignTaxExpenseBenefit",
        "146": "CurrentIncomeTaxExpenseBenefit",
        "147": "CurrentStateAndLocalTaxExpenseBenefit",
        "148": "CustomerAdvancesAndDeposits",
        "149": "DebtConversionConvertedInstrumentSharesIssued1",
        "150": "DebtCurrent",
        "151": "DebtInstrumentConvertibleCarryingAmountOfTheEquityComponent",
        "152": "DebtInstrumentConvertibleConversionPrice1",
        "153": "DebtInstrumentConvertibleConversionRatio",
        "154": "DebtInstrumentConvertibleConversionRatio1",
        "155": "DebtInstrumentConvertibleEffectiveInterestRate",
        "156": "DebtInstrumentConvertibleIfConvertedValueInExcessOfPrincipal",
        "157": "DebtInstrumentConvertibleInterestExpense",
        "158": "DebtInstrumentConvertibleNumberOfEquityInstruments",
        "159": "DebtInstrumentConvertibleThresholdPercentageOfStockPriceTrigger",
        "160": "DebtInstrumentFaceAmount",
        "161": "DebtInstrumentInterestRateEffectivePercentage",
        "162": "DebtInstrumentInterestRateStatedPercentage",
        "163": "DebtInstrumentUnamortizedDiscount",
        "164": "DebtInstrumentUnamortizedDiscountPremiumAndDebtIssuanceCostsNet",
        "165": "DebtSecuritiesAvailableForSaleContinuousUnrealizedLossPosition12MonthsOrLonger",
        "166": "DebtSecuritiesAvailableForSaleContinuousUnrealizedLossPosition12MonthsOrLongerAccumulatedLoss",
        "167": "DebtSecuritiesAvailableForSaleContinuousUnrealizedLossPositionLessThan12Months",
        "168": "DebtSecuritiesAvailableForSaleContinuousUnrealizedLossPositionLessThan12MonthsAccumulatedLoss",
        "169": "DebtSecuritiesAvailableForSaleUnrealizedLossPosition",
        "170": "DebtSecuritiesAvailableForSaleUnrealizedLossPositionAccumulatedLoss",
        "171": "DeferredCompensationLiabilityClassifiedNoncurrent",
        "172": "DeferredFederalIncomeTaxExpenseBenefit",
        "173": "DeferredForeignIncomeTaxExpenseBenefit",
        "174": "DeferredIncomeTaxAssetsNet",
        "175": "DeferredIncomeTaxExpenseBenefit",
        "176": "DeferredIncomeTaxLiabilities",
        "177": "DeferredIncomeTaxLiabilitiesNet",
        "178": "DeferredRentCredit",
        "179": "DeferredRevenue",
        "180": "DeferredRevenueAdditions",
        "181": "DeferredRevenueCurrent",
        "182": "DeferredRevenueNoncurrent",
        "183": "DeferredRevenueRevenueRecognized",
        "184": "DeferredRevenueRevenueRecognized1",
        "185": "DeferredStateAndLocalIncomeTaxExpenseBenefit",
        "186": "DeferredTaxAssetsGross",
        "187": "DeferredTaxAssetsGrossNoncurrent",
        "188": "DeferredTaxAssetsLiabilitiesNet",
        "189": "DeferredTaxAssetsLiabilitiesNetNoncurrent",
        "190": "DeferredTaxAssetsNet",
        "191": "DeferredTaxAssetsNetCurrent",
        "192": "DeferredTaxAssetsNetNoncurrent",
        "193": "DeferredTaxAssetsOperatingLossCarryforwards",
        "194": "DeferredTaxAssetsOperatingLossCarryforwardsDomestic",
        "195": "DeferredTaxAssetsOperatingLossCarryforwardsForeign",
        "196": "DeferredTaxAssetsOperatingLossCarryforwardsStateAndLocal",
        "197": "DeferredTaxAssetsTaxCreditCarryforwards",
        "198": "DeferredTaxAssetsTaxCreditCarryforwardsOther",
        "199": "DeferredTaxAssetsTaxCreditCarryforwardsResearch",
        "200": "DeferredTaxAssetsTaxDeferredExpenseCompensationAndBenefitsShareBasedCompensationCost",
        "201": "DeferredTaxAssetsTaxDeferredExpenseReservesAndAccruals",
        "202": "DeferredTaxAssetsValuationAllowance",
        "203": "DeferredTaxExpenseFromStockOptionsExercised",
        "204": "DeferredTaxLiabilities",
        "205": "DeferredTaxLiabilitiesNoncurrent",
        "206": "DeferredTaxLiabilitiesUndistributedForeignEarnings",
        "207": "DefinedContributionPlanCostRecognized",
        "208": "DefinedContributionPlanMaximumAnnualContributionsPerEmployeePercent",
        "209": "DepositsAssets",
        "210": "DepositsAssetsNoncurrent",
        "211": "Depreciation",
        "212": "DepreciationAndAmortization",
        "213": "DepreciationDepletionAndAmortization",
        "214": "DerivativeLiabilities",
        "215": "DerivativeLiabilitiesNoncurrent",
        "216": "Dividends",
        "217": "DividendsCash",
        "218": "DividendsCommonStockCash",
        "219": "EarningsPerShareBasic",
        "220": "EarningsPerShareDiluted",
        "221": "EffectiveIncomeTaxRateContinuingOperations",
        "222": "EffectiveIncomeTaxRateReconciliationAtFederalStatutoryIncomeTaxRate",
        "223": "EmployeeRelatedLiabilitiesCurrent",
        "224": "EmployeeServiceShareBasedCompensationAllocationOfRecognizedPeriodCostsCapitalizedAmount",
        "225": "EmployeeServiceShareBasedCompensationNonvestedAwardsTotalCompensationCostNotYetRecognized",
        "226": "EmployeeServiceShareBasedCompensationNonvestedAwardsTotalCompensationCostNotYetRecognizedPeriodForRecognition",
        "227": "EmployeeServiceShareBasedCompensationUnrecognizedCompensationCostsOnNonvestedAwards",
        "228": "EntityWideDisclosureOnGeographicAreasLongLivedAssetsInIndividualForeignCountriesAmount",
        "229": "EquitySecuritiesFvNi",
        "230": "EquitySecuritiesFvNiUnrealizedGain",
        "231": "EquitySecuritiesFvNiUnrealizedLoss",
        "232": "ExcessTaxBenefitFromShareBasedCompensationFinancingActivities",
        "233": "ExcessTaxBenefitFromShareBasedCompensationOperatingActivities",
        "234": "ExtinguishmentOfDebtAmount",
        "235": "FairValueMeasurementWithUnobservableInputsReconciliationRecurringBasisAssetTransfersNet",
        "236": "FairValueMeasurementWithUnobservableInputsReconciliationRecurringBasisAssetValue",
        "237": "FiniteLivedIntangibleAssetsAccumulatedAmortization",
        "238": "FiniteLivedIntangibleAssetsAmortizationExpense",
        "239": "FiniteLivedIntangibleAssetsAmortizationExpenseAfterYearFive",
        "240": "FiniteLivedIntangibleAssetsAmortizationExpenseNextTwelveMonths",
        "241": "FiniteLivedIntangibleAssetsAmortizationExpenseRemainderOfFiscalYear",
        "242": "FiniteLivedIntangibleAssetsAmortizationExpenseYearFive",
        "243": "FiniteLivedIntangibleAssetsAmortizationExpenseYearFour",
        "244": "FiniteLivedIntangibleAssetsAmortizationExpenseYearThree",
        "245": "FiniteLivedIntangibleAssetsAmortizationExpenseYearTwo",
        "246": "FiniteLivedIntangibleAssetsGross",
        "247": "FiniteLivedIntangibleAssetsNet",
        "248": "FiniteLivedIntangibleAssetsPeriodIncreaseDecrease",
        "249": "FiniteLivedIntangibleAssetsUsefulLife",
        "250": "ForeignCurrencyTransactionGainLossRealized",
        "251": "FutureAmortizationExpenseAfterYearFive",
        "252": "FutureAmortizationExpenseRemainderOfFiscalYear",
        "253": "FutureAmortizationExpenseYearFive",
        "254": "FutureAmortizationExpenseYearFour",
        "255": "FutureAmortizationExpenseYearOne",
        "256": "FutureAmortizationExpenseYearThree",
        "257": "FutureAmortizationExpenseYearTwo",
        "258": "GainLossOnForeignCurrencyCashFlowHedgeIneffectiveness",
        "259": "GainLossOnInvestments",
        "260": "GainsLossesOnExtinguishmentOfDebt",
        "261": "Goodwill",
        "262": "GoodwillAcquiredDuringPeriod",
        "263": "GoodwillImpairmentLoss",
        "264": "GoodwillPeriodIncreaseDecrease",
        "265": "GoodwillTranslationAndPurchaseAccountingAdjustments",
        "266": "GrossProfit",
        "267": "ImpairmentOfInvestments",
        "268": "IncomeLossFromContinuingOperationsBeforeIncomeTaxesDomestic",
        "269": "IncomeLossFromContinuingOperationsBeforeIncomeTaxesExtraordinaryItemsNoncontrollingInterest",
        "270": "IncomeLossFromContinuingOperationsBeforeIncomeTaxesForeign",
        "271": "IncomeLossFromContinuingOperationsBeforeIncomeTaxesMinorityInterestAndIncomeLossFromEquityMethodInvestments",
        "272": "IncomeTaxExpenseBenefit",
        "273": "IncomeTaxExpenseBenefitContinuingOperationsAdjustmentOfDeferredTaxAssetLiability",
        "274": "IncomeTaxesPaidNet",
        "275": "IncomeTaxReconciliationForeignIncomeTaxRateDifferential",
        "276": "IncomeTaxReconciliationIncomeTaxExpenseBenefitAtFederalStatutoryIncomeTaxRate",
        "277": "IncomeTaxReconciliationNondeductibleExpenseRestructuringCharges",
        "278": "IncomeTaxReconciliationNondeductibleExpenseShareBasedCompensationCost",
        "279": "IncomeTaxReconciliationOtherAdjustments",
        "280": "IncomeTaxReconciliationStateAndLocalIncomeTaxes",
        "281": "IncomeTaxReconciliationTaxCreditsResearch",
        "282": "IncreaseDecreaseInAccountsPayable",
        "283": "IncreaseDecreaseInAccountsReceivable",
        "284": "IncreaseDecreaseInAccruedLiabilities",
        "285": "IncreaseDecreaseInAccruedLiabilitiesAndOtherOperatingLiabilities",
        "286": "IncreaseDecreaseInContractWithCustomerLiability",
        "287": "IncreaseDecreaseInInventories",
        "288": "IncreaseDecreaseInOtherNoncurrentLiabilities",
        "289": "IncreaseDecreaseInOtherOperatingAssets",
        "290": "IncreaseDecreaseInPrepaidDeferredExpenseAndOtherAssets",
        "291": "IncrementalCommonSharesAttributableToCallOptionsAndWarrants",
        "292": "IncrementalCommonSharesAttributableToConversionOfDebtSecurities",
        "293": "InsuranceRecoveries",
        "294": "IntangibleAssetsNetExcludingGoodwill",
        "295": "InterestExpense",
        "296": "InterestExpenseDebt",
        "297": "InterestExpenseDebtExcludingAmortization",
        "298": "InterestPaidNet",
        "299": "InterestPayableCurrent",
        "300": "InterestRateDerivativeLiabilitiesAtFairValue",
        "301": "InventoryFinishedGoods",
        "302": "InventoryFinishedGoodsNetOfReserves",
        "303": "InventoryNet",
        "304": "InventoryRawMaterials",
        "305": "InventoryRawMaterialsNetOfReserves",
        "306": "InventoryWorkInProcess",
        "307": "InventoryWorkInProcessNetOfReserves",
        "308": "InvestmentIncomeInterest",
        "309": "LeaseAndRentalExpense",
        "310": "LesseeOperatingLeaseLiabilityPaymentsDue",
        "311": "LesseeOperatingLeaseLiabilityPaymentsDueAfterYearFive",
        "312": "LesseeOperatingLeaseLiabilityPaymentsDueNextTwelveMonths",
        "313": "LesseeOperatingLeaseLiabilityPaymentsDueYearFive",
        "314": "LesseeOperatingLeaseLiabilityPaymentsDueYearFour",
        "315": "LesseeOperatingLeaseLiabilityPaymentsDueYearThree",
        "316": "LesseeOperatingLeaseLiabilityPaymentsDueYearTwo",
        "317": "LesseeOperatingLeaseLiabilityPaymentsRemainderOfFiscalYear",
        "318": "LesseeOperatingLeaseLiabilityUndiscountedExcessAmount",
        "319": "Liabilities",
        "320": "LiabilitiesAndStockholdersEquity",
        "321": "LiabilitiesAssumed",
        "322": "LiabilitiesAssumed1",
        "323": "LiabilitiesCurrent",
        "324": "LineOfCreditFacilityCurrentBorrowingCapacity",
        "325": "LitigationSettlementExpense",
        "326": "LitigationSettlementGross",
        "327": "LongTermDebt",
        "328": "LongTermDebtCurrent",
        "329": "LongTermDebtNoncurrent",
        "330": "MarketableSecurities",
        "331": "MarketableSecuritiesCurrent",
        "332": "MarketableSecuritiesRealizedGainLossExcludingOtherThanTemporaryImpairments",
        "333": "MarketableSecuritiesRealizedGainLossOtherThanTemporaryImpairmentsAmount",
        "334": "MaterialsSuppliesAndOther",
        "335": "MoneyMarketFundsAtCarryingValue",
        "336": "NetCashProvidedByUsedInFinancingActivities",
        "337": "NetCashProvidedByUsedInInvestingActivities",
        "338": "NetCashProvidedByUsedInOperatingActivities",
        "339": "NetIncomeLoss",
        "340": "NoncashOrPartNoncashAcquisitionIntangibleAssetsAcquired",
        "341": "NoncurrentAssets",
        "342": "NonoperatingIncomeExpense",
        "343": "NumberOfForeignCurrencyDerivativesHeld",
        "344": "NumberOfReportableSegments",
        "345": "OperatingExpenses",
        "346": "OperatingIncomeLoss",
        "347": "OperatingLeaseCost",
        "348": "OperatingLeaseLiability",
        "349": "OperatingLeaseLiabilityCurrent",
        "350": "OperatingLeaseLiabilityNoncurrent",
        "351": "OperatingLeasePayments",
        "352": "OperatingLeaseRightOfUseAsset",
        "353": "OperatingLeasesFutureMinimumPaymentsDue",
        "354": "OperatingLeasesFutureMinimumPaymentsDueCurrent",
        "355": "OperatingLeasesFutureMinimumPaymentsDueInFiveYears",
        "356": "OperatingLeasesFutureMinimumPaymentsDueInFourYears",
        "357": "OperatingLeasesFutureMinimumPaymentsDueInThreeYears",
        "358": "OperatingLeasesFutureMinimumPaymentsDueInTwoYears",
        "359": "OperatingLeasesFutureMinimumPaymentsDueThereafter",
        "360": "OperatingLeaseWeightedAverageDiscountRatePercent",
        "361": "OperatingLossCarryforwards",
        "362": "OptionIndexedToIssuersEquityStrikePrice1",
        "363": "OtherAccruedLiabilitiesCurrent",
        "364": "OtherAssets",
        "365": "OtherAssetsCurrent",
        "366": "OtherAssetsMiscellaneousNoncurrent",
        "367": "OtherAssetsNoncurrent",
        "368": "OtherCommitment",
        "369": "OtherComprehensiveIncomeAvailableforsaleSecuritiesAdjustmentNetOfTaxPortionAttributableToParent",
        "370": "OtherComprehensiveIncomeDerivativesQualifyingAsHedgesNetOfTaxPortionAttributableToParent",
        "371": "OtherComprehensiveIncomeLossAvailableForSaleSecuritiesAdjustmentBeforeTax",
        "372": "OtherComprehensiveIncomeLossCashFlowHedgeGainLossAfterReclassificationAndTax",
        "373": "OtherComprehensiveIncomeLossCashFlowHedgeGainLossBeforeReclassificationAfterTax",
        "374": "OtherComprehensiveIncomeLossCashFlowHedgeGainLossReclassificationAfterTax",
        "375": "OtherComprehensiveIncomeLossDerivativesQualifyingAsHedgesNetOfTax",
        "376": "OtherComprehensiveIncomeLossNetOfTax",
        "377": "OtherComprehensiveIncomeLossNetOfTaxPeriodIncreaseDecrease",
        "378": "OtherComprehensiveIncomeLossNetOfTaxPortionAttributableToParent",
        "379": "OtherComprehensiveIncomeLossReclassificationAdjustmentForSaleOfSecuritiesIncludedInNetIncomeNetOfTax",
        "380": "OtherComprehensiveIncomeLossReclassificationAdjustmentForSaleOfSecuritiesIncludedInNetIncomeTax",
        "381": "OtherComprehensiveIncomeLossReclassificationAdjustmentFromAOCIForSaleOfSecuritiesNetOfTax",
        "382": "OtherComprehensiveIncomeLossReclassificationAdjustmentFromAOCIForSaleOfSecuritiesTax",
        "383": "OtherComprehensiveIncomeLossReclassificationAdjustmentFromAOCIOnDerivativesNetOfTax",
        "384": "OtherComprehensiveIncomeLossReclassificationAdjustmentFromAOCIPensionAndOtherPostretirementBenefitPlansForNetGainLossNetOfTax",
        "385": "OtherComprehensiveIncomeLossTax",
        "386": "OtherComprehensiveIncomeReclassificationAdjustmentForSaleOfSecuritiesIncludedInNetIncomeBeforeTax",
        "387": "OtherComprehensiveIncomeReclassificationAdjustmentForSaleOfSecuritiesIncludedInNetIncomeNetOfTax",
        "388": "OtherComprehensiveIncomeReclassificationAdjustmentForSaleOfSecuritiesIncludedInNetIncomeTax",
        "389": "OtherComprehensiveIncomeReclassificationOfDefinedBenefitPlansNetGainLossRecognizedInNetPeriodicBenefitCostNetOfTax",
        "390": "OtherComprehensiveIncomeUnrealizedGainLossOnDerivativesArisingDuringPeriodNetOfTax",
        "391": "OtherComprehensiveIncomeUnrealizedGainLossOnDerivativesArisingDuringPeriodTax",
        "392": "OtherComprehensiveIncomeUnrealizedHoldingGainLossOnSecuritiesArisingDuringPeriodBeforeTax",
        "393": "OtherComprehensiveIncomeUnrealizedHoldingGainLossOnSecuritiesArisingDuringPeriodNetOfTax",
        "394": "OtherComprehensiveIncomeUnrealizedHoldingGainLossOnSecuritiesArisingDuringPeriodTax",
        "395": "OtherLiabilitiesNoncurrent",
        "396": "OtherLongTermInvestments",
        "397": "OtherNoncashIncomeExpense",
        "398": "OtherNoncurrentLiabilities",
        "399": "OtherNonoperatingIncomeExpense",
        "400": "OtherNonrecurringExpense",
        "401": "OtherNonrecurringIncomeExpense",
        "402": "OtherPrepaidExpenseCurrent",
        "403": "OtherSundryLiabilitiesNoncurrent",
        "404": "OtherThanTemporaryImpairmentLossDebtSecuritiesAvailableForSaleRecognizedInEarnings",
        "405": "PaymentsForHedgeFinancingActivities",
        "406": "PaymentsForLegalSettlements",
        "407": "PaymentsForProceedsFromOtherInvestingActivities",
        "408": "PaymentsForRepurchaseOfCommonStock",
        "409": "PaymentsForRestructuring",
        "410": "PaymentsOfDebtIssuanceCosts",
        "411": "PaymentsOfDividends",
        "412": "PaymentsRelatedToTaxWithholdingForShareBasedCompensation",
        "413": "PaymentsToAcquireAvailableForSaleSecuritiesDebt",
        "414": "PaymentsToAcquireBusinessesNetOfCashAcquired",
        "415": "PaymentsToAcquireMarketableSecurities",
        "416": "PaymentsToAcquireOtherInvestments",
        "417": "PaymentsToAcquireProductiveAssets",
        "418": "PaymentsToAcquirePropertyPlantAndEquipment",
        "419": "PreferredStockParOrStatedValuePerShare",
        "420": "PreferredStockSharesAuthorized",
        "421": "PreferredStockSharesIssued",
        "422": "PreferredStockSharesOutstanding",
        "423": "PreferredStockValueOutstanding",
        "424": "PrepaidExpenseAndOtherAssetsCurrent",
        "425": "PrepaidExpenseCurrent",
        "426": "PrepaidInsurance",
        "427": "PrepaidRent",
        "428": "PrepaidTaxes",
        "429": "ProceedsFromConvertibleDebt",
        "430": "ProceedsFromDebtNetOfIssuanceCosts",
        "431": "ProceedsFromIncomeTaxRefunds",
        "432": "ProceedsFromIssuanceOfCommonStock",
        "433": "ProceedsFromIssuanceOfDebt",
        "434": "ProceedsFromIssuanceOfLongTermDebt",
        "435": "ProceedsFromIssuanceOfWarrants",
        "436": "ProceedsFromMaturitiesPrepaymentsAndCallsOfAvailableForSaleSecurities",
        "437": "ProceedsFromPaymentsForOtherFinancingActivities",
        "438": "ProceedsFromSaleAndMaturityOfMarketableSecurities",
        "439": "ProceedsFromSaleOfAvailableForSaleSecurities",
        "440": "ProceedsFromSaleOfAvailableForSaleSecuritiesDebt",
        "441": "ProceedsFromStockOptionsExercised",
        "442": "ProductWarrantyAccrual",
        "443": "ProductWarrantyAccrualClassifiedCurrent",
        "444": "ProductWarrantyAccrualPayments",
        "445": "ProductWarrantyAccrualWarrantiesIssued",
        "446": "ProductWarrantyExpense",
        "447": "PropertyPlantAndEquipmentGross",
        "448": "PropertyPlantAndEquipmentNet",
        "449": "PropertyPlantAndEquipmentUsefulLifeMaximum",
        "450": "PropertyPlantAndEquipmentUsefulLifeMinimum",
        "451": "PurchaseObligation",
        "452": "PurchaseObligationDueInFourthYear",
        "453": "PurchaseObligationDueInNextTwelveMonths",
        "454": "PurchaseObligationDueInSecondYear",
        "455": "PurchaseObligationDueInThirdYear",
        "456": "PurchaseObligationFutureMinimumPaymentsRemainderOfFiscalYear",
        "457": "RealizedInvestmentGainsLosses",
        "458": "ReclassificationFromAccumulatedOtherComprehensiveIncomeCurrentPeriodNetOfTax",
        "459": "RepaymentsOfConvertibleDebt",
        "460": "RepaymentsOfDebt",
        "461": "RepaymentsOfLongTermCapitalLeaseObligations",
        "462": "ResearchAndDevelopmentExpense",
        "463": "ResearchAndDevelopmentInProcess",
        "464": "RestructuringCharges",
        "465": "RestructuringReserve",
        "466": "RestructuringReserveSettledWithoutCash1",
        "467": "RestructuringReserveSettledWithoutCash2",
        "468": "RetainedEarningsAccumulatedDeficit",
        "469": "RevenueFromContractWithCustomerExcludingAssessedTax",
        "470": "RevenueRemainingPerformanceObligation",
        "471": "RevenueRemainingPerformanceObligationPercentage",
        "472": "Revenues",
        "473": "RightOfUseAssetObtainedInExchangeForOperatingLeaseLiability",
        "474": "SellingGeneralAndAdministrativeExpense",
        "475": "SettlementLiabilitiesCurrent",
        "476": "SeveranceCosts",
        "477": "SeveranceCosts1",
        "478": "ShareBasedCompensation",
        "479": "ShareBasedCompensationArrangementByShareBasedPaymentAwardEquityInstrumentsOtherThanOptionsForfeitedInPeriod",
        "480": "ShareBasedCompensationArrangementByShareBasedPaymentAwardEquityInstrumentsOtherThanOptionsForfeitedInPeriodWeightedAverageGrantDateFairValue",
        "481": "ShareBasedCompensationArrangementByShareBasedPaymentAwardEquityInstrumentsOtherThanOptionsForfeituresWeightedAverageGrantDateFairValue",
        "482": "ShareBasedCompensationArrangementByShareBasedPaymentAwardEquityInstrumentsOtherThanOptionsGrantsInPeriod",
        "483": "ShareBasedCompensationArrangementByShareBasedPaymentAwardEquityInstrumentsOtherThanOptionsGrantsInPeriodWeightedAverageGrantDateFairValue",
        "484": "ShareBasedCompensationArrangementByShareBasedPaymentAwardEquityInstrumentsOtherThanOptionsNonvestedNumber",
        "485": "ShareBasedCompensationArrangementByShareBasedPaymentAwardEquityInstrumentsOtherThanOptionsNonvestedWeightedAverageGrantDateFairValue",
        "486": "ShareBasedCompensationArrangementByShareBasedPaymentAwardEquityInstrumentsOtherThanOptionsVestedInPeriod",
        "487": "ShareBasedCompensationArrangementByShareBasedPaymentAwardEquityInstrumentsOtherThanOptionsVestedInPeriodWeightedAverageGrantDateFairValue",
        "488": "ShareBasedCompensationArrangementByShareBasedPaymentAwardMaximumEmployeeSubscriptionRate",
        "489": "ShareBasedCompensationArrangementByShareBasedPaymentAwardNonOptionEquityInstrumentsOther",
        "490": "ShareBasedCompensationArrangementByShareBasedPaymentAwardNonOptionEquityInstrumentsOutstandingNumber",
        "491": "ShareBasedCompensationArrangementByShareBasedPaymentAwardNumberOfSharesAvailableForGrant",
        "492": "ShareBasedCompensationArrangementByShareBasedPaymentAwardOptionsExercisableIntrinsicValue",
        "493": "ShareBasedCompensationArrangementByShareBasedPaymentAwardOptionsExercisableNumber",
        "494": "ShareBasedCompensationArrangementByShareBasedPaymentAwardOptionsExercisableWeightedAverageExercisePrice",
        "495": "ShareBasedCompensationArrangementByShareBasedPaymentAwardOptionsExercisableWeightedAverageRemainingContractualTerm",
        "496": "ShareBasedCompensationArrangementByShareBasedPaymentAwardOptionsExercisesInPeriod",
        "497": "ShareBasedCompensationArrangementByShareBasedPaymentAwardOptionsExercisesInPeriodTotalIntrinsicValue",
        "498": "ShareBasedCompensationArrangementByShareBasedPaymentAwardOptionsExercisesInPeriodWeightedAverageExercisePrice",
        "499": "ShareBasedCompensationArrangementByShareBasedPaymentAwardOptionsForfeituresAndExpirationsInPeriodWeightedAverageExercisePrice",
        "500": "ShareBasedCompensationArrangementByShareBasedPaymentAwardOptionsForfeituresInPeriod",
        "501": "ShareBasedCompensationArrangementByShareBasedPaymentAwardOptionsForfeituresInPeriodWeightedAverageExercisePrice",
        "502": "ShareBasedCompensationArrangementByShareBasedPaymentAwardOptionsGrantsInPeriod",
        "503": "ShareBasedCompensationArrangementByShareBasedPaymentAwardOptionsGrantsInPeriodGross",
        "504": "ShareBasedCompensationArrangementByShareBasedPaymentAwardOptionsGrantsInPeriodWeightedAverageExercisePrice",
        "505": "ShareBasedCompensationArrangementByShareBasedPaymentAwardOptionsGrantsInPeriodWeightedAverageGrantDateFairValue",
        "506": "ShareBasedCompensationArrangementByShareBasedPaymentAwardOptionsOtherIncreasesDecreasesInPeriod",
        "507": "ShareBasedCompensationArrangementByShareBasedPaymentAwardOptionsOutstandingIntrinsicValue",
        "508": "ShareBasedCompensationArrangementByShareBasedPaymentAwardOptionsOutstandingNumber",
        "509": "ShareBasedCompensationArrangementByShareBasedPaymentAwardOptionsOutstandingWeightedAverageExercisePrice",
        "510": "ShareBasedCompensationArrangementByShareBasedPaymentAwardOptionsOutstandingWeightedAverageRemainingContractualTerm",
        "511": "ShareBasedCompensationArrangementByShareBasedPaymentAwardOptionsVestedAndExpectedToVestExercisableNumber",
        "512": "ShareBasedCompensationArrangementByShareBasedPaymentAwardOptionsVestedAndExpectedToVestOutstandingAggregateIntrinsicValue",
        "513": "ShareBasedCompensationArrangementByShareBasedPaymentAwardOptionsVestedAndExpectedToVestOutstandingNumber",
        "514": "ShareBasedCompensationArrangementByShareBasedPaymentAwardOptionsVestedAndExpectedToVestOutstandingWeightedAverageExercisePrice",
        "515": "ShareBasedCompensationArrangementByShareBasedPaymentAwardOptionsVestedAndExpectedToVestOutstandingWeightedAverageRemainingContractualTerm",
        "516": "ShareBasedCompensationArrangementByShareBasedPaymentAwardOptionsVestedInPeriodFairValue",
        "517": "ShareBasedCompensationArrangementByShareBasedPaymentAwardPerShareWeightedAveragePriceOfSharesPurchased",
        "518": "ShareBasedCompensationArrangementsByShareBasedPaymentAwardOptionsExercisesInPeriodWeightedAverageExercisePrice",
        "519": "ShareBasedCompensationArrangementsByShareBasedPaymentAwardOptionsForfeituresInPeriodWeightedAverageExercisePrice",
        "520": "ShareBasedCompensationArrangementsByShareBasedPaymentAwardOptionsGrantsInPeriodWeightedAverageExercisePrice",
        "521": "SharebasedCompensationArrangementBySharebasedPaymentAwardAwardVestingRightsPercentage",
        "522": "SharebasedCompensationArrangementBySharebasedPaymentAwardOptionsExercisableIntrinsicValue1",
        "523": "SharebasedCompensationArrangementBySharebasedPaymentAwardOptionsOutstandingWeightedAverageRemainingContractualTerm1",
        "524": "SharebasedCompensationArrangementBySharebasedPaymentAwardOptionsVestedInPeriodFairValue1",
        "525": "SharebasedCompensationArrangementBySharebasedPaymentAwardPurchasePriceOfCommonStockPercent",
        "526": "SharebasedCompensationSharesAuthorizedUnderStockOptionPlansExercisePriceRangeExercisableOptionsWeightedAverageRemainingContractualTerm1",
        "527": "SharePrice",
        "528": "SharesPaidForTaxWithholdingForShareBasedCompensation",
        "529": "StockGrantedDuringPeriodValueSharebasedCompensationGross",
        "530": "StockholdersEquity",
        "531": "StockholdersEquityNoteStockSplitConversionRatio1",
        "532": "StockIssuedDuringPeriodSharesEmployeeStockPurchasePlans",
        "533": "StockIssuedDuringPeriodSharesNewIssues",
        "534": "StockIssuedDuringPeriodSharesStockOptionsExercised",
        "535": "StockIssuedDuringPeriodValueEmployeeStockOwnershipPlan",
        "536": "StockIssuedDuringPeriodValueShareBasedCompensation",
        "537": "StockOptionPlanExpense",
        "538": "StockRepurchasedAndRetiredDuringPeriodShares",
        "539": "StockRepurchasedAndRetiredDuringPeriodValue",
        "540": "StockRepurchasedDuringPeriodShares",
        "541": "StockRepurchasedDuringPeriodValue",
        "542": "StockRepurchaseProgramAuthorizedAmount",
        "543": "StockRepurchaseProgramAuthorizedAmount1",
        "544": "StockRepurchaseProgramRemainingAuthorizedRepurchaseAmount",
        "545": "StockRepurchaseProgramRemainingAuthorizedRepurchaseAmount1",
        "546": "TaxBenefitFromStockOptionsExercised",
        "547": "TaxBenefitFromStockOptionsExercised1",
        "548": "TaxCutsAndJobsActOf2017IncomeTaxExpenseBenefit",
        "549": "TaxesPayableCurrent",
        "550": "TemporaryEquityValueExcludingAdditionalPaidInCapital",
        "551": "TreasuryStockAcquiredAverageCostPerShare",
        "552": "TreasuryStockShares",
        "553": "TreasuryStockSharesAcquired",
        "554": "TreasuryStockValue",
        "555": "TreasuryStockValueAcquiredCostMethod",
        "556": "UndistributedEarningsOfForeignSubsidiaries",
        "557": "UnrealizedGainLossOnCashFlowHedgingInstruments",
        "558": "UnrealizedGainLossOnDerivatives",
        "559": "UnrealizedGainLossOnSecurities",
        "560": "UnrecognizedTaxBenefits",
        "561": "UnrecognizedTaxBenefitsDecreasesResultingFromPriorPeriodTaxPositions",
        "562": "UnrecognizedTaxBenefitsDecreasesResultingFromSettlementsWithTaxingAuthorities",
        "563": "UnrecognizedTaxBenefitsIncomeTaxPenaltiesAndInterestAccrued",
        "564": "UnrecognizedTaxBenefitsIncreasesResultingFromCurrentPeriodTaxPositions",
        "565": "UnrecognizedTaxBenefitsIncreasesResultingFromPriorPeriodTaxPositions",
        "566": "UnrecognizedTaxBenefitsReductionsResultingFromLapseOfApplicableStatuteOfLimitations",
        "567": "UnrecognizedTaxBenefitsThatWouldImpactEffectiveTaxRate",
        "568": "ValuationAllowancesAndReservesDeductions",
        "569": "WeightedAverageNumberDilutedSharesOutstandingAdjustment",
        "570": "WeightedAverageNumberOfDilutedSharesOutstanding",
        "571": "WeightedAverageNumberOfSharesOutstandingBasic",
        "572": "CapitalExpendituresIncurredButNotYetPaid",
        "573": "DeferredTaxAssetsOther",
        "574": "TreasuryStockRetiredCostMethodAmount",
        "575": "TreasuryStockSharesRetired",
        "576": "FinitelivedIntangibleAssetsAcquired1",
        "577": "InventoryWriteDown",
        "578": "ProductWarrantyAccrualPeriodIncreaseDecrease",
        "579": "ProductWarrantyAccrualPreexistingIncreaseDecrease",
        "580": "DeferredTaxLiabilitiesGoodwillAndIntangibleAssetsIntangibleAssets",
        "581": "EffectiveIncomeTaxRateReconciliationForeignIncomeTaxRateDifferential",
        "582": "EffectiveIncomeTaxRateReconciliationNondeductibleExpenseShareBasedCompensationCost",
        "583": "EffectiveIncomeTaxRateReconciliationOtherAdjustments",
        "584": "EffectiveIncomeTaxRateReconciliationStateAndLocalIncomeTaxes",
        "585": "EffectiveIncomeTaxRateReconciliationTaxCreditsResearch",
        "586": "PurchaseObligationDueAfterFifthYear",
        "587": "PurchaseObligationDueInFifthYear",
        "588": "UnrecognizedTaxBenefitsIncomeTaxPenaltiesAndInterestExpense",
        "589": "RestrictedCashCurrent",
        "590": "IncomeTaxExaminationIncreaseDecreaseInLiabilityFromPriorYear",
        "591": "EffectiveIncomeTaxRateReconciliationFdiiAmount",
        "592": "EffectiveIncomeTaxRateReconciliationFdiiPercent",
        "593": "EquitySecuritiesWithoutReadilyDeterminableFairValueAmount",
        "594": "EquitySecuritiesWithoutReadilyDeterminableFairValueUpwardPriceAdjustmentAnnualAmount",
        "595": "EquitySecuritiesWithoutReadilyDeterminableFairValueUpwardPriceAdjustmentCumulativeAmount",
        "596": "FinanceLeaseRightOfUseAssetAccumulatedAmortization",
        "597": "ProceedsFromStockPlans",
        "598": "BusinessCombinationConsiderationTransferred1",
        "599": "EquitySecuritiesFVNINoncurrent",
        "600": "InterestExpenseNonoperating",
        "601": "PaymentsToAcquireEquitySecuritiesFvNi",
        "602": "ProceedsFromSaleOfEquitySecuritiesFvNi",
        "603": "UnbilledContractsReceivable",
        "604": "IncomeTaxesReceivableNoncurrent"
    }
}



#44 project

import json
from functools import cache
import requests
import pandas as pd
import plotly.express as px
import logging


def get_all_cik():
    headers = {
        'User-Agent': "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/42.0.2311.135 Safari/537.36 Edge/12.246",
        'Accept': 'application/json'
    }
    url = "https://www.sec.gov/files/company_tickers.json"
    response = requests.get(url, headers=headers)

    if "json" in response.headers.get('Content-Type'):
        logging.info(f"Content Type is: --------------------------------------- {response.headers.get('Content-Type')}")
        response = response.json()
        df = pd.DataFrame.from_dict(response).T
        df.rename(columns={"cik_str": "cik", "title": "NAME"}, inplace=True)
        # formatting CIK number
        df["cik"] = df.apply(lambda x: "CIK" + (10 - len(str(x['cik']))) * '0' + str(x['cik']), axis=1)
        return df

    else:
        logging.error(f"Content Type not json, but is: --------------------------------------- {response.headers.get('Content-Type')}")
        html_text_response = response.text
        logging.error(html_text_response)
        df = pd.DataFrame.from_dict(json.loads(html_text_response)).T
        df.rename(columns={"cik_str": "cik", "title": "NAME"}, inplace=True)
        # formatting CIK number
        df["cik"] = df.apply(lambda x: "CIK" + (10 - len(str(x['cik']))) * '0' + str(x['cik']), axis=1)
        return df


def create_spark_line(data, _height: int = 100, _width: int = 250):
    df = None
    if not isinstance(data, pd.DataFrame):
        df = pd.DataFrame(data)

    df = data

    fig = px.area(df, height=_height, width=_width)

    # hide and lock down axes
    fig.update_xaxes(visible=False, fixedrange=True)
    fig.update_yaxes(visible=False, fixedrange=True)

    # remove facet/subplot labels
    fig.update_layout(annotations=[], overwrite=True)

    # strip down the rest of the plot
    fig.update_layout(
        showlegend=False,
        plot_bgcolor="white",
        margin=dict(t=10, l=10, b=10, r=10))
    fig.update_traces(line_color="#32CD32")

    return fig.show()


def get_company_logo_url(name):
    url = 'https://s3-symbol-logo.tradingview.com/'
    name = str.lower(name)
    word_to_remove = [".com", "the ", " (the)", 'company', 'group']
    for item in word_to_remove:
        if item in name:
            name = name.replace(item, "")

    suffix = name.split()

    suffix_list = ["corp.", "corporation", "inc", 'incorporated', 'inc.', '(the)'
                                                                          "limited", "ltd", 'plc', "laboratories", "communications", 'the', "company", ".com", " company", "new", 'motor', 'ag']

    if suffix[-1] in suffix_list:
        name = name.replace(suffix[-1], '-big.svg')
        name = name.replace(' ', '-')
        name = name.replace('and', '-')
        name = name.replace('&', '-')
        name = name.replace("'", '')
        result = f"{url}{name}"
        # print(result)
        return result
    else:
        name = name + '--big.svg'
        name = name.replace(' ', '-')
        name = name.replace('and', '-')
        name = name.replace("'", '')
        name = name.replace('&', '-')

        result = f"{url}{name}"
        # check if the logo exists:
        status_code = requests.get(url=result).status_code
        if status_code == 200:
            print(result)
            return result
        else:
            return "https://placehold.co/600x400?text=Logo"


def request_company_filing(cik: str) -> json:
    # Get a copy of the default headers that requests would use
    #headers = requests.utils.default_headers()  # type: ignore
    # headers.update({'User-Agent': 'My User Agent 1.0', })  # type: ignore

    headers = {
        'User-Agent': 'My User Agent 1.0',
        'accept': 'application/json'
    }
    url = f"https://data.sec.gov/api/xbrl/companyfacts/{cik}.json"
    response = requests.get(url, headers=headers)
    return response.json()


if __name__ == "__main__":
    #print(get_all_cik())
    response = request_company_filing("CIK0000320193")
    accounting_norm_list = [x for x in [*response["facts"].keys()] if x not in ["srt", "invest", "dei"]]
    print(accounting_norm_list)

import logging

import requests
import pandas as pd
import plotly.express as px
import plotly.graph_objects as go
from plotly import plot
from functools import reduce
from FortyFour.Finance.utils import get_all_cik, request_company_filing


class Company:
    # Initial initialization method
    def __init__(self, ticker, cik=None):
        self.cik = cik
        if cik is None:
            self.ticker = str.upper(ticker)
            df = get_all_cik()
            df.set_index('ticker', inplace=True)
            df = df[df.index == self.ticker]
            self.cik = df["cik"].values[0]

        self.response = request_company_filing(self.cik)  # If facts is not available in the database (mongodb), then fetch from sec url

        # for example us-gaap or ifrs etc...
        accounting_norm_list = [x for x in [*self.response["facts"].keys()] if x not in ["srt", "invest", "dei"]]
        logging.info(f"Accounting Norms for {ticker}: {accounting_norm_list}")

        self.GAAP_NORM = accounting_norm_list[-1]

        company_name = self.response['entityName']
        self.company_name = company_name
        self.gaap_List = self.response['facts'][self.GAAP_NORM].keys()

    def compounding_annual_growth_rate(self, df, nb_years, inline_graph=False):
        # Check if the data is a dataframe:
        is_dataframe = isinstance(df, pd.DataFrame)
        if not isinstance(df, pd.DataFrame):
            df = pd.DataFrame(df)

        if len(df.index) >= (nb_years + 1):
            ending_value = df.iloc[-1][0]
            beginning_value = df.iloc[-nb_years - 1][0]
            try:
                result = round(
                    ((ending_value / beginning_value) ** (1 / nb_years) - 1), 3) * 100
            except Exception as e:
                logging.warning('The function compounding_annual_growth_rate() encounter an exception: ', e)
                result = 0

            fig = go.Figure(go.Indicator(
                domain={'x': [0, 1], 'y': [0, 1]},
                value=result,
                mode="gauge+number+delta",
                title={
                    'text': f"{self.company_name}<br><sup> {nb_years}-Years GAGR</sup>"},
                delta={'reference': 10},
                gauge={'axis': {'range': [-20, 20]},
                       'steps': [
                           {'range': [-20, 0], 'color': "pink"},
                           {'range': [0, 5], 'color': "gray"},
                           {'range': [5, 10], 'color': "lightcyan"},
                           {'range': [10, +20], 'color': "lightgreen"}
                       ],
                       'threshold': {'line': {'color': "red", 'width': 4}, 'thickness': 0.75, 'value': 10}}))

            if inline_graph:  # usefully in jupyter notebook
                fig.show()

            return fig
        else:
            fig = go.Figure(go.Indicator(
                domain={'x': [0, 1], 'y': [0, 1]},
                value=0,
                mode="gauge+number+delta",
                title={
                    'text': f"{self.company_name}<br><sup>No Data for {nb_years}-Years GAGR</sup>"},
                delta={'reference': 0},
                gauge={'axis': {'range': [0, 20]},
                       'steps': [

                       ],
                       }))

            return fig

    def Financials(self, selected_gaap: [], form_type=None):
        """Select a list of gaap and the function will return a dataframe ot the selected gaaps"""

        df_list_to_merge = []

        for gaap in selected_gaap:
            df = pd.DataFrame()
            try:  # try gaap norm
                gaap_unit = list(self.response['facts'][self.GAAP_NORM][gaap]['units'].keys())[-1]  # sometimes it can be multiple currencies so I select the last
                df = pd.DataFrame.from_records(self.response['facts'][self.GAAP_NORM][gaap]["units"][gaap_unit]).dropna()
            except Exception as e:  # try dei
                # sometimes it can be multiple currencies so I select the last
                gaap_unit = list(self.response['facts']["dei"][gaap]['units'].keys())[-1]
                df = pd.DataFrame.from_records(self.response['facts']["dei"][gaap]["units"][gaap_unit])
                logging.error(f"An exception occurred while retrieving {gaap}")
            df.rename(columns={'val': gaap, 'end': 'Date'}, errors="ignore", inplace=True)

            # We want to drop the column only if it exists by using errors='ignore'
            df.drop(['accn', 'fy', 'fp', 'form', 'filed', 'start'], errors='ignore', axis=1, inplace=True)

            # if "frame column exists":
            if "frame" in df.columns:
                df = df.dropna(axis=0)
                match form_type:
                    case "10-Q":
                        df = df[df["frame"].str.contains('Q')]
                        df.drop(['frame'], axis=1, inplace=True)
                        df_list_to_merge.append(df)
                    case "10-K":
                        df = df[~df["frame"].str.contains('Q')]
                        df.drop(["frame"], axis=1, inplace=True)
                        df_list_to_merge.append(df)

                    case "20-F":
                        df = df[~df["frame"].str.contains('Q')]
                        df.drop(["frame"], axis=1, inplace=True)
                        df_list_to_merge.append(df)

                    case "20-F/A":
                        df = df[~df["frame"].str.contains('Q')]
                        df.drop(["frame"], axis=1, inplace=True)
                        df_list_to_merge.append(df)

                    case _:
                        df.drop(["frame"], axis=1, inplace=True)
                        df_list_to_merge.append(df)
                        pass
                # df.set_index('Date', inplace=True)
            else:
                pass

        final_df = reduce(lambda left, right: pd.merge(left, right, on=['Date'], how='outer'), df_list_to_merge)

        final_df['Date'] = pd.to_datetime(final_df['Date'])
        final_df.set_index('Date', inplace=True)
        final_df.sort_values(by="Date", inplace=True)
        return final_df

    def CommonStockSharesOutstanding(self, form_type="10-K"):
        # don't show de result of gaaplist on screen, just use the result
        gaaplist = self.gaap_List

        synonyms = ['WeightedAverageNumberOfDilutedSharesOutstanding',
                    'CommonStockSharesIssued',
                    'EntityCommonStockSharesOutstanding',
                    'NumberOfSharesOutstanding',
                    'NumberOfSharesIssuedAndFullyPaid',
                    'WeightedAverageShares',
                    'CommonStockSharesOutstanding',
                    'NumberOfSharesIssuedAndFullyPaid']

        list = [item for item in synonyms if item in gaaplist]

        df = self.Financials(list, form_type=form_type)
        df['NumberOfSharesOutstanding'] = reduce(lambda x, y: x.combine_first(y), [df[col] for col in list])
        return df

    def MarketCap(self, form_type="10-Q"):
        df = self.Financials(["EntityPublicFloat"], form_type="10-Q")
        return df

    def Equity(self, form_type="10-K"):
        gaaplist = self.gaap_List

        synonyms = ['StockholdersEquity', 'Equity', 'EquityAttributableToOwnersOfParent', 'StockholdersEquityIncludingPortionAttributableToNoncontrollingInterest']

        list = [item for item in synonyms if item in gaaplist]

        df = self.Financials(list, form_type=form_type)
        df['Equity'] = reduce(lambda x, y: x.combine_first(y), [df[col] for col in list])
        return df

    def AssetsLiabilitiesEquity(self, form_type="10-Q", show_graph=False):
        df = pd.DataFrame()
        df['Assets'] = self.Financials(["Assets"], form_type=form_type)["Assets"]
        df["Equity"] = self.Equity(form_type=form_type)["Equity"]
        df["Liabilities"] = df['Assets'] - df["Equity"]
        df['LiabilitiesToAssetsRatio'] = df["Liabilities"] / df['Assets']

        fig = px.area(df, x=df.index, y=['LiabilitiesToAssetsRatio'],
                      title=f"{self.company_name}<br><sup>Liabilities To Assets Ratio</sup>",
                      line_shape="spline",
                      template="seaborn",
                      )

        fig.update_xaxes(showline=True, linewidth=1.5, linecolor='black')
        fig.update_yaxes(showline=True, linewidth=1.5, linecolor='black')
        fig.update_layout(margin=dict(l=0, r=0),
                          # width=1200,
                          # height=500,
                          yaxis_title=None, xaxis_title=None)
        fig.update_layout(legend=dict(
            orientation="h",
            yanchor="bottom",
            y=1.02,
            xanchor="right",
            x=1,

        ))
        fig.update_traces(line_color="darksalmon")
        # fig.update_layout({'plot_bgcolor': 'rgba(0,0,0,0)','paper_bgcolor': 'rgba(0,0,0,0)'})

        if show_graph == True:
            fig.show()

        return df, fig

    def Capex(self, form_type="10-K"):

        list = []
        # don't show de result of gaaplist on screen, just use the result
        gaaplist = self.gaap_List

        for item in ['PaymentsToAcquirePropertyPlantAndEquipment',
                     "PaymentsToAcquireOtherPropertyPlantAndEquipment",
                     'PurchaseOfPropertyPlantAndEquipmentClassifiedAsInvestingActivities',
                     "PaymentsToAcquireRealEstateHeldForInvestment",
                     "PaymentsToDevelopRealEstateAssets",
                     "PaymentsForCapitalImprovements",
                     'PaymentsToAcquireAndDevelopRealEstate',
                     "PaymentsToAcquireRealEstate",
                     "PaymentsToAcquireCommercialRealEstate",
                     "PaymentsToAcquireProductiveAssets",
                     "PurchaseOfPropertyPlantAndEquipmentIntangibleAssetsOtherThanGoodwillInvestmentPropertyAndOtherNoncurrentAssets",
                     "PurchaseOfPropertyPlantAndEquipmentAndIntangibleAssets",
                     'PurchasesOfPropertyAndEquipmentAndIntangibleAssets',
                     "PurchasesOfPropertyAndEquipmentAndIntangibleAssets"]:

            if item in gaaplist:
                list.append(item)

        df = self.Financials(list, form_type=form_type)
        # This line remove duplicates horizontally across columns
        df = df.apply(lambda x: pd.Series(x.unique()), axis=1)
        df['CAPEX'] = df.sum(axis=1)

        return df

    def PropertyPlantAndEquipmentGross(self, form_type="10-Q"):
        # don't show de result of gaaplist on screen, just use the result
        gaaplist = self.gaap_List

        synonyms = ["PropertyPlantAndEquipmentGross", 'PropertyPlantAndEquipment', "RealEstateInvestmentPropertyAtCost", "GrossInvestmentInRealEstateAssets"]
        list = [item for item in synonyms if item in gaaplist]

        df = self.Financials(list, form_type=form_type)
        df['PropertyPlantAndEquipmentGross'] = reduce(lambda x, y: x.combine_first(y), [df[col] for col in list])
        return df

    def PropertyPlantAndEquipmentNet(self, form_type="10-Q"):
        # don't show de result of gaaplist on screen, just use the result
        gaaplist = self.gaap_List

        synonyms = ["PropertyPlantAndEquipmentNet", 'RealEstateInvestmentPropertyNet', 'NetInvestmentInRealEstateAssets', 'PropertyPlantAndEquipment']
        list = [item for item in synonyms if item in gaaplist]
        df = self.Financials(list, form_type=form_type)
        df['PropertyPlantAndEquipmentNet'] = reduce(lambda x, y: x.combine_first(y), [df[col] for col in list])
        return df

    def PropertyPlantAndEquipmentGrossAndNet(self, form_type="10-K"):
        list = []
        gaaplist = self.gaap_List

        for item in ["PropertyPlantAndEquipmentGross", 'PropertyPlantAndEquipment', "PropertyPlantAndEquipmentNet",
                     "RealEstateInvestmentPropertyAtCost", "RealEstateInvestmentPropertyNet"]:
            if item in gaaplist:
                list.append(item)

        df = self.Financials(list, form_type=form_type)
        df['Ratio'] = df[list[-1]] / df[list[0]]
        df['Ratio'] = df['Ratio'].round(2)
        df.ffill(inplace=True)
        return df

    def Revenues(self, form_type='10-K', show_graph=False):

        gaaplist = self.gaap_List
        synonyms = ['RevenueFromContractWithCustomerExcludingAssessedTax', 'Revenues',
                    'SalesRevenueNet', 'Revenue', 'RevenueFromSaleOfGoods', 'RevenueFromContractsWithCustomers',
                    'NoninterestIncome']
        list = [item for item in synonyms if item in gaaplist]

        df = self.Financials(list, form_type=form_type)
        df['Revenues'] = reduce(lambda x, y: x.combine_first(y), [df[col] for col in list])

        fig = px.bar(df, x=df.index, y=['Revenues'],
                     title=f"{self.company_name}<br><sup>Revenues</sup>",
                     # line_shape="spline",
                     text_auto=True,
                     template="seaborn"
                     )
        fig.update_traces(textangle=0, textposition="outside", cliponaxis=False)
        fig.update_xaxes(showline=True, linewidth=1.5, linecolor='black')
        fig.update_yaxes(showline=True, linewidth=1.5, linecolor='black')
        fig.update_layout(margin=dict(l=0, r=0),
                          # width=1200,
                          height=500,
                          yaxis_title=None, xaxis_title=None)
        fig.update_layout(legend=dict(
            orientation="h",
            yanchor="bottom",
            y=1.02,
            xanchor="right",
            x=1,

        ))
        # fig.update_layout({'plot_bgcolor': 'rgba(0,0,0,0)','paper_bgcolor': 'rgba(0,0,0,0)'})

        if show_graph == True:
            fig.show()

        return df['Revenues'], fig

    def NetIncomeLoss(self, form_type='10-K'):

        # don't show de result of gaaplist on screen, just use the result
        gaaplist = self.gaap_List

        synonyms = ["NetIncomeLoss", 'ProfitLossAttributableToOwnersOfParent', "NetIncomeLossAvailableToCommonStockholdersBasic"]
        list = [item for item in synonyms if item in gaaplist]
        df = self.Financials(list, form_type=form_type)
        df['NetIncome'] = reduce(lambda x, y: x.combine_first(y), [df[col] for col in list])
        return df

    def OperatingIncomeLoss(self, form_type='10-K'):  # should be same as EBIT

        gaaplist = self.gaap_List
        synonyms = ["OperatingIncomeLoss", "IncomeLossFromOperationsBeforeIncomeTaxExpenseBenefit",
                    "IncomeLossFromContinuingOperationsBeforeIncomeTaxesExtraordinaryItemsNoncontrollingInterest",
                    "IncomeLossFromContinuingOperationsBeforeIncomeTaxesExtraordinaryItemsNoncontrollingInterest",
                    "IncomeLossFromContinuingOperationsBeforeIncomeTaxesMinorityInterestAndIncomeLossFromEquityMethodInvestments"]

        list = [item for item in synonyms if item in gaaplist]
        df = self.Financials(list, form_type=form_type)
        df['OperatingIncome'] = reduce(lambda x, y: x.combine_first(y), [df[col] for col in list])
        return df

    def ProfitMargin(self, form_type='10-K', show_graph=False):
        df = pd.DataFrame()
        df["Revenues"], fig = self.Revenues(form_type=form_type, show_graph=False)
        df["NetIncome"] = self.NetIncomeLoss(form_type=form_type)["NetIncome"]
        df["ProfitMargin"] = df["NetIncome"] / df["Revenues"]

        fig = px.area(df, x=df.index, y=['ProfitMargin'],
                      title=f"{self.company_name}<br><sup>Net Profit Margin</sup>",
                      line_shape="vh",
                      template="seaborn"
                      )

        fig.update_xaxes(showline=True, linewidth=1.5, linecolor='black')
        fig.update_yaxes(showline=True, linewidth=1.5, linecolor='black')
        fig.update_layout(margin=dict(l=0, r=0),
                          # width=1200,
                          height=500,
                          yaxis_title=None, xaxis_title=None)
        fig.update_layout(legend=dict(
            orientation="h",
            yanchor="bottom",
            y=1.02,
            xanchor="right",
            x=1,

        ))
        # fig.update_layout({'plot_bgcolor': 'rgba(0,0,0,0)','paper_bgcolor': 'rgba(0,0,0,0)'})

        # ------------------------------------------------------------------ ADD THE 44SCIENTIFICS LOGO
        fig.add_layout_image(
            dict(
                source="/Users/cheikhcamara/Documents/GitHub/3SIGMA/44 Scientifics logo.jpeg",
                xref="paper", yref="paper",
                x=1, y=1.05,
                sizex=0.2, sizey=0.2,
                xanchor="right", yanchor="bottom",
            )
        )

        if show_graph:
            fig.show()

        return df['ProfitMargin'], fig

    def PriceToEarningsRatio(self, form_type="10-K"):
        df = pd.DataFrame()
        df["NetIncome"] = self.NetIncomeLoss(form_type=form_type)["NetIncome"]
        df["Equity"] = self.Equity(form_type=form_type)["Equity"]
        # df = df.resample('Y').agg({'NetIncome': 'sum', 'Equity': 'last'}).head()
        df["PE"] = df["Equity"] / df["NetIncome"]
        return df

    def EPS(self, form_type="10-K", show_graph=False):

        gaaplist = self.gaap_List

        synonyms = ["EarningsPerShareDiluted"]
        list = [item for item in synonyms if item in gaaplist]
        df = pd.DataFrame()

        if len(list) >= 1:
            df = self.Financials(list, form_type=form_type)
            # This line removes duplicates horizontally across columns
            df = df.apply(lambda x: pd.Series(x.unique()), axis=1)
            df['EarningsPerShare'] = df.sum(axis=1)
        else:
            df = self.NetIncomeLoss(form_type=form_type)
            df["SharesOutstanding"] = self.CommonStockSharesOutstanding(form_type=form_type)
            df["EarningsPerShare"] = df["NetIncome"] / df["SharesOutstanding"]
            df["EarningsPerShare"].fillna(method='ffill', inplace=True)
            df["EarningsPerShare"] = df["EarningsPerShare"].round(2)

        fig = px.area(df, x=df.index, y=['EarningsPerShare'],
                      title=f"{self.company_name}<br><sup>Diluted Earning Per Share (EPS)</sup>",
                      line_shape="vh",
                      template="seaborn"
                      )
        if df['EarningsPerShare'].min() <= 0:
            fig.add_shape(  # add a horizontal "target" line
                type="line", line_color="salmon", line_width=2, opacity=1, line_dash="dot",
                x0=0, x1=1, xref="paper", y0=0.4, y1=0.4, yref="y")
        fig.update_xaxes(showline=True, linewidth=1.5, linecolor='black')
        fig.update_yaxes(showline=True, linewidth=1.5, linecolor='black')
        fig.update_layout(margin=dict(l=0, r=0),
                          # width=1200,
                          height=500,
                          yaxis_title=None, xaxis_title=None)
        fig.update_layout(legend=dict(
            orientation="h",
            yanchor="bottom",
            y=1.02,
            xanchor="right",
            x=1,

        ))
        # fig.update_layout({'plot_bgcolor': 'rgba(0,0,0,0)','paper_bgcolor': 'rgba(0,0,0,0)'})
        # ------------------------------------------------------------------ ADD THE 44SCIENTIFICS LOGO
        fig.add_layout_image(
            dict(
                source="/Users/cheikhcamara/Documents/GitHub/3SIGMA/44 Scientifics logo.jpeg",
                xref="paper", yref="paper",
                x=1, y=1.05,
                sizex=0.2, sizey=0.2,
                xanchor="right", yanchor="bottom",
            )
        )

        if show_graph == True:
            fig.show()

        return df['EarningsPerShare'], fig

    def DividendPerShare(self, form_type="10-K", show_graph=False):
        # don't show de result of gaaplist on screen, just use the result
        gaaplist = self.gaap_List
        synonyms = ["CommonStockDividendsPerShareCashPaid", "CommonStockDividendsPerShareDeclared", "DividendsRecognisedAsDistributionsToOwnersPerShare"]
        mlist = [item for item in synonyms if item in gaaplist]
        df = self.Financials(mlist, form_type=form_type)
        df['DividendPerShare'] = reduce(lambda x, y: x.combine_first(y), [df[col] for col in mlist])
        return df

    def PaymentsOfDividends(self, form_type="10-K", show_graph=False):
        # don't show de result of gaaplist on screen, just use the result
        gaaplist = self.gaap_List

        synonyms = ["PaymentsOfDividendsCommonStock", "PaymentsOfDividends", 'PaymentsOfOrdinaryDividends',
                    'DividendsPaidToEquityHoldersOfParentClassifiedAsFinancingActivities',
                    'DividendsPaidClassifiedAsFinancingActivities']
        list = [item for item in synonyms if item in gaaplist]
        df = self.Financials(list, form_type=form_type)
        df['PaymentsOfDividends'] = reduce(lambda x, y: x.combine_first(y), [df[col] for col in list])

        return df

    def DividendPayoutRatio(self, form_type="10-K", show_graph=False):
        df = self.DividendPerShare(form_type=form_type, show_graph=False)
        eps_df, fig = self.EPS(form_type=form_type)
        eps = eps_df.to_frame()
        df['EarningsPerShare'] = eps['EarningsPerShare']
        df['DividendPayoutRatio'] = df['DividendPerShare'] / df['EarningsPerShare']

        template = "seaborn"

        fig = px.area(df, x=df.index, y=['DividendPayoutRatio'],
                      title=f"{self.company_name}<br><sup>Dividend Payout Ratio</sup>",
                      line_shape="hv",  # "spline",
                      # width=1200,
                      template=template
                      )

        fig.update_xaxes(showline=True, linewidth=1.5, linecolor='black')
        fig.update_yaxes(showline=True, linewidth=1.5, linecolor='black')
        fig.update_layout(margin=dict(l=0, r=0),
                          yaxis_title=None, xaxis_title=None)
        fig.update_layout(legend=dict(
            orientation="h",
            yanchor="bottom",
            y=1.02, x=1,
            xanchor="right",
        ))
        # ------------------------------------------------------------------ ADD THE 44SCIENTIFICS LOGO
        fig.add_layout_image(
            dict(
                source="/Users/cheikhcamara/Documents/GitHub/3SIGMA/44 Scientifics logo.jpeg",
                xref="paper", yref="paper",
                x=0, y=1.05,
                sizex=0.2, sizey=0.2,
                xanchor="left", yanchor="bottom",
            )
        )

        if show_graph == True:
            fig.show()

        return df[["EarningsPerShare", "DividendPerShare", "DividendPayoutRatio"]]

    def DividendYield(self, form_type="10-K", show_graph=False):
        # don't show de result of gaaplist on screen, just use the result
        gaaplist = self.gaap_List

        synonyms = [
            "ShareBasedCompensationArrangementByShareBasedPaymentAwardFairValueAssumptionsExpectedDividendRate"]

        list = [item for item in synonyms if item in gaaplist]
        df = self.Financials(list, form_type=form_type)
        df['DividendYield'] = reduce(lambda x, y: x.combine_first(y), [df[col] for col in list])

        return df

    # def NextDividendDate(self):
    #     compani = reader.get_quote_yahoo(self.ticker)
    #     next_dividend_date = datetime.fromtimestamp(
    #         compani["dividendDate"][0] if "dividendDate" in compani.columns else 0)
    #     print(next_dividend_date)
    #     day = next_dividend_date.day
    #     month = next_dividend_date.month
    #     year = next_dividend_date.year
    #
    #     return day, month, year

    def PropertyPlantEQuipmentRatio(self, form_type="10-Q", show_graph=False):

        # create an Empty DataFrame object
        df = pd.DataFrame()
        df["PropertyPlantAndEquipmentGross"] = self.PropertyPlantAndEquipmentGross(form_type=form_type)["PropertyPlantAndEquipmentsGross"]
        df["PropertyPlantAndEquipmentNet"] = self.PropertyPlantAndEquipmentNet(form_type=form_type)["PropertyPlantAndEquipmentsNet"]
        df['Ratio'] = df["PropertyPlantAndEquipmentNet"] / df["PropertyPlantAndEquipmentGross"]

        template = "seaborn"

        fig = px.line(df, x=df.index, y=['Ratio'],
                      title=f"{self.company_name}<br><sup>Current State of the company's fixed assets</sup>",
                      line_shape="spline",
                      # width=1200,
                      template=template
                      )
        if df['Ratio'].min() <= 0.4:
            fig.add_shape(  # add a horizontal "target" line
                type="line", line_color="salmon", line_width=2, opacity=1, line_dash="dot",
                x0=0, x1=1, xref="paper", y0=0.4, y1=0.4, yref="y",
            )
        fig.update_xaxes(showline=True, linewidth=1.5, linecolor='black')
        fig.update_yaxes(showline=True, linewidth=1.5, linecolor='black')
        fig.update_layout(margin=dict(l=0, r=0), yaxis_title=None, xaxis_title=None)
        fig.update_layout(legend=dict(
            orientation="h",
            yanchor="bottom",
            y=1.02,
            xanchor="right",
            x=1,

        ))

        if show_graph == True:
            fig.show()

        return df, fig

    def CashFromOperatingActivities(self, form_type="10-K"):
        # don't show de result of gaaplist on screen, just use the result
        gaaplist = self.gaap_List

        synonyms = ['NetCashProvidedByUsedInOperatingActivitiesContinuingOperations',
                    'CashFlowsFromUsedInOperatingActivities', 'NetCashProvidedByUsedInOperatingActivities']
        list = [item for item in synonyms if item in gaaplist]
        df = self.Financials(list, form_type=form_type)
        df['CashFromOperatingActivities'] = reduce(lambda x, y: x.combine_first(y), [df[col] for col in list])

        return df

    def CostOfGoodsAndServicesSold(self, form_type="10-K"):
        gaaplist = self.gaap_List

        synonyms = ['CostOfGoodsAndServicesSold', 'CostOfRevenue']

        list = [item for item in synonyms if item in gaaplist]
        df = self.Financials(list, form_type=form_type)
        df['CostOfGoodsAndServicesSold'] = reduce(lambda x, y: x.combine_first(y), [df[col] for col in list])

        return df

    def CashCashEquivalents(self, form_type="10-K", show_graph=False):
        # don't show de result of gaaplist on screen, just use the result
        gaaplist = self.gaap_List
        synonyms = ['CashCashEquivalentsRestrictedCashAndRestrictedCashEquivalents',
                    'CashCashEquivalentsRestrictedCashAndRestrictedCashEquivalentsIncludingDisposalGroupAndDiscontinuedOperations',
                    'CashAndCashEquivalents']
        list = [item for item in synonyms if item in gaaplist]
        df = self.Financials(list, form_type=form_type)
        df['CashCashEquivalents'] = reduce(lambda x, y: x.combine_first(y), [df[col] for col in list])

        template = "seaborn"

        fig = px.bar(df, x=df.index, y=['CashCashEquivalents'],
                     title=f"{self.company_name}<br><sup>Cash & Cash Equivalents</sup>",
                     # line_shape="spline",
                     # width=1200,
                     template=template
                     )

        fig.update_xaxes(showline=True, linewidth=1.5, linecolor='black')
        fig.update_yaxes(showline=True, linewidth=1.5, linecolor='black')
        fig.update_layout(margin=dict(l=0, r=0),
                          yaxis_title=None, xaxis_title=None)
        fig.update_layout(legend=dict(
            orientation="h",
            yanchor="bottom",
            y=1.02,
            xanchor="right",
            x=1,

        ))

        if show_graph == True:
            fig.show()

        return df, fig

    def MarketableSecurities(self, form_type="10-K", show_graph=False):
        # don't show de result of gaaplist on screen, just use the result
        gaaplist = self.gaap_List

        synonyms = ['MarketableSecuritiesCurrent', 'ShortTermInvestments']

        list = [item for item in synonyms if item in gaaplist]
        df = self.Financials(list, form_type=form_type)
        df['MarketableSecurities'] = reduce(lambda x, y: x.combine_first(y), [df[col] for col in list])

        df1, figu = self.CashCashEquivalents(form_type=form_type, show_graph=False)
        df['Cash'] = df1['CashCashEquivalents']
        df["Cash and MarketableSecurities"] = df['Cash'] + df['MarketableAssets']
        result = pd.concat([df1, df])

        template = "seaborn"

        fig = px.bar(result, x=result.index, y=['MarketableAssets', 'CashCashEquivalents'],
                     title=f"{self.company_name}<br><sup>Marketable Securities and Cash</sup>",
                     # width=1200,
                     # text_auto='.2f',
                     text_auto=True,
                     # text="nation",
                     template=template
                     )
        fig.update_traces(textangle=0,
                          textposition="outside", cliponaxis=False)
        fig.update_xaxes(showline=True, linewidth=1.5, linecolor='black')
        fig.update_yaxes(showline=True, linewidth=1.5, linecolor='black')
        fig.update_layout(margin=dict(l=0, r=0),
                          yaxis_title=None, xaxis_title=None)
        fig.update_layout(legend=dict(
            orientation="h",
            yanchor="bottom",
            y=1.02,
            xanchor="right",
            x=1,

        ))

        if show_graph == True:
            fig.show()

        return result, fig

    def CashFlowFromFinancingActivities(self, form_type='10-K'):
        # don't show de result of gaaplist on screen, just use the result
        gaaplist = self.gaap_List

        synonyms = ['NetCashProvidedByUsedInFinancingActivitiesContinuingOperations',
                    'CashFlowsFromUsedInFinancingActivities', 'NetCashProvidedByUsedInFinancingActivities']

        list = [item for item in synonyms if item in gaaplist]
        df = self.Financials(list, form_type=form_type)
        df['CashFlowFromFinancingActivities'] = reduce(lambda x, y: x.combine_first(y), [df[col] for col in list])
        return df

    def CashFlowFromInvestingActivities(self, form_type='10-K'):
        # don't show de result of gaaplist on screen, just use the result
        gaaplist = self.gaap_List
        synonyms = ['NetCashProvidedByUsedInInvestingActivitiesContinuingOperations',
                    'CashFlowsFromUsedInInvestingActivities', 'NetCashProvidedByUsedInInvestingActivities']

        list = [item for item in synonyms if item in gaaplist]
        df = self.Financials(list, form_type=form_type)
        df['CashFlowFromInvestingActivities'] = reduce(lambda x, y: x.combine_first(y), [df[col] for col in list])
        return df

    def OperatingCash_VS_Capex(self, form_type="10-K", show_graph=False):
        df = self.CashFromOperatingActivities(form_type=form_type)
        capex = self.Capex(form_type=form_type)
        df['CAPEX'] = capex['CAPEX']

        #  ===================================================GRAPH
        fig = go.Figure()

        fig.add_trace(go.Line(x=df.index, y=df["CashFromOperatingActivities"],
                              name="Cash From Operating Activities", yaxis='y',
                              line_shape="hv",

                              )
                      )

        fig.add_trace(go.Line(x=df.index, y=df["CAPEX"],
                              name="Capex", yaxis="y2",
                              line_shape="hv",

                              )
                      )

        # Create axis objects
        fig.update_layout(
            autosize=False,
            width=1200,
            # height=500,
            yaxis=dict(
                title="Cash From Operating Activities",
                titlefont=dict(color="#1f77b4"),
                tickfont=dict(color="#1f77b4")),

            # create 2nd y axis
            yaxis2=dict(title="Capital Expenditure (CAPEX)", overlaying="y", side="right", position=1))

        fig.update_xaxes(showline=True, linewidth=1.5, linecolor='black')
        fig.update_yaxes(showline=True, linewidth=1.5, linecolor='black')

        fig.update_layout(
            template="seaborn",

            title=f"{self.company_name}<br><sup>Capex vs. Cash From Operating Activities</sup>",
            legend=dict(
                orientation="h",
                yanchor="bottom",
                y=1.02,
                xanchor="right",
                x=1
            ))

        if show_graph == True:
            fig.show()

        return df, fig

    def IncomeTaxRate(self, form_type="10-K"):
        gaaplist = self.gaap_List

        synonyms = ['EffectiveIncomeTaxRateContinuingOperations']
        list = [item for item in synonyms if item in gaaplist]
        df = self.Financials(list, form_type=form_type)
        df['IncomeTaxRate'] = reduce(lambda x, y: x.combine_first(y), [df[col] for col in list])
        return df

    def DepreciationAndAmortization(self, form_type="10-K"):
        # don't show de result of gaaplist on screen, just use the result
        gaaplist = self.gaap_List
        synonyms = ['DepreciationAndAmortization', 'DepreciationAndAmortizationExcludingNuclearFuel',
                    "DepreciationDepletionAndAmortization"]

        list = [item for item in synonyms if item in gaaplist]
        df = self.Financials(list, form_type=form_type)
        df['DepreciationAndAmortization'] = reduce(lambda x, y: x.combine_first(y), [df[col] for col in list])
        return df

    def DebtInterestRate(self, form_type="10-K"):
        list = []
        # don't show de result of gaaplist on screen, just use the result
        gaaplist = self.gaap_List

        synonyms = ["DebtWeightedAverageInterestRate", "LongtermDebtWeightedAverageInterestRate",
                    'ShortTermDebtWeightedAverageInterestRate']
        list = [item for item in synonyms if item in gaaplist]
        df = self.Financials(list, form_type=form_type)
        df['DebtInterestRate'] = reduce(lambda x, y: x.combine_first(y), [df[col] for col in list])
        return df

    def InterestExpense(self, form_type="10-K"):
        list = []
        # don't show de result of gaaplist on screen, just use the result
        gaaplist = self.gaap_List

        synonyms = ["InterestExpense", "InterestExpenseNetOfHedgeIneffectiveness"]
        list = [item for item in synonyms if item in gaaplist]
        df = self.Financials(list, form_type=form_type)
        df['InterestExpense'] = reduce(lambda x, y: x.combine_first(y), [df[col] for col in list])
        return df

    def IncomeTaxes(self, form_type='10-K'):
        list = []
        # don't show de result of gaaplist on screen, just use the result
        gaaplist = self.gaap_List
        # IncomeTaxExpenseBenefit
        synonyms = ['IncomeTaxExpenseBenefit',
                    # "IncomeTaxesPaidNet"
                    ]
        list = [item for item in synonyms if item in gaaplist]
        df = self.Financials(list, form_type=form_type)
        df['IncomeTaxes'] = reduce(lambda x, y: x.combine_first(y), [df[col] for col in list])
        return df

    def CapitalEmployed(self, form_type="10-Q"):
        gaaplist = self.gaap_List

        synonyms = ['Assets', 'LiabilitiesCurrent']

        list = [item for item in synonyms if item in gaaplist]

        df = self.Financials(list, form_type=form_type)
        df["Capital Employed"] = df[df.columns[0]] - df[df.columns[-1]]
        return df

    def DebtRepayment(self, form_type="10-K"):
        # don't show de result of gaaplist on screen, just use the result
        gaaplist = self.gaap_List
        synonyms = ["RepaymentsOfLongTermDebt"]

        list = [item for item in synonyms if item in gaaplist]
        df = self.Financials(list, form_type=form_type)
        df['DebtRepayment'] = reduce(lambda x, y: x.combine_first(y), [df[col] for col in list])
        return df

    def LongTermDebt(self, form_type="10-K", show_graph=False):
        # don't show de result of gaaplist on screen, just use the result
        gaaplist = self.gaap_List
        synonyms = ['NoncurrentFinancialLiabilities', 'LongtermBorrowings', 'LongTermDebtNoncurrent', 'LongTermDebt',
                    "LongTermDebtAndCapitalLeaseObligations", "OtherLiabilitiesNoncurrent"]
        # ["DebtInstrumentCarryingAmount"]

        list = [item for item in synonyms if item in gaaplist]
        df = self.Financials(list, form_type=form_type)
        df['LongTermDebt'] = reduce(lambda x, y: x.combine_first(y), [df[col] for col in list])

        fig = px.area(df, x=df.index, y=['TotalLongTermDebt'],
                      title=f"{self.company_name}<br><sup>Long-Term Debt</sup>",
                      line_shape="hv",
                      # width=1200,
                      template="seaborn"
                      )

        fig.update_xaxes(showline=True, linewidth=1.5, linecolor='black')
        fig.update_yaxes(showline=True, linewidth=1.5, linecolor='black')
        fig.update_layout(margin=dict(l=0, r=0),
                          yaxis_title=None, xaxis_title=None)
        fig.update_layout(legend=dict(
            orientation="h",
            yanchor="bottom",
            y=1.02,
            xanchor="right",
            x=1,

        ))

        if show_graph == True:
            fig.show()

        return df, fig

    def ShortTermDebt(self, form_type="10-K", show_graph=False):
        gaaplist = self.gaap_List

        synonyms = ["TradeAndOtherCurrentPayables", "AccountsPayableCurrent", "EmployeeRelatedLiabilitiesCurrent",
                    "CurrentTaxLiabilitiesCurrent", "AccruedIncomeTaxesCurrent", "OtherShorttermProvisions",
                    "LiabilitiesIncludedInDisposalGroupsClassifiedAsHeldForSale",
                    "ContractWithCustomerLiabilityCurrent", "AccountsPayableAndAccruedLiabilitiesCurrent",
                    "AccruedLiabilitiesCurrent", "AccruedRebatesReturnsAndPromotions", "OtherLiabilitiesCurrent"]
        list = [item for item in synonyms if item in gaaplist]

        df = self.Financials(list, form_type=form_type)
        df = df.apply(lambda x: pd.Series(x.unique()), axis=1)
        df["total_to_subtract"] = df.sum(axis=1)
        df2 = self.Financials(["LiabilitiesCurrent"], form_type=form_type)
        df2["ShortTermDebt"] = df2["LiabilitiesCurrent"] - df["total_to_subtract"]
        return df2

    def TotalDebt(self, form_type="10-Q"):
        TotalDebt = self.ShortTermDebt(form_type=form_type, show_graph=False)
        TotalDebt["LongTermDebt"], fig = self.LongTermDebt(
            form_type=form_type, show_graph=False)

        TotalDebt["TotalDebt"] = TotalDebt["LongTermDebt"] + TotalDebt["ShortTermDebt"]

        return TotalDebt[["ShortTermDebt", "LongTermDebt", "TotalDebt"]]

    def ROE(self, form_type="10-Q"):

        match form_type:
            case "10-Q":
                df = self.AssetsLiabilitiesEquity(form_type=form_type)
                df['NetIncome'] = self.NetIncomeLoss(form_type=form_type)['NetIncome']
                df['ROE'] = df['NetIncome'] / df['Equity']
                return df
            case "10-K":
                df = self.AssetsLiabilitiesEquity(form_type="10-Q")
                df['NetIncome'] = self.NetIncomeLoss(form_type=form_type)['NetIncome']
                df['ROE'] = df['NetIncome'] / df['Equity']
                df.dropna(axis=0, inplace=True)
                return df
            case _:
                df = self.AssetsLiabilitiesEquity(form_type="10-Q")
                df['NetIncome'] = self.NetIncomeLoss(form_type=form_type)['NetIncome']
                df['ROE'] = df['NetIncome'] / df['Equity']
                df.dropna(axis=0, inplace=True)
                return df

    def ROCE(self, form_type='10-Q', show_graph=False):

        ebit = self.OperatingIncomeLoss(form_type="10-K")
        capitalEmployed = self.CapitalEmployed(form_type="10-Q")
        df = pd.concat([capitalEmployed["Capital Employed"], ebit["OperatingIncome"]], axis=1)
        df.dropna(axis=0, inplace=True)
        df['ROCE'] = df['OperatingIncome'] / df['Capital Employed']

        template = "seaborn"

        fig = px.area(df, x=df.index, y=['ROCE'],
                      title=f"{self.company_name}<br><sup>Return on Capital Employed (ROCE)</sup>",
                      line_shape="vh",
                      template=template
                      )

        fig.update_xaxes(showline=True, linewidth=1.5, linecolor='black')
        fig.update_yaxes(showline=True, linewidth=1.5, linecolor='black')
        fig.update_layout(margin=dict(l=0, r=0), yaxis_title=None, xaxis_title=None)
        fig.update_layout(legend=dict(
            orientation="h",
            yanchor="bottom",
            y=1.02,
            xanchor="right",
            x=1,
        ))
        if show_graph == True:
            fig.show()

        return df, fig

    def EBITDA(self, form_type="10-Q", show_graph=False):
        df = self.OperatingIncomeLoss(form_type=form_type)
        df2 = self.DepreciationAndAmortization(form_type=form_type)
        df["Amortization"] = df2["AmortizationAndDepreciation"]
        df["EBITDA"] = df["OperatingIncomeLoss"] + df["Amortization"]

        # PLOTLY FIGURE
        template = "seaborn"

        fig = px.bar(df, x=df.index, y=['EBITDA'],
                     title=f"{self.company_name}<br><sup>EBITDA</sup>",
                     # line_shape="spline",
                     template=template,
                     # width=1200
                     )

        fig.update_xaxes(showline=True, linewidth=1.5, linecolor='black')
        fig.update_yaxes(showline=True, linewidth=1.5, linecolor='black')
        fig.update_layout(margin=dict(l=0, r=0),
                          yaxis_title=None, xaxis_title=None)
        fig.update_layout(legend=dict(
            orientation="h",
            yanchor="bottom",
            y=1.02,
            xanchor="right",
            x=1,
        ))

        if show_graph == True:
            fig.show()

        figplot = plot(fig, output_type="div")
        return df, figplot

    def EBIT(self, form_type="10-K"):
        net_income = self.NetIncomeLoss(form_type=form_type)
        interest_expense = self.InterestExpense(form_type=form_type)
        income_tax = self.IncomeTaxes(form_type=form_type)
        df = pd.concat([net_income["NetIncome"], interest_expense["InterestExpense"], income_tax["IncomeTaxes"]], axis=1)

        df["EBIT"] = df.sum(axis=1)
        return df

    def WorkingCapital(self, form_type="10-Q", show_graph=False):
        list = []
        gaaplist = self.gaap_List

        for item in ["AssetsCurrent", 'LiabilitiesCurrent']:
            if item in gaaplist:
                list.append(item)

        df = self.Financials(list, form_type=form_type)
        # if form_type == "10-K":
        #    df = df.resample('BY').last()

        df["Working Capital"] = df["AssetsCurrent"] - df["LiabilitiesCurrent"]

        # PLOTLY FIGURE
        template = "seaborn"

        fig = px.line(df, x=df.index, y=['Working Capital'],
                      title=f"{self.company_name}<br><sup>Working Capital - Quarterely</sup>",
                      line_shape="spline",
                      template=template,
                      width=1200
                      )

        fig.update_xaxes(showline=True, linewidth=1.5, linecolor='black')
        fig.update_yaxes(showline=True, linewidth=1.5, linecolor='black')
        fig.update_layout(margin=dict(l=0, r=0),
                          yaxis_title=None, xaxis_title=None)
        fig.update_layout(legend=dict(
            orientation="h",
            yanchor="bottom",
            y=1.02,
            xanchor="right",
            x=1,
        ))

        if show_graph == True:
            fig.show()

        figplot = plot(fig, output_type="div")
        return df, figplot

    def FreeCashFlow(self, form_type='10-K', show_graph=False):
        df = self.CashFromOperatingActivities(form_type=form_type)
        df1 = self.Capex(form_type=form_type)
        df["CAPEX"] = df1['CAPEX']
        df["FreeCashFlow"] = df["CashFromOperatingActivities"] - df["CAPEX"]
        df['NumberOfSharesOutstanding'] = self.CommonStockSharesOutstanding(form_type=form_type)['NumberOfSharesOutstanding']
        df['NumberOfSharesOutstanding'].ffill(inplace=True)
        df["FreeCashFlowPerShare"] = df["FreeCashFlow"] / df['NumberOfSharesOutstanding']

        return df

    def EntrepriseValue(self, form_type='10-K', show_graph=False):
        list = []
        gaaplist = self.gaap_List

        for item in ["DepreciationDepletionAndAmortization", 'InterestExpense', 'NetIncomeLoss',
                     'IncomeTaxExpenseBenefit', 'InterestIncomeExpenseNet']:
            if item in gaaplist:
                list.append(item)

        df = self.Financials(list, form_type=form_type)
        df['EBITDA'] = df.sum(axis=1)
        return df

#sep api - adam getbags

# -*- coding: utf-8 -*-
"""

SEC Filing Scraper
@author: AdamGetbags

"""

# import modules
import requests
import pandas as pd

# create request header
headers = {'User-Agent': "email@address.com"}

# get all companies data
companyTickers = requests.get(
    "https://www.sec.gov/files/company_tickers.json",
    headers=headers
    )

# review response / keys
print(companyTickers.json().keys())

# format response to dictionary and get first key/value
firstEntry = companyTickers.json()['0']

# parse CIK // without leading zeros
directCik = companyTickers.json()['0']['cik_str']

# dictionary to dataframe
companyData = pd.DataFrame.from_dict(companyTickers.json(),
                                     orient='index')

# add leading zeros to CIK
companyData['cik_str'] = companyData['cik_str'].astype(
                           str).str.zfill(10)

# review data
print(companyData[:1])

cik = companyData[0:1].cik_str[0]

# get company specific filing metadata
filingMetadata = requests.get(
    f'https://data.sec.gov/submissions/CIK{cik}.json',
    headers=headers
    )

# review json 
print(filingMetadata.json().keys())
filingMetadata.json()['filings']
filingMetadata.json()['filings'].keys()
filingMetadata.json()['filings']['recent']
filingMetadata.json()['filings']['recent'].keys()

# dictionary to dataframe
allForms = pd.DataFrame.from_dict(
             filingMetadata.json()['filings']['recent']
             )

# review columns
allForms.columns
allForms[['accessionNumber', 'reportDate', 'form']].head(50)

# 10-Q metadata
allForms.iloc[11]

# get company facts data
companyFacts = requests.get(
    f'https://data.sec.gov/api/xbrl/companyfacts/CIK{cik}.json',
    headers=headers
    )

#review data
companyFacts.json().keys()
companyFacts.json()['facts']
companyFacts.json()['facts'].keys()

# filing metadata
companyFacts.json()['facts']['dei'][
    'EntityCommonStockSharesOutstanding']
companyFacts.json()['facts']['dei'][
    'EntityCommonStockSharesOutstanding'].keys()
companyFacts.json()['facts']['dei'][
    'EntityCommonStockSharesOutstanding']['units']
companyFacts.json()['facts']['dei'][
    'EntityCommonStockSharesOutstanding']['units']['shares']
companyFacts.json()['facts']['dei'][
    'EntityCommonStockSharesOutstanding']['units']['shares'][0]

# concept data // financial statement line items
companyFacts.json()['facts']['us-gaap']
companyFacts.json()['facts']['us-gaap'].keys()

# different amounts of data available per concept
companyFacts.json()['facts']['us-gaap']['AccountsPayable']
companyFacts.json()['facts']['us-gaap']['Revenues']
companyFacts.json()['facts']['us-gaap']['Assets']

# get company concept data
companyConcept = requests.get(
    (
    f'https://data.sec.gov/api/xbrl/companyconcept/CIK{cik}'
     f'/us-gaap/Assets.json'
    ),
    headers=headers
    )

# review data
companyConcept.json().keys()
companyConcept.json()['units']
companyConcept.json()['units'].keys()
companyConcept.json()['units']['USD']
companyConcept.json()['units']['USD'][0]

# parse assets from single filing
companyConcept.json()['units']['USD'][0]['val']

# get all filings data 
assetsData = pd.DataFrame.from_dict((
               companyConcept.json()['units']['USD']))

# review data
assetsData.columns
assetsData.form

# get assets from 10Q forms and reset index
assets10Q = assetsData[assetsData.form == '10-Q']
assets10Q = assets10Q.reset_index(drop=True)

# plot 
assets10Q.plot(x='end', y='val')




### invest programs
#!/usr/bin/python3
#     Programs called by invest.cgi
# 
#     Copyright (C)  2019 - 2024 Tom Stevelt
# 
#     This program is free software: you can redistribute it and/or modify
#     it under the terms of the GNU Affero General Public License as
#     published by the Free Software Foundation, either version 3 of the
#     License, or (at your option) any later version.
# 
#     This program is distributed in the hope that it will be useful,
#     but WITHOUT ANY WARRANTY; without even the implied warranty of
#     MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
#     GNU Affero General Public License for more details.
# 
#     You should have received a copy of the GNU Affero General Public License
#     along with this program.  If not, see <https://www.gnu.org/licenses/>.

import sys
import os
from datetime import datetime
import requests
import pandas as pd
import numpy as np
#import scipy as stats
from scipy.stats import linregress

Debug = 0

# 1 = update
# 2 = chart
# 3 = csv
# 4 = 10 year projected
Format = 1

if len(sys.argv) < 6:
	print ( "USAGE: FCF.py TICKER PRICE CIK FRAME UNIXTIME" )
	if Debug == 0:
		quit()
	
	ticker = 'BA'
	price = 195.00
	cik = '0000012927'
	
	ticker = 'IBM'
	price = 158.00
	cik = '0000051143'

	ticker = 'KVUE'
	price = 21.5
	cik = '0001944048'

	frame = 'CY1970Q'
	systime = 1699636971
else:
	ticker = sys.argv[1:]
	ticker = ticker[0]
	price = sys.argv[2:]
	price = float(price[0])
	cik = sys.argv[3:]
	cik = cik[0]
	frame = sys.argv[4:]
	frame = frame[0]
	systime = sys.argv[5:]
	systime = systime[0]

if Debug:
	print ( '-- ', ticker, price, cik, frame, systime)

# create request header
# get company facts data
headers = {'User-Agent': "tstevelt@silverhammersoftware.com"}
try:
	companyFacts = requests.get( f'https://data.sec.gov/api/xbrl/companyfacts/CIK{cik}.json', headers=headers )
except:
	print ( f'-- {ticker} {cik} no companyFacts' )
	quit()

if companyFacts.status_code != 200:
	print ( f'-- Stock {ticker} {cik} bad URL, status_code', companyFacts.status_code )
	print ( f'-- https://data.sec.gov/api/xbrl/companyfacts/CIK{cik}.json' )
	quit()
	
##
## Start up functions
##
def FormatOne ( FieldList ):
	for Field in FieldList:
		try:
			ndx = len(companyFacts.json()['facts']['dei'][f'{Field}']['units']['shares']) - 1
			Shares = companyFacts.json()['facts']['dei'][f'{Field}']['units']['shares'][ndx]['val']
			FiledDate = companyFacts.json()['facts']['dei'][f'{Field}']['units']['shares'][ndx]['filed']
			try:
				Frame = companyFacts.json()['facts']['dei'][f'{Field}']['units']['shares'][ndx]['frame']
				if Debug:
					print ( f'-- Format One: Found shares at {Field}' )
			except:
				Frame = "none"
			return Shares, Frame, FiledDate
		except:
			continue
	return 0, "none", "none"

def FormatTwo ( FieldList ):
	for Field in FieldList:
		try:
			ndx = len(companyFacts.json()['facts']['dei'][f'{Field}']['units']['USD']) - 1
			Shares = companyFacts.json()['facts']['dei'][f'{Field}']['units']['USD'][ndx]['val']
			FiledDate = companyFacts.json()['facts']['dei'][f'{Field}']['units']['USD'][ndx]['filed']
			try:
				Frame = companyFacts.json()['facts']['dei'][f'{Field}']['units']['USD'][ndx]['frame']
				if Debug:
					print ( f'-- Format Two: Found shares at {Field}' )
			except:
				Frame = "none"
			return Shares, Frame, FiledDate
		except:
			continue
	return 0, "none", "none"

def FormatThree ( FieldList, SubField ):
	for Field in FieldList:
		try:
			ndx = len(companyFacts.json()['facts']['us-gaap'][f'{Field}']['units'][f'{SubField}']) - 1
			Shares = companyFacts.json()['facts']['us-gaap'][f'{Field}']['units'][f'{SubField}'][ndx]['val']
			FiledDate = companyFacts.json()['facts']['us-gaap'][f'{Field}']['units'][f'{SubField}'][ndx]['filed']
			try:
				Frame = companyFacts.json()['facts']['us-gaap'][f'{Field}']['units'][f'{SubField}'][ndx]['frame']
				if Debug:
					print ( f'-- Format Three: Found shares at {Field}' )
			except:
				Frame = "none"
			return Shares, Frame, FiledDate
		except:
			continue
	return 0, "none", "none"

def days_between(d1, d2):
	d1 = datetime.strptime(d1, "%Y-%m-%d")
	d2 = datetime.strptime(d2, "%Y-%m-%d")
	return abs((d2 - d1).days)

##
## Start up -- get filed date and shares
##
FieldList = { 'EntityCommonStockSharesOutstanding', 'EntityPublicFloat', 'CommonStockSharesOutstanding', 'CommonStockSharesIssued', 'NumberOfSharesOutstanding', 'WeightedAverageNumberOfSharesOutstandingBasic' }
Shares, Frame, FiledDate = FormatOne ( FieldList )

if Frame == 'none':
	Shares, Frame, FiledDate = FormatTwo ( FieldList )

if Frame == 'none':
	Shares, Frame, FiledDate = FormatThree ( FieldList, 'shares' )

if Frame == 'none':
	print ( f'-- {ticker} unrecognizable json file' )
	print ( f'-- https://data.sec.gov/api/xbrl/companyfacts/CIK{cik}.json' )
	quit()
	
CurrentDate = datetime.today().strftime('%Y-%m-%d')

try:
	Days = days_between ( FiledDate, CurrentDate )
except:
	print ( f'-- {ticker} days_between failed' )
	print ( f'-- https://data.sec.gov/api/xbrl/companyfacts/CIK{cik}.json' )
	quit()

if Days < 100 and frame == Frame[0:8]:
    print ( f'-- No new filing for {ticker}' )
    quit ()
    
if frame > Frame[0:8]:
	if Debug:
		print ( f'-- Wacky {ticker} {frame} > {Frame}' )
    #quit ()
	Frame = frame

if Debug:
	print ( f'-- {ticker} Shares {Shares}, {Frame}, {FiledDate}' )

if Days > 100:
	print ( f'-- {ticker} processing old filed date {FiledDate}' )
	
##
## Functions for getting other data fields
##
def GetDataList ( Fields ):
	for Field in Fields:
		try:
			data = companyFacts.json()['facts']['us-gaap'][f'{Field}']['units']['USD']
			df = pd.DataFrame.from_dict(data)
			df = df.dropna()	
			if Debug:
				print ( f'-- Get Data List: found {Field} in USD' )
			return( df )
		except:
			try:
				data = companyFacts.json()['facts']['us-gaap'][f'{Field}']['units']['USD/shares']
				df = pd.DataFrame.from_dict(data)
				df = df.dropna()	
				if Debug:
					print ( f'-- Get Data List: found {Field} in USD/shares' )
				return( df )
			except:
				continue
	return pd.DataFrame()

def GetLatesetFiscalYear(df):
	Number = 0
	if df.empty:
		return Number
	Length = len(df)
	ndx = Length - 1
	while ndx >= 0:
		if 'fp' in df.columns and df.iloc[ndx]['fp'] == 'FY':
			Number = int(df.iloc[ndx]['val'])
			return Number
		ndx -= 1
	return Number

def MakeArray ( dfNetCash, dfCapEx ):
	Numbers = []
	if dfNetCash.empty:
		return Numbers
	lenNetCash = len(dfNetCash)
	ndx = 0
	xo = -1
	if dfCapEx.empty:
		lenCapEx = 0
	else:
		lenCapEx = len(dfCapEx)

	while ndx < lenNetCash:
		Numbers.append(dfNetCash.iloc[ndx]['val'])
		xo = xo + 1
		ThisDate = (dfNetCash.iloc[ndx]['end'])
		if not dfCapEx.empty:
			for xc in range(lenCapEx):
				if ThisDate == dfCapEx.iloc[xc]['end']:
					Numbers[xo] = Numbers[xo] - dfCapEx.iloc[xc]['val']
					break
		ndx = ndx + 1
	return Numbers

## 
## Get data fields
##

## 
## Free Cash Flow is commonly calculated as:
## 
## FCF=NetCashfromOperatingActivitiesCapitalExpendituresFCF=NetCashfromOperatingActivitiesCapitalExpenditures
## 
## Here are some XBRL tags that you might use:
## 
##     Net Cash Provided by Operating Activities:
##         XBRL Tag: us-gaap:NetCashProvidedByUsedInOperatingActivities
## 
##     Capital Expenditures (Capital Expenditures are typically part of Investing Activities):
##         XBRL Tag: us-gaap:PaymentsToAcquirePropertyPlantAndEquipmen/
## 
	
FieldList = { 'NetCashProvidedByUsedInOperatingActivities' }
dfNetCash = GetDataList ( FieldList )

if Debug:
	print ( f'{ticker} NetCashProvidedByUsedInOperatingActivities' )
	print ( dfNetCash.to_string() )

if not dfNetCash.empty:
	dfNetCash = dfNetCash.drop(dfNetCash[dfNetCash.fp != 'FY'].index)
else:
	print ( f'-- {ticker} missing NetCashProvidedByUsedInOperatingActivities' )
	quit()

NetCash = GetLatesetFiscalYear(dfNetCash)

FieldList = { 'PaymentsToAcquirePropertyPlantAndEquipment' }
dfCapEx = GetDataList ( FieldList )
if not dfCapEx.empty:
	dfCapEx = dfCapEx.drop(dfCapEx[dfCapEx.fp != 'FY'].index)
if Debug:
	print ( f'{ticker} PaymentsToAcquirePropertyPlantAndEquipment' )
	print ( dfCapEx.to_string() )
CapEx = GetLatesetFiscalYear(dfCapEx)

FreeCashFlow = NetCash - CapEx;
if Format == 2:
	print ( f'-- {ticker} NetCash {NetCash} - CapEx {CapEx} = FreeCashFlow {FreeCashFlow}')

FCF_Array = MakeArray ( dfNetCash, dfCapEx )

if len(FCF_Array) == 0:
	quit()

arr = np.array(FCF_Array)

xl = len(arr)
slope, intercept, r, p, std_err = 0.0, 0.0, 0.0, 0.0, 0.0
tangent = 0.0
MEAN = 0.0
CV = 0
if xl > 6:
	try:
		SD = np.std(FCF_Array)
		MEAN = np.mean(FCF_Array)
		if MEAN != 0:
			CV = SD / MEAN
		years = []
		value = 1
		for i in range(xl):
			years.append(value)
			value = value + 1
		slope, intercept, r, p, std_err = linregress(years, arr)
		if intercept != 0:
			tangent = slope / intercept
	except:
		slope, intercept, r, p, std_err = 0.0, 0.0, 0.0, 0.0, 0.0

if Format == 1 and FreeCashFlow != 0:
	tangent = tangent * 100.0
	if pd.isna(tangent) or np.isinf(tangent):
		tangent = 0.0;
	if pd.isna(CV) or np.isinf(CV):
		CV = 0.0;
	print ( f"update fundamental set Ffreecash = {FreeCashFlow}, Ffcfgrow = {tangent:.2f}, Ffcfcv = {CV:.2f}, Fupdated = {systime} where Fticker = '{ticker}';" );

if Format == 2:
	print ( f'-- {ticker} StdDev {SD:.0f}  Mean {MEAN:.0f}   CV {CV:.3f}' )
	print ( f'-- {ticker} slope {slope}, intercept {intercept}, r {r}, p {p}, std_err {std_err}' )
	x = 0
	for i in range(xl):
		x = years[i]
		y = intercept + slope * x
		print ( arr[i], ',',  y )
		
	for i in range(10):
		y = intercept + slope * (x + i + 1)
		print ( ' ', ',',  y )
	
if Format == 3 and xl > 8:
	print ( f'{ticker}, {xl}, {CV:.3f}, {slope:.0f}, {intercept:.0f}, {tangent:.3f}, {FreeCashFlow}' )

if Format == 4 and xl > 0:
	x = xl
	y0 = intercept + slope * (x)
	x = x + 1
	y1 = intercept + slope * (x)
	x = x + 1
	y2 = intercept + slope * (x)
	x = x + 1
	y3 = intercept + slope * (x)
	x = x + 1
	y4 = intercept + slope * (x)
	x = x + 1
	y5 = intercept + slope * (x)
	x = x + 1
	y6 = intercept + slope * (x)
	x = x + 1
	y7 = intercept + slope * (x)
	x = x + 1
	y8 = intercept + slope * (x)
	x = x + 1
	y9 = intercept + slope * (x)
	print ( f'{ticker}, {y0:.0f}, {y1:.0f}, {y2:.0f}, {y3:.0f}, {y4:.0f}, {y5:.0f}, {y6:.0f}, {y7:.0f}, {y8:.0f}, {y9:.0f}' )

#!/usr/bin/python3
#     Programs called by invest.cgi
# 
#     Copyright (C)  2019 - 2024 Tom Stevelt
# 
#     This program is free software: you can redistribute it and/or modify
#     it under the terms of the GNU Affero General Public License as
#     published by the Free Software Foundation, either version 3 of the
#     License, or (at your option) any later version.
# 
#     This program is distributed in the hope that it will be useful,
#     but WITHOUT ANY WARRANTY; without even the implied warranty of
#     MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
#     GNU Affero General Public License for more details.
# 
#     You should have received a copy of the GNU Affero General Public License
#     along with this program.  If not, see <https://www.gnu.org/licenses/>.

import sys
import os
from datetime import datetime
import requests
import pandas as pd
import numpy as np

if len(sys.argv) < 6:
	print ( "USAGE: FCF.py TICKER PRICE CIK FRAME UNIXTIME" )
	#fixit
	quit()
	
	ticker = 'BA'
	price = 195.00
	cik = '0000012927'
	
	ticker = 'IBM'
	price = 158.00
	cik = '0000051143'

	frame = 'CY1970Q'
	systime = 1699636971
else:
	ticker = sys.argv[1:]
	ticker = ticker[0]
	price = sys.argv[2:]
	price = float(price[0])
	cik = sys.argv[3:]
	cik = cik[0]
	frame = sys.argv[4:]
	frame = frame[0]
	systime = sys.argv[5:]
	systime = systime[0]

#fixit
Debug = 0
if Debug:
	print ( '-- ', ticker, price, cik, frame, systime)

# create request header
# get company facts data
headers = {'User-Agent': "tstevelt@silverhammersoftware.com"}
try:
	companyFacts = requests.get( f'https://data.sec.gov/api/xbrl/companyfacts/CIK{cik}.json', headers=headers )
except:
	print ( f'-- {ticker} {cik} no companyFacts' )
	quit()

if companyFacts.status_code != 200:
	print ( f'-- Stock {ticker} {cik} bad URL, status_code', companyFacts.status_code )
	print ( f'-- https://data.sec.gov/api/xbrl/companyfacts/CIK{cik}.json' )
	quit()
	
##
## Start up functions
##
def FormatOne ( FieldList ):
	for Field in FieldList:
		try:
			ndx = len(companyFacts.json()['facts']['dei'][f'{Field}']['units']['shares']) - 1
			Shares = companyFacts.json()['facts']['dei'][f'{Field}']['units']['shares'][ndx]['val']
			FiledDate = companyFacts.json()['facts']['dei'][f'{Field}']['units']['shares'][ndx]['filed']
			try:
				Frame = companyFacts.json()['facts']['dei'][f'{Field}']['units']['shares'][ndx]['frame']
				if Debug:
					print ( f'-- Format One: Found shares at {Field}' )
			except:
				Frame = "none"
			return Shares, Frame, FiledDate
		except:
			continue
	return 0, "none", "none"

def FormatTwo ( FieldList ):
	for Field in FieldList:
		try:
			ndx = len(companyFacts.json()['facts']['dei'][f'{Field}']['units']['USD']) - 1
			Shares = companyFacts.json()['facts']['dei'][f'{Field}']['units']['USD'][ndx]['val']
			FiledDate = companyFacts.json()['facts']['dei'][f'{Field}']['units']['USD'][ndx]['filed']
			try:
				Frame = companyFacts.json()['facts']['dei'][f'{Field}']['units']['USD'][ndx]['frame']
				if Debug:
					print ( f'-- Format Two: Found shares at {Field}' )
			except:
				Frame = "none"
			return Shares, Frame, FiledDate
		except:
			continue
	return 0, "none", "none"

def FormatThree ( FieldList, SubField ):
	for Field in FieldList:
		try:
			ndx = len(companyFacts.json()['facts']['us-gaap'][f'{Field}']['units'][f'{SubField}']) - 1
			Shares = companyFacts.json()['facts']['us-gaap'][f'{Field}']['units'][f'{SubField}'][ndx]['val']
			FiledDate = companyFacts.json()['facts']['us-gaap'][f'{Field}']['units'][f'{SubField}'][ndx]['filed']
			try:
				Frame = companyFacts.json()['facts']['us-gaap'][f'{Field}']['units'][f'{SubField}'][ndx]['frame']
				if Debug:
					print ( f'-- Format Three: Found shares at {Field}' )
			except:
				Frame = "none"
			return Shares, Frame, FiledDate
		except:
			continue
	return 0, "none", "none"

def days_between(d1, d2):
	d1 = datetime.strptime(d1, "%Y-%m-%d")
	d2 = datetime.strptime(d2, "%Y-%m-%d")
	return abs((d2 - d1).days)

##
## Start up -- get filed date and shares
##
FieldList = { 'EntityCommonStockSharesOutstanding', 'EntityPublicFloat', 'CommonStockSharesOutstanding', 'CommonStockSharesIssued', 'NumberOfSharesOutstanding', 'WeightedAverageNumberOfSharesOutstandingBasic' }
Shares, Frame, FiledDate = FormatOne ( FieldList )

if Frame == 'none':
	Shares, Frame, FiledDate = FormatTwo ( FieldList )

if Frame == 'none':
	Shares, Frame, FiledDate = FormatThree ( FieldList, 'shares' )

if Frame == 'none':
	print ( f'-- {ticker} unrecognizable json file' )
	print ( f'-- https://data.sec.gov/api/xbrl/companyfacts/CIK{cik}.json' )
	quit()
	
CurrentDate = datetime.today().strftime('%Y-%m-%d')

try:
	Days = days_between ( FiledDate, CurrentDate )
except:
	print ( f'-- {ticker} days_between failed' )
	print ( f'-- https://data.sec.gov/api/xbrl/companyfacts/CIK{cik}.json' )
	quit()

if Days < 100 and frame == Frame[0:8]:
    print ( f'-- No new filing for {ticker}' )
    quit ()
    
if frame > Frame[0:8]:
	if Debug:
		print ( f'-- Wacky {ticker} {frame} > {Frame}' )
    #quit ()
	Frame = frame

if Debug:
	print ( f'-- {ticker} Shares {Shares}, {Frame}, {FiledDate}' )

if Days > 100:
	print ( f'-- {ticker} processing old filed date {FiledDate}' )
	
##
## Functions for getting other data fields
##
def PrintLastTen ( df ):
	Length = len(df)
	if Length >= 15:
		print(df.iloc[Length-15:Length].to_string())
	else:
		print(df.to_string())

def GetDataList ( Fields ):
	for Field in Fields:
		try:
			data = companyFacts.json()['facts']['us-gaap'][f'{Field}']['units']['USD']
			df = pd.DataFrame.from_dict(data)
			df = df.dropna()	
			if Debug:
				print ( f'-- Get Data List: found {Field} in USD' )
			return( df )
		except:
			try:
				data = companyFacts.json()['facts']['us-gaap'][f'{Field}']['units']['USD/shares']
				df = pd.DataFrame.from_dict(data)
				df = df.dropna()	
				if Debug:
					print ( f'-- Get Data List: found {Field} in USD/shares' )
				return( df )
			except:
				continue
	return pd.DataFrame()

def GetLatesetFiscalYear(df):
	Number = 0
	if df.empty:
		return Number
	Length = len(df)
	ndx = Length - 1
	while ndx >= 0:
		if 'fp' in df.columns and df.iloc[ndx]['fp'] == 'FY':
			Number = int(df.iloc[ndx]['val'])
			return Number
		ndx -= 1
	return Number

## 
## Get data fields
##

## 
## Free Cash Flow is commonly calculated as:
## 
## FCF=NetCashfromOperatingActivitiesCapitalExpendituresFCF=NetCashfromOperatingActivitiesCapitalExpenditures
## 
## Here are some XBRL tags that you might use:
## 
##     Net Cash Provided by Operating Activities:
##         XBRL Tag: us-gaap:NetCashProvidedByUsedInOperatingActivities
## 
##     Capital Expenditures (Capital Expenditures are typically part of Investing Activities):
##         XBRL Tag: us-gaap:PaymentsToAcquirePropertyPlantAndEquipmen/
## 
	
FieldList = { 'NetCashProvidedByUsedInOperatingActivities' }
df = GetDataList ( FieldList )
if Debug:
	print ( f'{ticker} NetCashProvidedByUsedInOperatingActivities' )
	PrintLastTen ( df )
NetCash = GetLatesetFiscalYear(df)

FieldList = { 'PaymentsToAcquirePropertyPlantAndEquipment' }
df = GetDataList ( FieldList )
if Debug:
	print ( f'{ticker} PaymentsToAcquirePropertyPlantAndEquipment' )
	PrintLastTen ( df )
CapEx = GetLatesetFiscalYear(df)

FreeCashFlow = NetCash - CapEx;

print ( f'-- {ticker} NetCash {NetCash} - CapEx {CapEx} = FreeCashFlow {FreeCashFlow}')
if FreeCashFlow != 0:
	print ( f"update fundamental set Ffreecash = {FreeCashFlow}, Fupdated = {systime} where Fticker = '{ticker}';" );

#!/usr/bin/python3
#     Programs called by invest.cgi
# 
#     Copyright (C)  2019 - 2024 Tom Stevelt
# 
#     This program is free software: you can redistribute it and/or modify
#     it under the terms of the GNU Affero General Public License as
#     published by the Free Software Foundation, either version 3 of the
#     License, or (at your option) any later version.
# 
#     This program is distributed in the hope that it will be useful,
#     but WITHOUT ANY WARRANTY; without even the implied warranty of
#     MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
#     GNU Affero General Public License for more details.
# 
#     You should have received a copy of the GNU Affero General Public License
#     along with this program.  If not, see <https://www.gnu.org/licenses/>.

import sys
import os
from datetime import datetime
import requests
import pandas as pd

Debug = 0

if len(sys.argv) < 6:
	print ( "USAGE: getfundSEC.py TICKER PRICE CIK FRAME UNIXTIME" )
	if not Debug:
		quit()
	
	ticker = 'IBM'
	price = 158.00
	cik = '0000051143'

	ticker = 'SKW'
	price = 94.0
	cik = '0000093556'

	ticker = 'BA'
	price = 195.00
	cik = '0000012927'
	
	frame = 'CY1970Q'
	systime = 1699636971
else:
	ticker = sys.argv[1:]
	ticker = ticker[0]
	price = sys.argv[2:]
	price = float(price[0])
	cik = sys.argv[3:]
	cik = cik[0]
	frame = sys.argv[4:]
	frame = frame[0]
	systime = sys.argv[5:]
	systime = systime[0]

if Debug:
	print ( '-- ', ticker, price, cik, frame, systime)

# create request header
# get company facts data
headers = {'User-Agent': "tstevelt@silverhammersoftware.com"}
try:
	companyFacts = requests.get( f'https://data.sec.gov/api/xbrl/companyfacts/CIK{cik}.json', headers=headers )
except:
	print ( f'-- {ticker} {cik} no companyFacts' )
	quit()

if companyFacts.status_code != 200:
	print ( f'-- Stock {ticker} {cik} bad URL, status_code', companyFacts.status_code )
	print ( f'-- https://data.sec.gov/api/xbrl/companyfacts/CIK{cik}.json' )
	quit()
	
##
## Start up functions
##
def FormatOne ( FieldList ):
	for Field in FieldList:
		try:
			ndx = len(companyFacts.json()['facts']['dei'][f'{Field}']['units']['shares']) - 1
			Shares = companyFacts.json()['facts']['dei'][f'{Field}']['units']['shares'][ndx]['val']
			FiledDate = companyFacts.json()['facts']['dei'][f'{Field}']['units']['shares'][ndx]['filed']
			try:
				Frame = companyFacts.json()['facts']['dei'][f'{Field}']['units']['shares'][ndx]['frame']
				if Debug:
					print ( f'-- FormatOne: Found shares at {Field}' )
			except:
				Frame = "none"
			return Shares, Frame, FiledDate
		except:
			continue
	return 0, "none", "none"

def FormatTwo ( FieldList ):
	for Field in FieldList:
		try:
			ndx = len(companyFacts.json()['facts']['dei'][f'{Field}']['units']['USD']) - 1
			Shares = companyFacts.json()['facts']['dei'][f'{Field}']['units']['USD'][ndx]['val']
			FiledDate = companyFacts.json()['facts']['dei'][f'{Field}']['units']['USD'][ndx]['filed']
			try:
				Frame = companyFacts.json()['facts']['dei'][f'{Field}']['units']['USD'][ndx]['frame']
				if Debug:
					print ( f'-- FormatTwo: Found shares at {Field}' )
			except:
				Frame = "none"
			return Shares, Frame, FiledDate
		except:
			continue
	return 0, "none", "none"

def FormatThree ( FieldList, SubField ):
	for Field in FieldList:
		try:
			ndx = len(companyFacts.json()['facts']['us-gaap'][f'{Field}']['units'][f'{SubField}']) - 1
			Shares = companyFacts.json()['facts']['us-gaap'][f'{Field}']['units'][f'{SubField}'][ndx]['val']
			FiledDate = companyFacts.json()['facts']['us-gaap'][f'{Field}']['units'][f'{SubField}'][ndx]['filed']
			try:
				Frame = companyFacts.json()['facts']['us-gaap'][f'{Field}']['units'][f'{SubField}'][ndx]['frame']
				if Debug:
					print ( f'-- FormatThree: Found shares at {Field}' )
			except:
				Frame = "none"
			return Shares, Frame, FiledDate
		except:
			continue
	return 0, "none", "none"

def days_between(d1, d2):
	d1 = datetime.strptime(d1, "%Y-%m-%d")
	d2 = datetime.strptime(d2, "%Y-%m-%d")
	return abs((d2 - d1).days)

##
## Start up -- get filed date and shares
##
FieldList = { 'EntityCommonStockSharesOutstanding', 'EntityPublicFloat', 'CommonStockSharesOutstanding', 'CommonStockSharesIssued', 'NumberOfSharesOutstanding', 'WeightedAverageNumberOfSharesOutstandingBasic' }
Shares, Frame, FiledDate = FormatOne ( FieldList )

if Frame == 'none':
	Shares, Frame, FiledDate = FormatTwo ( FieldList )

if Frame == 'none':
	Shares, Frame, FiledDate = FormatThree ( FieldList, 'shares' )

if Frame == 'none':
	print ( f'-- {ticker} unrecognizable json file' )
	print ( f'-- https://data.sec.gov/api/xbrl/companyfacts/CIK{cik}.json' )
	quit()
	
CurrentDate = datetime.today().strftime('%Y-%m-%d')

try:
	Days = days_between ( FiledDate, CurrentDate )
except:
	print ( f'-- {ticker} days_between failed' )
	print ( f'-- https://data.sec.gov/api/xbrl/companyfacts/CIK{cik}.json' )
	quit()

if Days < 100 and frame == Frame[0:8]:
    print ( f'-- No new filing for {ticker}' )
    quit ()
    
if frame > Frame[0:8]:
	if Debug:
		print ( f'-- Wacky {ticker} {frame} > {Frame}' )
    #quit ()
	Frame = frame

if Debug:
	print ( f'-- {ticker} Shares {Shares}, {Frame}, {FiledDate}' )

if Days > 100:
	print ( f'-- {ticker} processing old filed date {FiledDate}' )
	
##
## Functions for getting other data fields
##
def PrintLastTen ( df ):
	Length = len(df)
	if Length >= 10:
		print(df.iloc[Length-10:Length])
	else:
		print(df)

SubFields = { 'USD', 'USD/shares', 'shares' }
def GetData ( Field ):
	for SubField in SubFields:
		try:
			data = companyFacts.json()['facts']['us-gaap'][f'{Field}']['units'][f'{SubField}']
			df = pd.DataFrame.from_dict(data)
			df = df.dropna()	
			return( df )
		except:
			continue
	return pd.DataFrame()

def GetDataList ( Fields ):
	for Field in Fields:
		try:
			data = companyFacts.json()['facts']['us-gaap'][f'{Field}']['units']['USD']
			df = pd.DataFrame.from_dict(data)
			df = df.dropna()	
			if Debug:
				print ( f'-- GetDataList: found {Field} in USD' )
			return( df )
		except:
			try:
				data = companyFacts.json()['facts']['us-gaap'][f'{Field}']['units']['USD/shares']
				df = pd.DataFrame.from_dict(data)
				df = df.dropna()	
				if Debug:
					print ( f'-- GetDataList: found {Field} in USD/shares' )
				return( df )
			except:
				continue
	return pd.DataFrame()

def GetValueTTM ( df ):
	if df.empty:
		return 0
	try:
		Length = len(df)
		if Length < 7:
			return 0
	except:
		return 0
	ndx = Length - 1
	Value = 0
	if df.iloc[ndx]['fp'] == 'FY':
		Value = df.iloc[ndx]['val']
	elif df.iloc[ndx]['fp'] == 'Q1':
		for x in range(ndx-1,Length):
			Value += df.iloc[x]['val']
		Value -= df.iloc[ndx-4]['val']
	elif df.iloc[ndx]['fp'] == 'Q2':
		for x in range(ndx-2,Length):
			Value += df.iloc[x]['val']
		Value -= df.iloc[ndx-4]['val']
		Value -= df.iloc[ndx-5]['val']
	elif df.iloc[ndx]['fp'] == 'Q3':
		for x in range(ndx-3,Length):
			Value += df.iloc[x]['val']
		Value -= df.iloc[ndx-4]['val']
		Value -= df.iloc[ndx-5]['val']
		Value -= df.iloc[ndx-6]['val']
	return Value

def GetValueLastFourQtr ( df ):
	if df.empty:
		return 0
	try:
		Length = len(df)
		if Length < 10:
			return 0
	except:
		return 0
	Value = 0
	Count = 0
	ndx = Length - 1
	try:
		while Count < 4 and ndx >= 0:
			if df.iloc[ndx]['fp'] == 'Q1' or df.iloc[ndx]['fp'] == 'Q2' or df.iloc[ndx]['fp'] == 'Q3' or df.iloc[ndx]['fp'] == 'Q4':
				Value += df.iloc[ndx]['val']
				Count += 1
			ndx -= 1
	except:
		print  ( f'GetValueLastFourQtr failed on {ticker}, length {Length}' );
	return Value

def GetValueLast ( df ):
	if df.empty:
		return 0
	try:
		Length = len(df)
	except:
		return 0
	Value = df.iloc[Length-1]['val']
	return Value

def GetQuarters(df):
	Numbers = []
	if df.empty:
		return Numbers
	Length = len(df)
	if Length < 4:
		return Numbers
	ndx = Length - 1
	while ndx >=4:
		if df.iloc[ndx]['fp'] == 'FY':
			QtrValue = df.iloc[ndx]['val'] - (df.iloc[ndx-1]['val'] + df.iloc[ndx-2]['val']  + df.iloc[ndx-3]['val'])
			Numbers.append(int(QtrValue))
		else:
			Numbers.append(df.iloc[ndx]['val'])
		ndx = ndx - 1
	return Numbers

def CalcYoYPercent (Numbers):
	Length = len(Numbers)
	if Length < 5:
		return 0.0
	if Numbers[4] <= 0 or Numbers[0] <= 0:
		return (0.0)
	Answer = 100.0 * (Numbers[0] - Numbers[4]) / Numbers[4]
	return Answer
	
def GetLatesetFiscalYear(df):
	Number = 0
	if df.empty:
		return Number
	Length = len(df)
	ndx = Length - 1
	while ndx >= 0:
		if 'fp' in df.columns and df.iloc[ndx]['fp'] == 'FY':
			Number = int(df.iloc[ndx]['val'])
			return Number
		ndx -= 1
	return Number

## 
## Get data fields
##
if Shares < 1000:
	df = GetData('CommonStockSharesIssued')
	if Debug > 1:
		print ( df )
	Shares = GetValueLast ( df )
	
df = GetDataList ( {'EarningsPerShareDiluted','EarningsPerShareBasic'} )
if Debug > 1:
	PrintLastTen ( df )
EPS = GetValueTTM ( df )

MyArray = GetQuarters ( df )
EarnGrowPct = CalcYoYPercent(MyArray)
if Debug:
	print ( f'MyArray {MyArray}')
	print ( f'EarnGrowPct {EarnGrowPct}')

FieldList = { 'Revenues', 'RevenuesNetOfInterestExpense',  'RevenueFromContractWithCustomerExcludingAssessedTax' }
df = GetDataList ( FieldList )
if Debug > 1:
	PrintLastTen ( df )
Revenue = GetValueTTM ( df )
if Revenue <= 0:
	Revenue = GetValueLastFourQtr ( df )

MyArray = GetQuarters ( df )
RevenueGrowPct = CalcYoYPercent(MyArray)
if Debug:
	print ( f'MyArray {MyArray}')
	print ( f'RevenueGrowPct {RevenueGrowPct}')

df = GetData ( 'Assets' )
Assets = GetValueLast ( df )

df = GetDataList({'StockholdersEquity','StockholdersEquityIncludingPortionAttributableToNoncontrollingInterest'})
StockholdersEquity = GetValueLast(df)

df = GetData('LiabilitiesAndStockholdersEquity')
LiabilitiesAndStockholdersEquity = GetValueLast(df)

df = GetData('LongTermDebt')
LongTermDebt = GetValueLast(df)

# df = GetData('IncomeLossFromContinuingOperations')
#df = GetData('OperatingIncomeLoss')
#Earnings = GetValueLast ( df )

df = GetData('InterestExpense')
Interest = GetValueTTM ( df )

FieldList = { 'IncomeTaxesPaid', 'IncomeTaxesPaidNet' }
df = GetDataList(FieldList)
Taxes  = GetValueTTM ( df )

FieldList = { 'Depreciation', 'DepreciationAndAmortization', 'NetIncomeLossFromContinuingOperationsAvailableToCommonShareholdersBasic' }
df = GetDataList(FieldList)
Depreciation = GetValueTTM ( df )

df = GetData ( 'AmortizationOfFinancingCosts' )
AmortFinance = GetValueTTM ( df )

df = GetData ( 'AmortizationOfIntangibleAssets' )
AmortIntangible = GetValueTTM ( df )

df = GetData('MinorityInterest')
MinorityInterest = GetValueLast ( df )

df = GetData('LongTermDebtNoncurrent')
LongTermDebtNoncurrent = GetValueLast ( df )

df = GetData('OperatingLeaseLiabilityCurrent')
OperatingLeaseLiabilityCurrent = GetValueLast ( df )

df = GetData('AccountsPayableCurrent')
AccountsPayableCurrent = GetValueLast ( df )

df = GetData('CashAndCashEquivalentsAtCarryingValue')
CashEquiv = GetValueLast ( df )

FieldList = { 'NetIncomeLoss', 'NetIncomeLossAvailableToCommonStockholdersDiluted' }
df = GetDataList ( FieldList )
NetIncome = GetValueTTM ( df )

df = GetData ( 'AssetsCurrent' )
AssetsCurrent = GetValueLast ( df )

df = GetData ( 'TreasuryStockShares' )
TreasuryStockShares = GetValueLast ( df )

FieldList = { 'Liabilities', 'LiabilitiesCurrent' }
df = GetDataList ( FieldList )
Liabilities = GetValueLast ( df )

FieldList = { 'NetCashProvidedByUsedInOperatingActivities' }
df = GetDataList ( FieldList )
NetCash = GetLatesetFiscalYear(df)

FieldList = { 'PaymentsToAcquirePropertyPlantAndEquipment' }
df = GetDataList ( FieldList )
CapEx = GetLatesetFiscalYear(df)

if Debug:
	print ( '-- After extract all data' )
	print ( f'-- Shares {Shares}' )
	print ( f'-- EPS {EPS}' )
	print ( f'-- Assets {Assets}' )
	print ( f'-- StockholdersEquity {StockholdersEquity}' )
	print ( f'-- LiabilitiesAndStockholdersEquity {LiabilitiesAndStockholdersEquity}' )
	print ( f'-- LongTermDebt {LongTermDebt}' )
	print ( f'-- LongTermDebtNoncurrent {LongTermDebtNoncurrent}' )
	print ( f'-- Interest {Interest}' )
	print ( f'-- Taxes {Taxes}' )
	print ( f'-- Depreciation {Depreciation}' )
	print ( f'-- AmortFinance {AmortFinance}' )
	print ( f'-- AmortIntangible {AmortIntangible}' )
	print ( f'-- MinorityInterest {MinorityInterest}' )
	print ( f'-- OperatingLeaseLiabilityCurrent {OperatingLeaseLiabilityCurrent}' )
	print ( f'-- AccountsPayableCurrent {AccountsPayableCurrent}' )
	print ( f'-- CashEquiv {CashEquiv}' )
	print ( f'-- NetIncome {NetIncome}' )
	print ( f'-- Revenue {Revenue}' )
	print ( f'-- AssetsCurrent {AssetsCurrent}' )
	print ( f'-- TreasuryStockShares {TreasuryStockShares}' )
	print ( f'-- Liabilities {Liabilities}\n\n' )
	print ( f'-- NetCash {NetCash}' )
	print ( f'-- CapEx {CapEx}' )

##
## Calculations
##
if Debug:
	print ( '-- Calculations' )

if StockholdersEquity == 0:
	StockholdersEquity = LiabilitiesAndStockholdersEquity - Liabilities

Fdebteq = 0.0
if StockholdersEquity > 0:
	Fdebteq = LongTermDebt / StockholdersEquity
	if Debug:
		print ( f'-- Fdebteq {Fdebteq}' )

Earnings = Shares * EPS
if Debug:
	print ( f'-- Earnings     {Earnings}' )

EBIT   = Earnings + Interest + Taxes
EBITDA = EBIT + Depreciation + AmortFinance + AmortIntangible

if Debug:
	print ( f'-- EBIT         {EBIT}' )
	print ( f'-- Depreciation {Depreciation}' )
	print ( f'-- AmortFinance {AmortFinance}' )
	print ( f'-- AmortIntangible {AmortIntangible}' )
	print ( f'-- EBITDA       {EBITDA}' )

MarketCap = Shares * price;

EnterpriseValue = MarketCap + MinorityInterest + LongTermDebtNoncurrent + OperatingLeaseLiabilityCurrent + AccountsPayableCurrent-CashEquiv
if Debug:
	print ( f'-- ticker {MarketCap} + {MinorityInterest} + {LongTermDebtNoncurrent} + {OperatingLeaseLiabilityCurrent} + {AccountsPayableCurrent} + {CashEquiv} = {EnterpriseValue}' )

if EBITDA > 0:
	StockValue = EnterpriseValue / EBITDA
else:
	StockValue = 0.0
	# this is normal for ETF

if Assets > 0:
	ReturnAssets = 100.0 * NetIncome / Assets
else:
	ReturnAssets = 0.0
	print ( f'-- {ticker} No Assets!!!' )

if Revenue > 0:
	GrossMargin = 100.0 * NetIncome / Revenue
else:
	GrossMargin = 0.0
	print ( f'-- {ticker} No Revenue!!!' )


"""
Quick Ratio from https://www.investopedia.com/terms/q/quickratio.asp
"""
TenPercent = Shares * 0.1	
if TreasuryStockShares > TenPercent:
	MarketableSecurities = price * TenPercent 
	if Debug:
		print ( f'-- price {price}  10% Shares {TenPercent} MarketableSecurities {MarketableSecurities}' )
else:
	MarketableSecurities = price * TreasuryStockShares
	if Debug:
		print ( f'-- price {price} TreasuryStockShares {TreasuryStockShares} MarketableSecurities {MarketableSecurities}' )

if Debug:
	print ( f'-- MarketableSecurities', MarketableSecurities)

if Liabilities > 0:
	QuickRatio = ( AssetsCurrent + CashEquiv + MarketableSecurities) / (Liabilities )
else:
	QuickRatio = 0.0
	print ( f'-- {ticker} No Liabilities!!!' )

if LongTermDebtNoncurrent == 0 and LongTermDebt > 0:
	LongTermDebtNoncurrent = LongTermDebt

FreeCashFlow = NetCash - CapEx;

print ( f"update fundamental set Fdebteq = {Fdebteq:.2f}, Ftotasst = {Assets}, FreturnA = {ReturnAssets:.2f}, Fmargin = {GrossMargin:.2f}, Fequity = {StockholdersEquity:.0f}, Febitda = {EBITDA}, Fminority = {MinorityInterest}, FdebtLT = {LongTermDebt}, FdebtNC = {LongTermDebtNoncurrent}, Fliab = {Liabilities}, Fpayables = {AccountsPayableCurrent}, Fcurasst = {AssetsCurrent}, Fcash = {CashEquiv}, Ftreasury = {TreasuryStockShares:.0f}, Frevenuegrow = {RevenueGrowPct:.2f}, Fearngrow = {EarnGrowPct:.2f}, Fframe = '{Frame[0:8]}', Ffreecash = {FreeCashFlow}, Fupdated = {systime} where Fticker = '{ticker}';" )



### data scrpaing
import requests
import pandas as pd
import matplotlib.pyplot as plt
from datetime import datetime, timedelta

# Define the CIK for Microsoft (0000789019)
cik = '0000789019'

# Set headers for the request
headers = {'User-Agent': "joshjothom05@gmail.com"}

# Get filing metadata for Microsoft
filingMetadata = requests.get(
    f'https://data.sec.gov/submissions/CIK{cik}.json',
    headers=headers
)

# Convert filing metadata to DataFrame
allForms = pd.DataFrame.from_dict(filingMetadata.json()['filings']['recent'])

# Filter for only 10-K forms in the last 5 years
allForms['reportDate'] = pd.to_datetime(allForms['reportDate'], errors='coerce')
five_years_ago = datetime.now() - timedelta(days=5*365)
tenK_filings = allForms[(allForms['form'] == '10-K') & (allForms['reportDate'] > five_years_ago)]

# Get company facts data
companyFacts = requests.get(
    f'https://data.sec.gov/api/xbrl/companyfacts/CIK{cik}.json',
    headers=headers
)

# Retrieve 'us-gaap: Assets' data
companyConcept = requests.get(
    f'https://data.sec.gov/api/xbrl/companyconcept/CIK{cik}/us-gaap/Assets.json',
    headers=headers
)

# Convert assets data to DataFrame
assetsData = pd.DataFrame.from_dict(companyConcept.json()['units']['USD'])

# Filter only 10-K forms and dates in the past 5 years
assetsData['end'] = pd.to_datetime(assetsData['end'], errors='coerce')
assets10K = assetsData[(assetsData['form'] == '10-K') & (assetsData['end'] > five_years_ago)]
assets10K = assets10K.reset_index(drop=True)

# Plot assets over time for 10-K filings
assets10K.plot(x='end', y='val', title='Microsoft Annual Assets (10-K Filings in Last 5 Years)', marker='o')
plt.xlabel('Year')
plt.ylabel('Assets (in USD)')
plt.grid(True)
plt.show()


### data scraping
import pandas as pd
import requests
import os

def get_income_statement(ticker, limit, key, period):
    URL = f'https://financialmodelingprep.com/api/v3/income-statement/{ticker}?period={period}&limit={limit}&apikey={key}'
    try:
        r = requests.get(URL)
        r.raise_for_status()  # Check if the request was successful
        print(f"Received Income Statement for {ticker}")
        income_statement = pd.DataFrame.from_dict(r.json())
        return income_statement
    except requests.exceptions.HTTPError as e:
        print(f"HTTPError for {ticker}: {str(e)}")
        print(f"Response: {r.text}")  
    except Exception as e:
        print(f"Unexpected ERROR while requesting Income Statement for {ticker}: {str(e)}")

# (Other functions remain the same...)

if __name__ == "__main__":
    # Reading API key and ticker list
    try:
        # Check if the key.txt file exists
        if not os.path.exists('key.txt'):
            raise FileNotFoundError("API key file 'key.txt' not found.")
        
        # Load the API key
        key = pd.read_csv('key.txt', header=None)[0][0]
        
        # Check if the tickers.txt file exists
        if not os.path.exists('tickers.txt'):
            raise FileNotFoundError("Ticker list file 'tickers.txt' not found.")
        
        # Load the tickers from tickers.txt
        tickers = pd.read_csv('tickers.txt', header=None)[0]
        
        if tickers.empty:
            raise ValueError("The tickers list is empty. Please provide valid tickers in 'tickers.txt'.")
        
        print(f"API Key: {key}")
        print(f"Tickers: {tickers}")
    except FileNotFoundError as e:
        print(f"File not found: {str(e)}")
    except pd.errors.EmptyDataError:
        print("Error: 'tickers.txt' or 'key.txt' is empty.")
    except Exception as e:
        print(f"Error reading files: {str(e)}")
    
    # Check if 'tickers' was successfully defined
    if 'tickers' in locals():
        # Process each ticker
        for ticker in tickers:
            print(f"\nProcessing ticker: {ticker}")
            IS = get_income_statement(ticker=ticker, limit=6, key=key, period='annual')
            # ... (Other API calls and Excel writing steps remain the same) ...
    else:
        print("Error: 'tickers' is not defined. Please check the 'tickers.txt' file.")

# -*- coding: utf-8 -*-
"""
SEC Filing Scraper
@author: AdamGetbags
"""

# import modules
import requests
import pandas as pd
import matplotlib.pyplot as plt

# create request header
headers = {'User-Agent': "email@address.com"}

# get all companies data from SEC
company_tickers_url = "https://www.sec.gov/files/company_tickers.json"
company_tickers_response = requests.get(company_tickers_url, headers=headers)

# format response to dictionary and get first key/value
company_tickers_dict = company_tickers_response.json()
first_entry = company_tickers_dict['0']

# parse CIK without leading zeros
direct_cik = first_entry['cik_str']

# dictionary to dataframe
company_data = pd.DataFrame.from_dict(company_tickers_dict, orient='index')

# add leading zeros to CIK
company_data['cik_str'] = company_data['cik_str'].astype(str).str.zfill(10)

# review first entry
print(company_data.head(1))

# Get the first company's CIK
cik = company_data.iloc[0]['cik_str']

# get company specific filing metadata from SEC's EDGAR API
filing_metadata_url = f'https://data.sec.gov/submissions/CIK{cik}.json'
filing_metadata_response = requests.get(filing_metadata_url, headers=headers)

# review keys
filing_metadata_json = filing_metadata_response.json()
print(filing_metadata_json.keys())

# parse filings
filings = filing_metadata_json['filings']['recent']

# dictionary to dataframe
all_forms = pd.DataFrame.from_dict(filings)

# review columns and extract specific filing metadata
print(all_forms[['accessionNumber', 'reportDate', 'form']].head(50))

# Example: get metadata for 10-Q filings
form_10q_metadata = all_forms[all_forms['form'] == '10-Q']
print(form_10q_metadata.head())

# get company facts data (e.g., stock shares outstanding)
company_facts_url = f'https://data.sec.gov/api/xbrl/companyfacts/CIK{cik}.json'
company_facts_response = requests.get(company_facts_url, headers=headers)

# parse facts
company_facts_json = company_facts_response.json()
print(company_facts_json['facts'].keys())

# example: get stock shares outstanding
stock_shares_outstanding = company_facts_json['facts']['dei']['EntityCommonStockSharesOutstanding']['units']['shares'][0]
print(stock_shares_outstanding)

# get company concept data (e.g., Assets from filings)
company_concept_url = f'https://data.sec.gov/api/xbrl/companyconcept/CIK{cik}/us-gaap/Assets.json'
company_concept_response = requests.get(company_concept_url, headers=headers)

# parse concept data
company_concept_json = company_concept_response.json()
assets_data = pd.DataFrame.from_dict(company_concept_json['units']['USD'])

# filter assets data for 10-Q forms and reset index
assets_10k = assets_data[assets_data['form'] == '10-K'].reset_index(drop=True)

# plot assets over time for 10-Q filings
assets_10k.plot(x='end', y='val', kind='line', title='Assets (10-K) Over Time')
plt.xlabel('Filing Date')
plt.ylabel('Assets (USD)')
plt.show()



"""
This is a script that will scrape financial information using the
FinancialModelingPrep API.
Website: https://financialmodelingprep.com/developer/
Free plan with 250 requests per day.

Includes: 
    - Income Statements 
    - Balance Sheets 
    - Cash Flow Statements 
    - Financial Ratios 
    - Key Metrics 
    - Daily Prices 
    - Enterprise Value

Parameter specification:
    - ticker: Company stock name (e.g., AAPL for Apple)
    - limit: Number of records you'd like (e.g., for annual, 6 will give 6 years)
    - key: API key generated from the Financial Modeling Prep account
    - period: 'annual' or 'quarter'
"""
import Financial_Data_Scraping as fds
import pandas as pd

if __name__ == "__main__":
    """Running the scraper to obtain financial data."""

    # Load API key from a file
    key = pd.read_csv('key.txt', header=None)[0][0]

    # Load list of tickers from a file
    tickers = pd.read_csv('tickers.txt', header=None)[0]

    # Loop through each ticker to scrape data
    for ticker in tickers:
        print(f"Starting scraping for: {ticker}")

        # Scrape financial data using the custom module 'Financial_Data_Scraping'
        IS = fds.get_income_statement(ticker=ticker, limit=6, key=key, period='annual')
        BS = fds.get_balance_sheet(ticker=ticker, limit=6, key=key, period='annual')
        CF = fds.get_cash_flow_statement(ticker=ticker, limit=6, key=key, period='annual')
        FR = fds.get_financial_ratios(ticker=ticker, limit=6, key=key, period='annual')
        KM = fds.get_key_metrics(ticker=ticker, limit=6, key=key, period='annual')
        P = fds.get_daily_prices(ticker=ticker, timeseries=5 * 261, key=key)  # 5 years of daily prices
        EV = fds.get_enterprise_value(ticker=ticker, rate=5 * 261, key=key, period='annual')  # 5 years of enterprise value

        # Creating an Excel writer object to save data to an Excel file
        writer = pd.ExcelWriter(f'{ticker}.xlsx', engine='xlsxwriter')

        # Write data to separate sheets within the Excel file
        IS.to_excel(writer, sheet_name='Income Statement')
        BS.to_excel(writer, sheet_name='Balance Sheet Statement')
        CF.to_excel(writer, sheet_name='Cash Flow Statement')
        FR.to_excel(writer, sheet_name='Financial Ratios')
        KM.to_excel(writer, sheet_name='Key Metrics')
        P.to_excel(writer, sheet_name='Daily Prices')
        EV.to_excel(writer, sheet_name='Enterprise Value')

        # Save the Excel file
        writer.save()

        print(f'Finished scraping for: {ticker}')

    print("All tickers processed.")



import pandas as pd
import numpy as np

# Load existing dataset
file_path = r'synthetic_dataset.csv'

try:
    df = pd.read_csv(file_path)
    print("Original data loaded successfully!")
except FileNotFoundError as e:
    print(f"Error: {e}")
    exit()

# Function to generate synthetic data
def generate_synthetic_data(df, n_samples=500):
    synthetic_data = []

    for _ in range(n_samples):
        # Randomly select values based on existing data distributions
        timestamp = pd.Timestamp.now()  # You can modify this to create realistic timestamps
        name = f"Synthetic_{np.random.randint(1000)}"  # Generate a synthetic name
        gender = np.random.choice(df['Gender'].dropna().unique())  # Sample gender
        section = np.random.choice(df['Section'].dropna().unique())  # Sample section
        age = np.random.randint(17, 25)  # Assuming age range from 17 to 25
        
        collision_types = np.random.choice(
            ['Body to Ground', 'Ball to body Impact', 'Head to Body collisions', 'None'], 
            size=np.random.randint(1, 4), replace=False
        )
        collision_types_str = ';'.join(collision_types)
        
        injuries = np.random.choice(
            ['Knee Injury', 'Head Injury', 'Ligament Tear', 'Dislocation of joint', 'None'], 
            size=np.random.randint(1, 3), replace=False
        )
        injuries_str = ';'.join(injuries)

        symptoms = np.random.choice(
            ['Swelling', 'Pain', 'Bruising', 'Weakness', 'None'], 
            size=np.random.randint(1, 4), replace=False
        )
        symptoms_str = ';'.join(symptoms)

        knee_injury_overtime = np.random.choice(['Yes', 'No'])
        knee_injury_instant = np.random.choice(['Yes', 'No'])
        
        synthetic_data.append([
            timestamp, name, gender, section, age,
            collision_types_str, injuries_str,
            symptoms_str, knee_injury_overtime,
            knee_injury_instant
        ])
    
    # Create a DataFrame from synthetic data
    columns = [
        "Timestamp", "Name", "Gender", "Section", "Age",
        "Have you experienced these collision during any sports activity?",
        "What kind of injuries have you experienced during game?",
        "Symptoms experienced by player after injury?",
        "Did you experience knee injury overtime?",
        "Did you experience knee injury at one instant of the game?"
    ]
    
    synthetic_df = pd.DataFrame(synthetic_data, columns=columns)
    return synthetic_df

# Generate synthetic data
synthetic_df = generate_synthetic_data(df)

# Combine original and synthetic datasets if needed
combined_df = pd.concat([df, synthetic_df], ignore_index=True)

# Save the combined dataset to an Excel file
output_file_path = r'C:\Users\User\Desktop\project 1\synthetic_dataset.xlsx'
combined_df.to_excel(output_file_path, index=False)
print(f"Combined dataset saved successfully to {output_file_path}")

import requests
import pandas as pd
import matplotlib.pyplot as plt
from datetime import datetime, timedelta

# Define the CIK for Microsoft (0000789019)
cik = '0000789019'

# Set headers for the request
headers = {'User-Agent': "joshjothom05@gmail.com"}

# Get filing metadata for Microsoft
filingMetadata = requests.get(
    f'https://data.sec.gov/submissions/CIK{cik}.json',
    headers=headers
)

# Convert filing metadata to DataFrame
allForms = pd.DataFrame.from_dict(filingMetadata.json()['filings']['recent'])

# Filter for only 10-K forms in the last 5 years
allForms['reportDate'] = pd.to_datetime(allForms['reportDate'], errors='coerce')
five_years_ago = datetime.now() - timedelta(days=5*365)
tenK_filings = allForms[(allForms['form'] == '10-K') & (allForms['reportDate'] > five_years_ago)]

# Get company facts data
companyFacts = requests.get(
    f'https://data.sec.gov/api/xbrl/companyfacts/CIK{cik}.json',
    headers=headers
)

# Retrieve 'us-gaap: Assets' data
companyConcept = requests.get(
    f'https://data.sec.gov/api/xbrl/companyconcept/CIK{cik}/us-gaap/Assets.json',
    headers=headers
)

# Convert assets data to DataFrame
assetsData = pd.DataFrame.from_dict(companyConcept.json()['units']['USD'])

# Filter only 10-K forms and dates in the past 5 years
assetsData['end'] = pd.to_datetime(assetsData['end'], errors='coerce')
assets10K = assetsData[(assetsData['form'] == '10-K') & (assetsData['end'] > five_years_ago)]
assets10K = assets10K.reset_index(drop=True)

# Plot assets over time for 10-K filings
assets10K.plot(x='end', y='val', title='Microsoft Annual Assets (10-K Filings in Last 5 Years)', marker='o')
plt.xlabel('Year')
plt.ylabel('Assets (in USD)')
plt.grid(True)
plt.show()



### edgar video
import pandas as pd
import requests
import edgar_functions as edgar
from headers import headers
import os
def get_facts(ticker, headers=headers):
    cik = edgar.cik_matching_ticker(ticker)
    url = f"https://data.sec.gov/api/xbrl/companyfacts/CIK{cik}.json"
    company_facts = requests.get(url, headers=headers).json()
    return company_facts
facts = get_facts("WSM")
facts["facts"]["us-gaap"]["AccountsPayableCurrent"]['units']['USD']
def facts_DF(ticker, headers=headers):
    facts = get_facts(ticker, headers)
    us_gaap_data = facts["facts"]["us-gaap"]
    df_data = []
    for fact, details in us_gaap_data.items():
        for unit in details["units"]:
            for item in details["units"][unit]:
                row = item.copy()
                row["fact"] = fact
                df_data.append(row)

    df = pd.DataFrame(df_data)
    df["end"] = pd.to_datetime(df["end"])
    df["start"] = pd.to_datetime(df["start"])
    df = df.drop_duplicates(subset=["fact", "end", "val"])
    df.set_index("end", inplace=True)
    labels_dict = {fact: details["label"] for fact, details in us_gaap_data.items()}
    return df, labels_dict
facts, label_dict = facts_DF("WSM")
def annual_facts(ticker, headers=headers):
    accession_nums = edgar.get_filtered_filings(
        ticker, ten_k=True, just_accession_numbers=True
    )
    df, label_dict = facts_DF(ticker, headers)
    ten_k = df[df["accn"].isin(accession_nums)]
    ten_k = ten_k[ten_k.index.isin(accession_nums.index)]
    pivot = ten_k.pivot_table(values="val", columns="fact", index="end")
    pivot.rename(columns=label_dict, inplace=True)
    return pivot.T
def quarterly_facts(ticker, headers=headers):
    accession_nums = get_filtered_filings(
        ticker, ten_k=False, just_accession_numbers=True
    )
    df, label_dict = facts_DF(ticker, headers)
    ten_q = df[df["accn"].isin(accession_nums)]
    ten_q = ten_q[ten_q.index.isin(accession_nums.index)].reset_index(drop=False)
    ten_q = ten_q.drop_duplicates(subset=["fact", "end"], keep="last")
    pivot = ten_q.pivot_table(values="val", columns="fact", index="end")
    pivot.rename(columns=label_dict, inplace=True)
    return pivot.T
quarterly = quarterly_facts('WSM')
def save_dataframe_to_csv(dataframe, folder_name, ticker, statement_name, frequency):
    directory_path = os.path.join(folder_name, ticker)
    os.makedirs(directory_path, exist_ok=True)
    file_path = os.path.join(directory_path, f"{statement_name}_{frequency}.csv")
    dataframe.to_csv(file_path)
    return None

import os
import pandas as pd
import requests
from headers import headers  # change to your own headers file or add variable in code


def cik_matching_ticker(ticker, headers=headers):
    ticker = ticker.upper().replace(".", "-")
    ticker_json = requests.get(
        "https://www.sec.gov/files/company_tickers.json", headers=headers
    ).json()

    for company in ticker_json.values():
        if company["ticker"] == ticker:
            cik = str(company["cik_str"]).zfill(10)
            return cik
    raise ValueError(f"Ticker {ticker} not found in SEC database")


def get_submission_data_for_ticker(ticker, headers=headers, only_filings_df=False):
    """
    Get the data in json form for a given ticker. For example: 'cik', 'entityType', 'sic', 'sicDescription', 'insiderTransactionForOwnerExists', 'insiderTransactionForIssuerExists', 'name', 'tickers', 'exchanges', 'ein', 'description', 'website', 'investorWebsite', 'category', 'fiscalYearEnd', 'stateOfIncorporation', 'stateOfIncorporationDescription', 'addresses', 'phone', 'flags', 'formerNames', 'filings'

    Args:
        ticker (str): The ticker symbol of the company.

    Returns:
        json: The submissions for the company.
    """
    cik = cik_matching_ticker(ticker)
    headers = headers
    url = f"https://data.sec.gov/submissions/CIK{cik}.json"
    company_json = requests.get(url, headers=headers).json()
    if only_filings_df:
        return pd.DataFrame(company_json["filings"]["recent"])
    else:
        return company_json


def get_filtered_filings(
    ticker, ten_k=True, just_accession_numbers=False, headers=headers
):
    company_filings_df = get_submission_data_for_ticker(
        ticker, only_filings_df=True, headers=headers
    )
    if ten_k:
        df = company_filings_df[company_filings_df["form"] == "10-K"]
    else:
        df = company_filings_df[company_filings_df["form"] == "10-Q"]
    if just_accession_numbers:
        df = df.set_index("reportDate")
        accession_df = df["accessionNumber"]
        return accession_df
    else:
        return df


def get_facts(ticker, headers=headers):
    cik = cik_matching_ticker(ticker)
    url = f"https://data.sec.gov/api/xbrl/companyfacts/CIK{cik}.json"
    company_facts = requests.get(url, headers=headers).json()
    return company_facts


def facts_DF(ticker, headers=headers):
    facts = get_facts(ticker, headers)
    us_gaap_data = facts["facts"]["us-gaap"]
    df_data = []
    for fact, details in us_gaap_data.items():
        for unit in details["units"]:
            for item in details["units"][unit]:
                row = item.copy()
                row["fact"] = fact
                df_data.append(row)

    df = pd.DataFrame(df_data)
    df["end"] = pd.to_datetime(df["end"])
    df["start"] = pd.to_datetime(df["start"])
    df = df.drop_duplicates(subset=["fact", "end", "val"])
    df.set_index("end", inplace=True)
    labels_dict = {fact: details["label"] for fact, details in us_gaap_data.items()}
    return df, labels_dict


def annual_facts(ticker, headers=headers):
    accession_nums = get_filtered_filings(
        ticker, ten_k=True, just_accession_numbers=True
    )
    df, label_dict = facts_DF(ticker, headers)
    ten_k = df[df["accn"].isin(accession_nums)]
    ten_k = ten_k[ten_k.index.isin(accession_nums.index)]
    pivot = ten_k.pivot_table(values="val", columns="fact", index="end")
    pivot.rename(columns=label_dict, inplace=True)
    return pivot.T


def quarterly_facts(ticker, headers=headers):
    accession_nums = get_filtered_filings(
        ticker, ten_k=False, just_accession_numbers=True
    )
    df, label_dict = facts_DF(ticker, headers)
    ten_q = df[df["accn"].isin(accession_nums)]
    ten_q = ten_q[ten_q.index.isin(accession_nums.index)].reset_index(drop=False)
    ten_q = ten_q.drop_duplicates(subset=["fact", "end"], keep="last")
    pivot = ten_q.pivot_table(values="val", columns="fact", index="end")
    pivot.rename(columns=label_dict, inplace=True)
    return pivot.T


def save_dataframe_to_csv(dataframe, folder_name, ticker, statement_name, frequency):
    directory_path = os.path.join(folder_name, ticker)
    os.makedirs(directory_path, exist_ok=True)
    file_path = os.path.join(directory_path, f"{statement_name}_{frequency}.csv")
    dataframe.to_csv(file_path)
    return None


import pandas as pd
import requests
from headers import headers  # change to your own headers file or add variable in code


def cik_matching_ticker(ticker, headers=headers):
    ticker = ticker.upper().replace(".", "-")
    ticker_json = requests.get(
        "https://www.sec.gov/files/company_tickers.json", headers=headers
    ).json()

    for company in ticker_json.values():
        if company["ticker"] == ticker:
            cik = str(company["cik_str"]).zfill(10)
            return cik
    raise ValueError(f"Ticker {ticker} not found in SEC database")


def get_submission_data_for_ticker(ticker, headers=headers, only_filings_df=False):
    """
    Get the data in json form for a given ticker. For example: 'cik', 'entityType', 'sic', 'sicDescription', 'insiderTransactionForOwnerExists', 'insiderTransactionForIssuerExists', 'name', 'tickers', 'exchanges', 'ein', 'description', 'website', 'investorWebsite', 'category', 'fiscalYearEnd', 'stateOfIncorporation', 'stateOfIncorporationDescription', 'addresses', 'phone', 'flags', 'formerNames', 'filings'

    Args:
        ticker (str): The ticker symbol of the company.

    Returns:
        json: The submissions for the company.

    Raises:
        ValueError: If ticker is not a string.
    """
    cik = cik_matching_ticker(ticker)
    headers = headers
    url = f"https://data.sec.gov/submissions/CIK{cik}.json"
    company_json = requests.get(url, headers=headers).json()
    if only_filings_df:
        return pd.DataFrame(company_json["filings"]["recent"])
    else:
        return company_json


def get_filtered_filings(
    ticker, ten_k=True, just_accession_numbers=False, headers=headers
):
    company_filings_df = get_submission_data_for_ticker(
        ticker, only_filings_df=True, headers=headers
    )
    if ten_k:
        df = company_filings_df[company_filings_df["form"] == "10-K"]
    else:
        df = company_filings_df[company_filings_df["form"] == "10-Q"]
    if just_accession_numbers:
        df = df.set_index("reportDate")
        accession_df = df["accessionNumber"]
        return accession_df
    else:
        return df



import os
import pandas as pd
import numpy as np  # make sure to add
import requests
from bs4 import BeautifulSoup
import logging  # make sure to add
import calendar  # make sure to add
from headers import headers  # change to your own headers file or add variable in code

pd.options.display.float_format = (
    lambda x: "{:,.0f}".format(x) if int(x) == x else "{:,.2f}".format(x)
)

statement_keys_map = {
    "balance_sheet": [
        "balance sheet",
        "balance sheets",
        "statement of financial position",
        "consolidated balance sheets",
        "consolidated balance sheet",
        "consolidated financial position",
        "consolidated balance sheets - southern",
        "consolidated statements of financial position",
        "consolidated statement of financial position",
        "consolidated statements of financial condition",
        "combined and consolidated balance sheet",
        "condensed consolidated balance sheets",
        "consolidated balance sheets, as of december 31",
        "dow consolidated balance sheets",
        "consolidated balance sheets (unaudited)",
    ],
    "income_statement": [
        "income statement",
        "income statements",
        "statement of earnings (loss)",
        "statements of consolidated income",
        "consolidated statements of operations",
        "consolidated statement of operations",
        "consolidated statements of earnings",
        "consolidated statement of earnings",
        "consolidated statements of income",
        "consolidated statement of income",
        "consolidated income statements",
        "consolidated income statement",
        "condensed consolidated statements of earnings",
        "consolidated results of operations",
        "consolidated statements of income (loss)",
        "consolidated statements of income - southern",
        "consolidated statements of operations and comprehensive income",
        "consolidated statements of comprehensive income",
    ],
    "cash_flow_statement": [
        "cash flows statement",
        "cash flows statements",
        "statement of cash flows",
        "statements of consolidated cash flows",
        "consolidated statements of cash flows",
        "consolidated statement of cash flows",
        "consolidated statement of cash flow",
        "consolidated cash flows statements",
        "consolidated cash flow statements",
        "condensed consolidated statements of cash flows",
        "consolidated statements of cash flows (unaudited)",
        "consolidated statements of cash flows - southern",
    ],
}


def cik_matching_ticker(ticker, headers=headers):
    ticker = ticker.upper().replace(".", "-")
    ticker_json = requests.get(
        "https://www.sec.gov/files/company_tickers.json", headers=headers
    ).json()

    for company in ticker_json.values():
        if company["ticker"] == ticker:
            cik = str(company["cik_str"]).zfill(10)
            return cik
    raise ValueError(f"Ticker {ticker} not found in SEC database")


def get_submission_data_for_ticker(ticker, headers=headers, only_filings_df=False):
    """
    Get the data in json form for a given ticker. For example: 'cik', 'entityType', 'sic', 'sicDescription', 'insiderTransactionForOwnerExists', 'insiderTransactionForIssuerExists', 'name', 'tickers', 'exchanges', 'ein', 'description', 'website', 'investorWebsite', 'category', 'fiscalYearEnd', 'stateOfIncorporation', 'stateOfIncorporationDescription', 'addresses', 'phone', 'flags', 'formerNames', 'filings'

    Args:
        ticker (str): The ticker symbol of the company.

    Returns:
        json: The submissions for the company.
    """
    cik = cik_matching_ticker(ticker)
    headers = headers
    url = f"https://data.sec.gov/submissions/CIK{cik}.json"
    company_json = requests.get(url, headers=headers).json()
    if only_filings_df:
        return pd.DataFrame(company_json["filings"]["recent"])
    else:
        return company_json


def get_filtered_filings(
    ticker, ten_k=True, just_accession_numbers=False, headers=None
):
    """
    Retrieves either 10-K or 10-Q filings for a given ticker and optionally returns just accession numbers.

    Args:
        ticker (str): Stock ticker symbol.
        ten_k (bool): If True, fetches 10-K filings; otherwise, fetches 10-Q filings.
        just_accession_numbers (bool): If True, returns only accession numbers; otherwise, returns full data.
        headers (dict): Headers for HTTP request.

    Returns:
        DataFrame or Series: DataFrame of filings or Series of accession numbers.
    """
    # Fetch submission data for the given ticker
    company_filings_df = get_submission_data_for_ticker(
        ticker, only_filings_df=True, headers=headers
    )
    # Filter for 10-K or 10-Q forms
    df = company_filings_df[company_filings_df["form"] == ("10-K" if ten_k else "10-Q")]
    # Return accession numbers if specified
    if just_accession_numbers:
        df = df.set_index("reportDate")
        accession_df = df["accessionNumber"]
        return accession_df
    else:
        return df


def get_facts(ticker, headers=None):
    """
    Retrieves company facts for a given ticker.

    Args:
        ticker (str): Stock ticker symbol.
        headers (dict): Headers for HTTP request.

    Returns:
        dict: Company facts in JSON format.
    """
    # Get CIK number matching the ticker
    cik = cik_matching_ticker(ticker)
    # Construct URL for company facts
    url = f"https://data.sec.gov/api/xbrl/companyfacts/CIK{cik}.json"
    # Fetch and return company facts
    company_facts = requests.get(url, headers=headers).json()
    return company_facts


def facts_DF(ticker, headers=None):
    """
    Converts company facts into a DataFrame.

    Args:
        ticker (str): Stock ticker symbol.
        headers (dict): Headers for HTTP request.

    Returns:
        tuple: DataFrame of facts and a dictionary of labels.
    """
    # Retrieve facts data
    facts = get_facts(ticker, headers)
    us_gaap_data = facts["facts"]["us-gaap"]
    df_data = []

    # Process each fact and its details
    for fact, details in us_gaap_data.items():
        for unit in details["units"]:
            for item in details["units"][unit]:
                row = item.copy()
                row["fact"] = fact
                df_data.append(row)

    df = pd.DataFrame(df_data)
    # Convert 'end' and 'start' to datetime
    df["end"] = pd.to_datetime(df["end"])
    df["start"] = pd.to_datetime(df["start"])
    # Drop duplicates and set index
    df = df.drop_duplicates(subset=["fact", "end", "val"])
    df.set_index("end", inplace=True)
    # Create a dictionary of labels for facts
    labels_dict = {fact: details["label"] for fact, details in us_gaap_data.items()}
    return df, labels_dict


def annual_facts(ticker, headers=None):
    """
    Fetches and processes annual (10-K) financial facts for a given ticker.

    Args:
        ticker (str): Stock ticker symbol.
        headers (dict): Headers for HTTP request.

    Returns:
        DataFrame: Transposed pivot table of annual financial facts.
    """
    # Get accession numbers for 10-K filings
    accession_nums = get_filtered_filings(
        ticker, ten_k=True, just_accession_numbers=True, headers=headers
    )
    # Extract and process facts data
    df, label_dict = facts_DF(ticker, headers)
    # Filter data for 10-K filings
    ten_k = df[df["accn"].isin(accession_nums)]
    ten_k = ten_k[ten_k.index.isin(accession_nums.index)]
    # Pivot and format the data
    pivot = ten_k.pivot_table(values="val", columns="fact", index="end")
    pivot.rename(columns=label_dict, inplace=True)
    return pivot.T


def quarterly_facts(ticker, headers=None):
    """
    Fetches and processes quarterly (10-Q) financial facts for a given ticker.

    Args:
        ticker (str): Stock ticker symbol.
        headers (dict): Headers for HTTP request.

    Returns:
        DataFrame: Transposed pivot table of quarterly financial facts.
    """
    # Get accession numbers for 10-Q filings
    accession_nums = get_filtered_filings(
        ticker, ten_k=False, just_accession_numbers=True, headers=headers
    )
    # Extract and process facts data
    df, label_dict = facts_DF(ticker, headers)
    # Filter data for 10-Q filings
    ten_q = df[df["accn"].isin(accession_nums)]
    ten_q = ten_q[ten_q.index.isin(accession_nums.index)].reset_index(drop=False)
    # Remove duplicate entries
    ten_q = ten_q.drop_duplicates(subset=["fact", "end"], keep="last")
    # Pivot and format the data
    pivot = ten_q.pivot_table(values="val", columns="fact", index="end")
    pivot.rename(columns=label_dict, inplace=True)
    return pivot.T


def save_dataframe_to_csv(dataframe, folder_name, ticker, statement_name, frequency):
    """
    Saves a given DataFrame to a CSV file in a specified directory.

    Args:
        dataframe (DataFrame): The DataFrame to be saved.
        folder_name (str): The folder name where the CSV file will be saved.
        ticker (str): Stock ticker symbol.
        statement_name (str): Name of the financial statement.
        frequency (str): Frequency of the financial data (e.g., annual, quarterly).

    Returns:
        None
    """
    # Create directory path
    directory_path = os.path.join(folder_name, ticker)
    os.makedirs(directory_path, exist_ok=True)
    # Construct file path and save DataFrame
    file_path = os.path.join(directory_path, f"{statement_name}_{frequency}.csv")
    dataframe.to_csv(file_path)


def _get_file_name(report):
    """
    Extracts the file name from an XML report tag.

    Args:
        report (Tag): BeautifulSoup tag representing the report.

    Returns:
        str: File name extracted from the tag.
    """
    html_file_name_tag = report.find("HtmlFileName")
    xml_file_name_tag = report.find("XmlFileName")
    # Return the appropriate file name
    if html_file_name_tag:
        return html_file_name_tag.text
    elif xml_file_name_tag:
        return xml_file_name_tag.text
    else:
        return ""


def _is_statement_file(short_name_tag, long_name_tag, file_name):
    """
    Determines if a given file is a financial statement file.

    Args:
        short_name_tag (Tag): BeautifulSoup tag for the short name.
        long_name_tag (Tag): BeautifulSoup tag for the long name.
        file_name (str): Name of the file.

    Returns:
        bool: True if it's a statement file, False otherwise.
    """
    return (
        short_name_tag is not None
        and long_name_tag is not None
        and file_name  # Ensure file_name is not an empty string
        and "Statement" in long_name_tag.text
    )


def get_statement_file_names_in_filing_summary(ticker, accession_number, headers=None):
    """
    Retrieves file names of financial statements from a filing summary.

    Args:
        ticker (str): Stock ticker symbol.
        accession_number (str): SEC filing accession number.
        headers (dict): Headers for HTTP request.

    Returns:
        dict: Dictionary mapping statement types to their file names.
    """
    try:
        # Set up request session and get filing summary
        session = requests.Session()
        cik = cik_matching_ticker(ticker)
        base_link = f"https://www.sec.gov/Archives/edgar/data/{cik}/{accession_number}"
        filing_summary_link = f"{base_link}/FilingSummary.xml"
        filing_summary_response = session.get(
            filing_summary_link, headers=headers
        ).content.decode("utf-8")

        # Parse the filing summary
        filing_summary_soup = BeautifulSoup(filing_summary_response, "lxml-xml")
        statement_file_names_dict = {}
        # Extract file names for statements
        for report in filing_summary_soup.find_all("Report"):
            file_name = _get_file_name(report)
            short_name, long_name = report.find("ShortName"), report.find("LongName")
            if _is_statement_file(short_name, long_name, file_name):
                statement_file_names_dict[short_name.text.lower()] = file_name
        return statement_file_names_dict
    except requests.RequestException as e:
        print(f"An error occurred: {e}")
        return {}


def get_statement_soup(
    ticker, accession_number, statement_name, headers, statement_keys_map
):
    """
    Retrieves the BeautifulSoup object for a specific financial statement.

    Args:
        ticker (str): Stock ticker symbol.
        accession_number (str): SEC filing accession number.
        statement_name (str): has to be 'balance_sheet', 'income_statement', 'cash_flow_statement'
        headers (dict): Headers for HTTP request.
        statement_keys_map (dict): Mapping of statement names to keys.

    Returns:
        BeautifulSoup: Parsed HTML/XML content of the financial statement.

    Raises:
        ValueError: If the statement file name is not found or if there is an error fetching the statement.
    """
    session = requests.Session()
    cik = cik_matching_ticker(ticker)
    base_link = f"https://www.sec.gov/Archives/edgar/data/{cik}/{accession_number}"
    # Get statement file names
    statement_file_name_dict = get_statement_file_names_in_filing_summary(
        ticker, accession_number, headers
    )
    statement_link = None
    # Find the specific statement link
    for possible_key in statement_keys_map.get(statement_name.lower(), []):
        file_name = statement_file_name_dict.get(possible_key.lower())
        if file_name:
            statement_link = f"{base_link}/{file_name}"
            break
    if not statement_link:
        raise ValueError(f"Could not find statement file name for {statement_name}")
    # Fetch the statement
    try:
        statement_response = session.get(statement_link, headers=headers)
        statement_response.raise_for_status()  # Check for a successful request
        # Parse and return the content
        if statement_link.endswith(".xml"):
            return BeautifulSoup(
                statement_response.content, "lxml-xml", from_encoding="utf-8"
            )
        else:
            return BeautifulSoup(statement_response.content, "lxml")
    except requests.RequestException as e:
        raise ValueError(f"Error fetching the statement: {e}")


def extract_columns_values_and_dates_from_statement(soup):
    """
    Extracts columns, values, and dates from an HTML soup object representing a financial statement.

    Args:
        soup (BeautifulSoup): The BeautifulSoup object of the HTML document.

    Returns:
        tuple: Tuple containing columns, values_set, and date_time_index.
    """
    columns = []
    values_set = []
    date_time_index = get_datetime_index_dates_from_statement(soup)

    for table in soup.find_all("table"):
        unit_multiplier = 1
        special_case = False

        # Check table headers for unit multipliers and special cases
        table_header = table.find("th")
        if table_header:
            header_text = table_header.get_text()
            # Determine unit multiplier based on header text
            if "in Thousands" in header_text:
                unit_multiplier = 1
            elif "in Millions" in header_text:
                unit_multiplier = 1000
            # Check for special case scenario
            if "unless otherwise specified" in header_text:
                special_case = True

        # Process each row of the table
        for row in table.select("tr"):
            onclick_elements = row.select("td.pl a, td.pl.custom a")
            if not onclick_elements:
                continue

            # Extract column title from 'onclick' attribute
            onclick_attr = onclick_elements[0]["onclick"]
            column_title = onclick_attr.split("defref_")[-1].split("',")[0]
            columns.append(column_title)

            # Initialize values array with NaNs
            values = [np.NaN] * len(date_time_index)

            # Process each cell in the row
            for i, cell in enumerate(row.select("td.text, td.nump, td.num")):
                if "text" in cell.get("class"):
                    continue

                # Clean and parse cell value
                value = keep_numbers_and_decimals_only_in_string(
                    cell.text.replace("$", "")
                    .replace(",", "")
                    .replace("(", "")
                    .replace(")", "")
                    .strip()
                )
                if value:
                    value = float(value)
                    # Adjust value based on special case and cell class
                    if special_case:
                        value /= 1000
                    else:
                        if "nump" in cell.get("class"):
                            values[i] = value * unit_multiplier
                        else:
                            values[i] = -value * unit_multiplier

            values_set.append(values)

    return columns, values_set, date_time_index


def get_datetime_index_dates_from_statement(soup: BeautifulSoup) -> pd.DatetimeIndex:
    """
    Extracts datetime index dates from the HTML soup object of a financial statement.

    Args:
        soup (BeautifulSoup): The BeautifulSoup object of the HTML document.

    Returns:
        pd.DatetimeIndex: A Pandas DatetimeIndex object containing the extracted dates.
    """
    table_headers = soup.find_all("th", {"class": "th"})
    dates = [str(th.div.string) for th in table_headers if th.div and th.div.string]
    dates = [standardize_date(date).replace(".", "") for date in dates]
    index_dates = pd.to_datetime(dates)
    return index_dates


def standardize_date(date: str) -> str:
    """
    Standardizes date strings by replacing abbreviations with full month names.

    Args:
        date (str): The date string to be standardized.

    Returns:
        str: The standardized date string.
    """
    for abbr, full in zip(calendar.month_abbr[1:], calendar.month_name[1:]):
        date = date.replace(abbr, full)
    return date


def keep_numbers_and_decimals_only_in_string(mixed_string: str):
    """
    Filters a string to keep only numbers and decimal points.

    Args:
        mixed_string (str): The string containing mixed characters.

    Returns:
        str: String containing only numbers and decimal points.
    """
    num = "1234567890."
    allowed = list(filter(lambda x: x in num, mixed_string))
    return "".join(allowed)


def create_dataframe_of_statement_values_columns_dates(
    values_set, columns, index_dates
) -> pd.DataFrame:
    """
    Creates a DataFrame from statement values, columns, and index dates.

    Args:
        values_set (list): List of values for each column.
        columns (list): List of column names.
        index_dates (pd.DatetimeIndex): DatetimeIndex for the DataFrame index.

    Returns:
        pd.DataFrame: DataFrame constructed from the given data.
    """
    transposed_values_set = list(zip(*values_set))
    df = pd.DataFrame(transposed_values_set, columns=columns, index=index_dates)
    return df


def process_one_statement(ticker, accession_number, statement_name):
    """
    Processes a single financial statement identified by ticker, accession number, and statement name.

    Args:
        ticker (str): The stock ticker.
        accession_number (str): The SEC accession number.
        statement_name (str): Name of the financial statement.

    Returns:
        pd.DataFrame or None: DataFrame of the processed statement or None if an error occurs.
    """
    try:
        # Fetch the statement HTML soup
        soup = get_statement_soup(
            ticker,
            accession_number,
            statement_name,
            headers=headers,
            statement_keys_map=statement_keys_map,
        )
    except Exception as e:
        logging.error(
            f"Failed to get statement soup: {e} for accession number: {accession_number}"
        )
        return None

    if soup:
        try:
            # Extract data and create DataFrame
            columns, values, dates = extract_columns_values_and_dates_from_statement(
                soup
            )
            df = create_dataframe_of_statement_values_columns_dates(
                values, columns, dates
            )

            if not df.empty:
                # Remove duplicate columns
                df = df.T.drop_duplicates()
            else:
                logging.warning(
                    f"Empty DataFrame for accession number: {accession_number}"
                )
                return None

            return df
        except Exception as e:
            logging.error(f"Error processing statement: {e}")
            return None


def get_label_dictionary(ticker, headers):
    facts = get_facts(ticker, headers)
    us_gaap_data = facts["facts"]["us-gaap"]
    labels_dict = {fact: details["label"] for fact, details in us_gaap_data.items()}
    return labels_dict


def rename_statement(statement, label_dictionary):
    # Extract the part after the first "_" and then map it using the label dictionary
    statement.index = statement.index.map(
        lambda x: label_dictionary.get(x.split("_", 1)[-1], x)
    )
    return statement



from edgar_functions import *
from headers import headers

ticker = "WSM"
accn = get_filtered_filings(
    ticker, ten_k=True, just_accession_numbers=False, headers=headers
)
acc_num = accn['accessionNumber'].iloc[2].replace("-", "")
soup = get_statement_soup(
    ticker,
    acc_num,
    "balance_sheet",
    headers=headers,
    statement_keys_map=statement_keys_map,
)
accn


extract_columns_values_and_dates_from_statement(soup)


process_one_statement(ticker, acc_num, "cash_flow_statement")




import os
import pandas as pd
import numpy as np  # make sure to add
import requests
from bs4 import BeautifulSoup
import logging  # make sure to add
import calendar  # make sure to add
from headers import headers  # change to your own headers file or add variable in code

pd.options.display.float_format = (
    lambda x: "{:,.0f}".format(x) if int(x) == x else "{:,.2f}".format(x)
)

statement_keys_map = {
    "balance_sheet": [
        "balance sheet",
        "balance sheets",
        "statement of financial position",
        "consolidated balance sheets",
        "consolidated balance sheet",
        "consolidated financial position",
        "consolidated balance sheets - southern",
        "consolidated statements of financial position",
        "consolidated statement of financial position",
        "consolidated statements of financial condition",
        "combined and consolidated balance sheet",
        "condensed consolidated balance sheets",
        "consolidated balance sheets, as of december 31",
        "dow consolidated balance sheets",
        "consolidated balance sheets (unaudited)",
    ],
    "income_statement": [
        "income statement",
        "income statements",
        "statement of earnings (loss)",
        "statements of consolidated income",
        "consolidated statements of operations",
        "consolidated statement of operations",
        "consolidated statements of earnings",
        "consolidated statement of earnings",
        "consolidated statements of income",
        "consolidated statement of income",
        "consolidated income statements",
        "consolidated income statement",
        "condensed consolidated statements of earnings",
        "consolidated results of operations",
        "consolidated statements of income (loss)",
        "consolidated statements of income - southern",
        "consolidated statements of operations and comprehensive income",
        "consolidated statements of comprehensive income",
    ],
    "cash_flow_statement": [
        "cash flows statement",
        "cash flows statements",
        "statement of cash flows",
        "statements of consolidated cash flows",
        "consolidated statements of cash flows",
        "consolidated statement of cash flows",
        "consolidated statement of cash flow",
        "consolidated cash flows statements",
        "consolidated cash flow statements",
        "condensed consolidated statements of cash flows",
        "consolidated statements of cash flows (unaudited)",
        "consolidated statements of cash flows - southern",
    ],
}


def cik_matching_ticker(ticker, headers=headers):
    ticker = ticker.upper().replace(".", "-")
    ticker_json = requests.get(
        "https://www.sec.gov/files/company_tickers.json", headers=headers
    ).json()

    for company in ticker_json.values():
        if company["ticker"] == ticker:
            cik = str(company["cik_str"]).zfill(10)
            return cik
    raise ValueError(f"Ticker {ticker} not found in SEC database")


def get_submission_data_for_ticker(ticker, headers=headers, only_filings_df=False):
    """
    Get the data in json form for a given ticker. For example: 'cik', 'entityType', 'sic', 'sicDescription', 'insiderTransactionForOwnerExists', 'insiderTransactionForIssuerExists', 'name', 'tickers', 'exchanges', 'ein', 'description', 'website', 'investorWebsite', 'category', 'fiscalYearEnd', 'stateOfIncorporation', 'stateOfIncorporationDescription', 'addresses', 'phone', 'flags', 'formerNames', 'filings'

    Args:
        ticker (str): The ticker symbol of the company.

    Returns:
        json: The submissions for the company.
    """
    cik = cik_matching_ticker(ticker)
    headers = headers
    url = f"https://data.sec.gov/submissions/CIK{cik}.json"
    company_json = requests.get(url, headers=headers).json()
    if only_filings_df:
        return pd.DataFrame(company_json["filings"]["recent"])
    else:
        return company_json


def get_filtered_filings(
    ticker, ten_k=True, just_accession_numbers=False, headers=None
):
    """
    Retrieves either 10-K or 10-Q filings for a given ticker and optionally returns just accession numbers.

    Args:
        ticker (str): Stock ticker symbol.
        ten_k (bool): If True, fetches 10-K filings; otherwise, fetches 10-Q filings.
        just_accession_numbers (bool): If True, returns only accession numbers; otherwise, returns full data.
        headers (dict): Headers for HTTP request.

    Returns:
        DataFrame or Series: DataFrame of filings or Series of accession numbers.
    """
    # Fetch submission data for the given ticker
    company_filings_df = get_submission_data_for_ticker(
        ticker, only_filings_df=True, headers=headers
    )
    # Filter for 10-K or 10-Q forms
    df = company_filings_df[company_filings_df["form"] == ("10-K" if ten_k else "10-Q")]
    # Return accession numbers if specified
    if just_accession_numbers:
        df = df.set_index("reportDate")
        accession_df = df["accessionNumber"]
        return accession_df
    else:
        return df


def get_facts(ticker, headers=None):
    """
    Retrieves company facts for a given ticker.

    Args:
        ticker (str): Stock ticker symbol.
        headers (dict): Headers for HTTP request.

    Returns:
        dict: Company facts in JSON format.
    """
    # Get CIK number matching the ticker
    cik = cik_matching_ticker(ticker)
    # Construct URL for company facts
    url = f"https://data.sec.gov/api/xbrl/companyfacts/CIK{cik}.json"
    # Fetch and return company facts
    company_facts = requests.get(url, headers=headers).json()
    return company_facts


def facts_DF(ticker, headers=None):
    """
    Converts company facts into a DataFrame.

    Args:
        ticker (str): Stock ticker symbol.
        headers (dict): Headers for HTTP request.

    Returns:
        tuple: DataFrame of facts and a dictionary of labels.
    """
    # Retrieve facts data
    facts = get_facts(ticker, headers)
    us_gaap_data = facts["facts"]["us-gaap"]
    df_data = []

    # Process each fact and its details
    for fact, details in us_gaap_data.items():
        for unit in details["units"]:
            for item in details["units"][unit]:
                row = item.copy()
                row["fact"] = fact
                df_data.append(row)

    df = pd.DataFrame(df_data)
    # Convert 'end' and 'start' to datetime
    df["end"] = pd.to_datetime(df["end"])
    df["start"] = pd.to_datetime(df["start"])
    # Drop duplicates and set index
    df = df.drop_duplicates(subset=["fact", "end", "val"])
    df.set_index("end", inplace=True)
    # Create a dictionary of labels for facts
    labels_dict = {fact: details["label"] for fact, details in us_gaap_data.items()}
    return df, labels_dict


def annual_facts(ticker, headers=None):
    """
    Fetches and processes annual (10-K) financial facts for a given ticker.

    Args:
        ticker (str): Stock ticker symbol.
        headers (dict): Headers for HTTP request.

    Returns:
        DataFrame: Transposed pivot table of annual financial facts.
    """
    # Get accession numbers for 10-K filings
    accession_nums = get_filtered_filings(
        ticker, ten_k=True, just_accession_numbers=True, headers=headers
    )
    # Extract and process facts data
    df, label_dict = facts_DF(ticker, headers)
    # Filter data for 10-K filings
    ten_k = df[df["accn"].isin(accession_nums)]
    ten_k = ten_k[ten_k.index.isin(accession_nums.index)]
    # Pivot and format the data
    pivot = ten_k.pivot_table(values="val", columns="fact", index="end")
    pivot.rename(columns=label_dict, inplace=True)
    return pivot.T


def quarterly_facts(ticker, headers=None):
    """
    Fetches and processes quarterly (10-Q) financial facts for a given ticker.

    Args:
        ticker (str): Stock ticker symbol.
        headers (dict): Headers for HTTP request.

    Returns:
        DataFrame: Transposed pivot table of quarterly financial facts.
    """
    # Get accession numbers for 10-Q filings
    accession_nums = get_filtered_filings(
        ticker, ten_k=False, just_accession_numbers=True, headers=headers
    )
    # Extract and process facts data
    df, label_dict = facts_DF(ticker, headers)
    # Filter data for 10-Q filings
    ten_q = df[df["accn"].isin(accession_nums)]
    ten_q = ten_q[ten_q.index.isin(accession_nums.index)].reset_index(drop=False)
    # Remove duplicate entries
    ten_q = ten_q.drop_duplicates(subset=["fact", "end"], keep="last")
    # Pivot and format the data
    pivot = ten_q.pivot_table(values="val", columns="fact", index="end")
    pivot.rename(columns=label_dict, inplace=True)
    return pivot.T


def save_dataframe_to_csv(dataframe, folder_name, ticker, statement_name, frequency):
    """
    Saves a given DataFrame to a CSV file in a specified directory.

    Args:
        dataframe (DataFrame): The DataFrame to be saved.
        folder_name (str): The folder name where the CSV file will be saved.
        ticker (str): Stock ticker symbol.
        statement_name (str): Name of the financial statement.
        frequency (str): Frequency of the financial data (e.g., annual, quarterly).

    Returns:
        None
    """
    # Create directory path
    directory_path = os.path.join(folder_name, ticker)
    os.makedirs(directory_path, exist_ok=True)
    # Construct file path and save DataFrame
    file_path = os.path.join(directory_path, f"{statement_name}_{frequency}.csv")
    dataframe.to_csv(file_path)


def _get_file_name(report):
    """
    Extracts the file name from an XML report tag.

    Args:
        report (Tag): BeautifulSoup tag representing the report.

    Returns:
        str: File name extracted from the tag.
    """
    html_file_name_tag = report.find("HtmlFileName")
    xml_file_name_tag = report.find("XmlFileName")
    # Return the appropriate file name
    if html_file_name_tag:
        return html_file_name_tag.text
    elif xml_file_name_tag:
        return xml_file_name_tag.text
    else:
        return ""


def _is_statement_file(short_name_tag, long_name_tag, file_name):
    """
    Determines if a given file is a financial statement file.

    Args:
        short_name_tag (Tag): BeautifulSoup tag for the short name.
        long_name_tag (Tag): BeautifulSoup tag for the long name.
        file_name (str): Name of the file.

    Returns:
        bool: True if it's a statement file, False otherwise.
    """
    return (
        short_name_tag is not None
        and long_name_tag is not None
        and file_name  # Ensure file_name is not an empty string
        and "Statement" in long_name_tag.text
    )


def get_statement_file_names_in_filing_summary(ticker, accession_number, headers=None):
    """
    Retrieves file names of financial statements from a filing summary.

    Args:
        ticker (str): Stock ticker symbol.
        accession_number (str): SEC filing accession number.
        headers (dict): Headers for HTTP request.

    Returns:
        dict: Dictionary mapping statement types to their file names.
    """
    try:
        # Set up request session and get filing summary
        session = requests.Session()
        cik = cik_matching_ticker(ticker)
        base_link = f"https://www.sec.gov/Archives/edgar/data/{cik}/{accession_number}"
        filing_summary_link = f"{base_link}/FilingSummary.xml"
        filing_summary_response = session.get(
            filing_summary_link, headers=headers
        ).content.decode("utf-8")

        # Parse the filing summary
        filing_summary_soup = BeautifulSoup(filing_summary_response, "lxml-xml")
        statement_file_names_dict = {}
        # Extract file names for statements
        for report in filing_summary_soup.find_all("Report"):
            file_name = _get_file_name(report)
            short_name, long_name = report.find("ShortName"), report.find("LongName")
            if _is_statement_file(short_name, long_name, file_name):
                statement_file_names_dict[short_name.text.lower()] = file_name
        return statement_file_names_dict
    except requests.RequestException as e:
        print(f"An error occurred: {e}")
        return {}


def get_statement_soup(
    ticker, accession_number, statement_name, headers, statement_keys_map
):
    """
    Retrieves the BeautifulSoup object for a specific financial statement.

    Args:
        ticker (str): Stock ticker symbol.
        accession_number (str): SEC filing accession number.
        statement_name (str): has to be 'balance_sheet', 'income_statement', 'cash_flow_statement'
        headers (dict): Headers for HTTP request.
        statement_keys_map (dict): Mapping of statement names to keys.

    Returns:
        BeautifulSoup: Parsed HTML/XML content of the financial statement.

    Raises:
        ValueError: If the statement file name is not found or if there is an error fetching the statement.
    """
    session = requests.Session()
    cik = cik_matching_ticker(ticker)
    base_link = f"https://www.sec.gov/Archives/edgar/data/{cik}/{accession_number}"
    # Get statement file names
    statement_file_name_dict = get_statement_file_names_in_filing_summary(
        ticker, accession_number, headers
    )
    statement_link = None
    # Find the specific statement link
    for possible_key in statement_keys_map.get(statement_name.lower(), []):
        file_name = statement_file_name_dict.get(possible_key.lower())
        if file_name:
            statement_link = f"{base_link}/{file_name}"
            break
    if not statement_link:
        raise ValueError(f"Could not find statement file name for {statement_name}")
    # Fetch the statement
    try:
        statement_response = session.get(statement_link, headers=headers)
        statement_response.raise_for_status()  # Check for a successful request
        # Parse and return the content
        if statement_link.endswith(".xml"):
            return BeautifulSoup(
                statement_response.content, "lxml-xml", from_encoding="utf-8"
            )
        else:
            return BeautifulSoup(statement_response.content, "lxml")
    except requests.RequestException as e:
        raise ValueError(f"Error fetching the statement: {e}")


def extract_columns_values_and_dates_from_statement(soup):
    """
    Extracts columns, values, and dates from an HTML soup object representing a financial statement.

    Args:
        soup (BeautifulSoup): The BeautifulSoup object of the HTML document.

    Returns:
        tuple: Tuple containing columns, values_set, and date_time_index.
    """
    columns = []
    values_set = []
    date_time_index = get_datetime_index_dates_from_statement(soup)

    for table in soup.find_all("table"):
        unit_multiplier = 1
        special_case = False

        # Check table headers for unit multipliers and special cases
        table_header = table.find("th")
        if table_header:
            header_text = table_header.get_text()
            # Determine unit multiplier based on header text
            if "in Thousands" in header_text:
                unit_multiplier = 1
            elif "in Millions" in header_text:
                unit_multiplier = 1000
            # Check for special case scenario
            if "unless otherwise specified" in header_text:
                special_case = True

        # Process each row of the table
        for row in table.select("tr"):
            onclick_elements = row.select("td.pl a, td.pl.custom a")
            if not onclick_elements:
                continue

            # Extract column title from 'onclick' attribute
            onclick_attr = onclick_elements[0]["onclick"]
            column_title = onclick_attr.split("defref_")[-1].split("',")[0]
            columns.append(column_title)

            # Initialize values array with NaNs
            values = [np.NaN] * len(date_time_index)

            # Process each cell in the row
            for i, cell in enumerate(row.select("td.text, td.nump, td.num")):
                if "text" in cell.get("class"):
                    continue

                # Clean and parse cell value
                value = keep_numbers_and_decimals_only_in_string(
                    cell.text.replace("$", "")
                    .replace(",", "")
                    .replace("(", "")
                    .replace(")", "")
                    .strip()
                )
                if value:
                    value = float(value)
                    # Adjust value based on special case and cell class
                    if special_case:
                        value /= 1000
                    else:
                        if "nump" in cell.get("class"):
                            values[i] = value * unit_multiplier
                        else:
                            values[i] = -value * unit_multiplier

            values_set.append(values)

    return columns, values_set, date_time_index


def get_datetime_index_dates_from_statement(soup: BeautifulSoup) -> pd.DatetimeIndex:
    """
    Extracts datetime index dates from the HTML soup object of a financial statement.

    Args:
        soup (BeautifulSoup): The BeautifulSoup object of the HTML document.

    Returns:
        pd.DatetimeIndex: A Pandas DatetimeIndex object containing the extracted dates.
    """
    table_headers = soup.find_all("th", {"class": "th"})
    dates = [str(th.div.string) for th in table_headers if th.div and th.div.string]
    dates = [standardize_date(date).replace(".", "") for date in dates]
    index_dates = pd.to_datetime(dates)
    return index_dates


def standardize_date(date: str) -> str:
    """
    Standardizes date strings by replacing abbreviations with full month names.

    Args:
        date (str): The date string to be standardized.

    Returns:
        str: The standardized date string.
    """
    for abbr, full in zip(calendar.month_abbr[1:], calendar.month_name[1:]):
        date = date.replace(abbr, full)
    return date


def keep_numbers_and_decimals_only_in_string(mixed_string: str):
    """
    Filters a string to keep only numbers and decimal points.

    Args:
        mixed_string (str): The string containing mixed characters.

    Returns:
        str: String containing only numbers and decimal points.
    """
    num = "1234567890."
    allowed = list(filter(lambda x: x in num, mixed_string))
    return "".join(allowed)


def create_dataframe_of_statement_values_columns_dates(
    values_set, columns, index_dates
) -> pd.DataFrame:
    """
    Creates a DataFrame from statement values, columns, and index dates.

    Args:
        values_set (list): List of values for each column.
        columns (list): List of column names.
        index_dates (pd.DatetimeIndex): DatetimeIndex for the DataFrame index.

    Returns:
        pd.DataFrame: DataFrame constructed from the given data.
    """
    transposed_values_set = list(zip(*values_set))
    df = pd.DataFrame(transposed_values_set, columns=columns, index=index_dates)
    return df


def process_one_statement(ticker, accession_number, statement_name):
    """
    Processes a single financial statement identified by ticker, accession number, and statement name.

    Args:
        ticker (str): The stock ticker.
        accession_number (str): The SEC accession number.
        statement_name (str): Name of the financial statement.

    Returns:
        pd.DataFrame or None: DataFrame of the processed statement or None if an error occurs.
    """
    try:
        # Fetch the statement HTML soup
        soup = get_statement_soup(
            ticker,
            accession_number,
            statement_name,
            headers=headers,
            statement_keys_map=statement_keys_map,
        )
    except Exception as e:
        logging.error(
            f"Failed to get statement soup: {e} for accession number: {accession_number}"
        )
        return None

    if soup:
        try:
            # Extract data and create DataFrame
            columns, values, dates = extract_columns_values_and_dates_from_statement(
                soup
            )
            df = create_dataframe_of_statement_values_columns_dates(
                values, columns, dates
            )

            if not df.empty:
                # Remove duplicate columns
                df = df.T.drop_duplicates()
            else:
                logging.warning(
                    f"Empty DataFrame for accession number: {accession_number}"
                )
                return None

            return df
        except Exception as e:
            logging.error(f"Error processing statement: {e}")
            return None



### equity research


import requests
import json
from sec_cik_mapper import StockMapper
global symbol

def add_zeros_to_cik():
    global symbol
    while True:
        symbol = input("Enter a ticker symbol: ")
        mapper = StockMapper()
        num = mapper.ticker_to_cik[f'{symbol.upper()}']
        if num.isdigit() and 0 < len(num) <= 10:
            return str(num).zfill(10)
        else:
            print("Invalid input. Please enter a numeric CIK number with 1-10 digits.")

cik = add_zeros_to_cik()

sec_url = f"https://data.sec.gov/api/xbrl/companyfacts/CIK{cik}.json"
print(sec_url)

headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3',
    'Accept': 'application/json',
    'Accept-Language': 'en-US,en;q=0.5',
    'Accept-Encoding': 'gzip, deflate',
    'Connection': 'keep-alive'
}

response = requests.get(sec_url, headers=headers)

if response.status_code == 200:
    data = response.json()
    with open(f'./SEC_company_facts/{symbol.upper()}.json', 'w') as f:
        json.dump(data, f, indent=4)
    print(f"Response saved as {symbol.upper()}.json")
else:
    print("Failed to retrieve data")
https://data.sec.gov/api/xbrl/companyfacts/CIK0000921114.json
Response saved as response.json



### equity reserach

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from scipy.stats import norm
import requests

def get_world_bank_data(indicator, country, start_year, end_year):
    """
    Get data from the World Bank API.

    Parameters:
    indicator (str): The indicator code (e.g. 'SP.POP.TOTL' for total population)
    country (str): The country code (e.g. 'USA' for United States)
    start_year (int): The start year for the data
    end_year (int): The end year for the data

    Returns:
    pandas.DataFrame: The data from the World Bank API
    """
    url = f'http://api.worldbank.org/v2/country/{country}/indicator/{indicator}?format=json&per_page=1000&date={start_year}:{end_year}'
    response = requests.get(url)
    data = response.json()[1]
    df = pd.DataFrame(data)
    return df

def calculate_healthcare_costs(resistance_rate, population, avg_treatment_cost):
    """
    Calculate the cumulative annual healthcare costs for treating drug-resistant bacterial infections.

    Parameters:
    resistance_rate (float): The rate of antimicrobial resistance (e.g. 0.2 for 20%)
    population (int): The population of the country/region
    avg_treatment_cost (float): The average cost of treating a drug-resistant bacterial infection

    Returns:
    float: The cumulative annual healthcare costs
    """
    return resistance_rate * population * avg_treatment_cost

def estimate_market_size(year, growth_rate, initial_market_size):
    """
    Estimate the market size for novel antibacterial therapies.

    Parameters:
    year (int): The year for which to estimate the market size
    growth_rate (float): The annual growth rate of the market (e.g. 0.1 for 10%)
    initial_market_size (float): The initial market size

    Returns:
    float: The estimated market size for the given year
    """
    return initial_market_size * (1 + growth_rate) ** (year - 2023)

def job_creation_estimate(market_size, job_creation_rate):
    """
    Estimate the number of jobs created in the biotechnology and pharmaceutical industries.

    Parameters:
    market_size (float): The estimated market size for novel antibacterial therapies
    job_creation_rate (float): The rate of job creation per dollar of market size (e.g. 0.01 for 1%)

    Returns:
    int: The estimated number of jobs created
    """
    return int(market_size * job_creation_rate)

def plot_healthcare_costs(years, healthcare_costs):
    """
    Plot the cumulative annual healthcare costs over time.

    Parameters:
    years (list): The years for which to plot the data
    healthcare_costs (list): The cumulative annual healthcare costs for each year
    """
    plt.plot(years, healthcare_costs)
    plt.xlabel('Year')
    plt.ylabel('Cumulative Annual Healthcare Costs (billions USD)')
    plt.title('Cumulative Annual Healthcare Costs for Treating Drug-Resistant Bacterial Infections')
    plt.show()

def main():
    # Get population data from the World Bank API
    population_df = get_world_bank_data('SP.POP.TOTL', 'USA', 2010, 2030)
    population = population_df['value'].iloc[-1]

    # Example usage:
    resistance_rate = 0.2  # 20% antimicrobial resistance rate
    avg_treatment_cost = 10000  # average cost of treating a drug-resistant bacterial infection

    healthcare_costs = []
    years = list(range(2023, 2031))
    for year in years:
        healthcare_cost = calculate_healthcare_costs(resistance_rate, population, avg_treatment_cost)
        healthcare_costs.append(healthcare_cost)

    print(f"Cumulative annual healthcare costs: ${healthcare_costs[-1]:.2f} billion")

    # Plot the cumulative annual healthcare costs
    plot_healthcare_costs(years, healthcare_costs)

    initial_market_size = 1e9  # initial market size for novel antibacterial therapies (2023)
    growth_rate = 0.1  # 10% annual growth rate

    market_size = estimate_market_size(2030, growth_rate, initial_market_size)
    print(f"Estimated market size for novel antibacterial therapies in 2030: ${market_size:.2f} billion")

    job_creation_rate = 0.01  # 1% job creation rate per dollar of market size
    jobs_created = job_creation_estimate(market_size, job_creation_rate)
    print(f"Estimated number of jobs created in biotechnology and pharmaceutical industries: {jobs_created:,}")

if __name__ == "__main__":
    main()



### finpy

from finpy.edgar.company import company
from finpy.utils.components import sp500
from finpy.utils.components import russel3000
from finpy.utils.components import custom
import argparse
import pandas as pd
import os
import sqlite3
if __name__ == "__main__":
    parser = argparse.ArgumentParser(description='Name and Email.')
    parser.add_argument('-dir', default="app", help="app directory")
    parser.add_argument('-tick', help='ticker file list')
    parser.add_argument('-sp500', action="store_true", default=False, help="include all tickers in s&p 500")
    parser.add_argument('-russel3000', action="store_true", default=False, help="include all tickers in russel 3000")
    parser.add_argument('-header', default='{% extends "base.html" %}\n{% block content %}\n', help="header: html code before the table")
    parser.add_argument('-footer', default="{% endblock %}", help="footer: html code before the table")
    args = parser.parse_args()
    tickers = custom(args.tick)
    error_tickers = []
    r = []
    e = []
    if args.sp500:
        tickers += sp500();
    if args.russel3000:
        missing_file = os.path.join(args.dir, "missing.txt");
        r3k_missing = custom(missing_file)
        r3k = russel3000(format="pandas")
        r3k = r3k.drop(columns=['Notional Value', 'Asset Class', 'Location','Exchange','Currency','FX Rate','Market Currency','Accrual Date', 'Market Value', 'Shares'])
        r3k = r3k[~r3k['Ticker'].isin(r3k_missing)]
        tickers += list(r3k['Ticker'])
        r3k.insert(2, 'CJK', "")
    tickers = list(set(tickers))
    company_ticker_json_db = os.path.join(os.path.join(os.environ['FINPYDATA'], "edgar", "files", "company_tickers.db"))
    conn = sqlite3.connect(company_ticker_json_db)
    # print(tickers, len(tickers))
    for i in tickers:
        try:
            r.append(company(i, conn))
        except:
            error_tickers.append(i) 
            print("Error {}".format(i)) 
    conn.close()    
    data = []
    for i in r:
        latest_str = "<a href={}>{}</a>".format(i.latest_inline_xbrl, i.latest_form)
        cik_str = "<a href=https://data.sec.gov/api/xbrl/companyfacts/CIK{}.json>{}</a>".format(i.cik, i.cik)
        tick_str = "<a href={}_edgar.html>{}</a>".format(i.ticker, i.ticker)
        name_str = "<a href=https://www.sec.gov/edgar/browse/?CIK={}&owner=exclude>{}</a>".format(i.cik, i.name)
        r3k.loc[r3k.Ticker == i.ticker, 'CJK'] = cik_str 
        tick_file = os.path.join(args.dir, "{}.html".format(i.ticker))
        r3k.loc[r3k.Ticker == i.ticker, 'Name'] = name_str 
        if os.path.isfile(tick_file):
            r3k.loc[r3k.Ticker == i.ticker, 'Ticker'] = tick_str 
        df2 = {'Ranking' : i.ranking, 'Ticker' : tick_str, 'name' : name_str, 'cik' : cik_str, 'sic' : i.sic, 'sicDesciption' : i.sicDescription, 'latest': latest_str}
        data.append(df2)
    df = pd.DataFrame(data)   
    sics = set(df['sic'])
    for sic in sics:
        sic_file_name = os.path.join(args.dir, "sic_{}.html".format(sic))
        with open(sic_file_name, 'w') as sic_f:
            sic_f.write(args.header)
            sic_f.write(df.loc[df['sic'] == sic].sort_values(by=['Ranking']).to_html(escape=False,table_id="EdgarMain",index=False))    
            sic_f.write(args.footer)
        sic_str = "<a href=sic_{}.html>{}</a>".format(sic, sic)
        df.loc[df['sic'] == sic, 'sic'] = sic_str
    file_name = os.path.join(args.dir, 'edgar.html')
    with open(file_name , 'w') as f:
        f.write(args.header)
        f.write(df.sort_values(by=['Ranking']).to_html(escape=False,table_id="EdgarMain",index=False))    
        f.write(args.footer)
    if args.russel3000:
        russel3000_file = os.path.join(args.dir, "russel3000.html");
        with open(russel3000_file , 'w') as f:
            f.write(args.header)
            f.write(r3k.to_html(escape=False,table_id="Russel 3000",index=False))    
            f.write(args.footer)


from finpy.edgar.download import download
from finpy.edgar.download import async_download_url
from finpy.utils.components import sp500 
from finpy.utils.components import russel3000
from finpy.utils.components import custom
from aiolimiter import AsyncLimiter
import asyncio
import time
from datetime import date
import argparse
import os
import json
import sqlite3
from contextlib import closing
import logging
logger = logging.getLogger(__name__)

async def async_get_company_ticker_json(hdr, tickers, limiter, semaphore):
    url_str = 'https://www.sec.gov/files/company_tickers.json'
    if not os.path.isdir(os.path.join(os.environ['FINPYDATA'], "edgar", "files")):
        os.makedirs(os.path.join(os.environ['FINPYDATA'], "edgar", "files"));
    company_ticker_json_file = os.path.join(os.path.join(os.environ['FINPYDATA'], "edgar", "files", "company_tickers.json"))
    company_ticker_json_db = os.path.join(os.path.join(os.environ['FINPYDATA'], "edgar", "files", "company_tickers.db"))
    content = await async_download_url(url_str, hdr, limiter, semaphore)
    print(company_ticker_json_file)
    with open(company_ticker_json_file, 'w') as file:
        file.write(content)
    company_tickers_json = json.loads(content)
    try:       
        with closing(sqlite3.connect(company_ticker_json_db)) as conn:
            with closing(conn.cursor()) as cursor:
                cursor.execute('''CREATE TABLE COMPANY (ranking INT NOT NULL,
                                                        cik TEXT NOT NULL,
                                                        ticker TEXT PRIMARY KEY NOT NULL,
                                                        name TEXT NOT NULL,
                                                        sic INT NULL,
                                                        sicDescription TEXT NULL,
                                                        latest_filing_date TEXT NULL,
                                                        latest_report_date TEXT NULL,
                                                        latest_primaryDocument TEXT NULL,
                                                        latest_accessionNumber TEXT NULL,
                                                        latest_form TEXT NULL
                                                        );''')
                for i in company_tickers_json:
                    if company_tickers_json[i]["ticker"] in set(tickers):
                        cursor.execute("INSERT INTO COMPANY (ranking, cik, ticker, name) VALUES (?, ?, ?, ?)", \
                                   (i, str(company_tickers_json[i]["cik_str"]).zfill(10), company_tickers_json[i]["ticker"], company_tickers_json[i]["title"]))
            conn.commit()
    except:       
        with closing(sqlite3.connect(company_ticker_json_db)) as conn:
            with closing(conn.cursor()) as cursor:
                for i in company_tickers_json:
                    cursor.execute("""
                                   INSERT OR IGNORE INTO COMPANY (ranking, cik, ticker, name) VALUES (?, ?, ?, ?)
                                   """, \
                                   (i, str(company_tickers_json[i]["cik_str"]).zfill(10), company_tickers_json[i]["ticker"], company_tickers_json[i]["title"]))
            conn.commit()
             
async def main(name, email, nodownload, missing_tick, tickers):
    slot = 0.25
    limiter = AsyncLimiter(1, slot)
    tasks = []
    semaphore = asyncio.Semaphore(value=10)
    hdr = {'User-Agent' : name + email}
    await async_get_company_ticker_json(hdr, tickers, limiter, semaphore)
    company_ticker_json_db = os.path.join(os.path.join(os.environ['FINPYDATA'], "edgar", "files", "company_tickers.db"))
    r = {}
    num = 0

    with open(missing_tick, 'w') as file:
        for ticker in tickers:
            with closing(sqlite3.connect(company_ticker_json_db)) as conn:
                with closing(conn.cursor()) as cursor:
                    row = cursor.execute("SELECT * FROM COMPANY WHERE ticker = '{}'".format(ticker)).fetchone()
                    ticker_info = {}
                    try:
                        ticker_info['ranking'] = row[0]
                        ticker_info['cik'] = row[1]
                        ticker_info['ticker'] = row[2]
                        ticker_info['name'] = row[3]
                        ticker_info['sic'] = row[4]
                        ticker_info['sicDescription'] = row[5]
                        ticker_info['latest_filing_date'] = date.fromisoformat(row[6]) if isinstance(row[6], str) else row[6]
                        ticker_info['latest_report_date'] = date.fromisoformat(row[7]) if isinstance(row[7], str) else row[7]
                        ticker_info['latest_primaryDocument'] = row[8]
                        ticker_info['latest_accessionNumber'] = row[9]
                        ticker_info['latest_form'] = row[10]
                        tasks.append(download.async_create(ticker_info, name, email, nodownload, True, limiter, semaphore, r))
                    except:
                        file.write("{}\n".format(ticker))
                        print("Error", ticker)
    await asyncio.wait(tasks)
#    for i in r:
#        print(i)
    return r
    

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description='Name and Email.')
    parser.add_argument('-name', help='Name')
    parser.add_argument('-email', help='E-Mail address')
    parser.add_argument('-tick', help='ticker file list')
    parser.add_argument('-sp500', action="store_true", default=False, help="include all tickers in s&p 500")
    parser.add_argument('-russel3000', action="store_true", default=False, help="include all tickers in russel 3000")
    parser.add_argument('-nodownload', action="store_true", default=False, help="only update the database from the exisiting json files")
    parser.add_argument('-dir', default="app", help="app directory")
    args = parser.parse_args()
    tickers = custom(args.tick)
    if args.sp500:
        sp500 = sp500()
        tickers += sp500
    if args.russel3000:
        r3k = russel3000(format="list")
        tickers += r3k
    tickers = list(set(tickers))
    missing_tick = os.path.join(args.dir, "missing.txt")
    s = time.perf_counter()
    asyncio.run(main(args.name, args.email, args.nodownload, missing_tick,  tickers)) # Activate this line if the code is to be executed in VS Code
    # , etc. Otherwise deactivate it.
    # r = await main()          # Activate this line if the code is to be executed in Jupyter 
    # Notebook! Otherwise deactivate it.
    elapsed = time.perf_counter() - s
    print(f"Execution time: {elapsed:0.2f} seconds.")


from finpy.edgar.company import company
from finpy.utils.components import sp500
from finpy.utils.components import russel3000
from finpy.utils.components import custom
import argparse
import pandas as pd
import os
import sqlite3
if __name__ == "__main__":
    parser = argparse.ArgumentParser(description='Name and Email.')
    parser.add_argument('-dir', default="app", help="app directory")
    parser.add_argument('-tick', help='ticker file list')
    parser.add_argument('-sp500', action="store_true", default=False, help="include all tickers in s&p 500")
    parser.add_argument('-russel3000', action="store_true", default=False, help="include all tickers in russel 3000")
    parser.add_argument('-header', default='{% extends "base.html" %}\n{% block content %}\n', help="header: html code before the table")
    parser.add_argument('-footer', default="{% endblock %}", help="footer: html code before the table")
    args = parser.parse_args()
    tickers = custom(args.tick)
    error_tickers = []
    r = []
    e = []
    if args.sp500:
        tickers += sp500();
    if args.russel3000:
        missing_file = os.path.join(args.dir, "missing.txt");
        r3k_missing = custom(missing_file)
        r3k = russel3000(format="pandas")
        r3k = r3k.drop(columns=['Notional Value', 'Asset Class', 'Location','Exchange','Currency','FX Rate','Market Currency','Accrual Date', 'Market Value', 'Shares'])
        r3k = r3k[~r3k['Ticker'].isin(r3k_missing)]
        tickers += list(r3k['Ticker'])
        r3k.insert(2, 'CJK', "")
    tickers = list(set(tickers))
    company_ticker_json_db = os.path.join(os.path.join(os.environ['FINPYDATA'], "edgar", "files", "company_tickers.db"))
    conn = sqlite3.connect(company_ticker_json_db)
    # print(tickers, len(tickers))
    for i in tickers:
        try:
            r.append(company(i, conn))
        except:
            error_tickers.append(i) 
            print("Error {}".format(i)) 
    conn.close()    
    data = []
    for i in r:
        latest_str = "<a href={}>{}</a>".format(i.latest_inline_xbrl, i.latest_form)
        cik_str = "<a href=https://data.sec.gov/api/xbrl/companyfacts/CIK{}.json>{}</a>".format(i.cik, i.cik)
        tick_str = "<a href={}_edgar.html>{}</a>".format(i.ticker, i.ticker)
        name_str = "<a href=https://www.sec.gov/edgar/browse/?CIK={}&owner=exclude>{}</a>".format(i.cik, i.name)
        r3k.loc[r3k.Ticker == i.ticker, 'CJK'] = cik_str 
        tick_file = os.path.join(args.dir, "{}.html".format(i.ticker))
        r3k.loc[r3k.Ticker == i.ticker, 'Name'] = name_str 
        if os.path.isfile(tick_file):
            r3k.loc[r3k.Ticker == i.ticker, 'Ticker'] = tick_str 
        df2 = {'Ranking' : i.ranking, 'Ticker' : tick_str, 'name' : name_str, 'cik' : cik_str, 'sic' : i.sic, 'sicDesciption' : i.sicDescription, 'latest': latest_str}
        data.append(df2)
    df = pd.DataFrame(data)   
    sics = set(df['sic'])
    for sic in sics:
        sic_file_name = os.path.join(args.dir, "sic_{}.html".format(sic))
        with open(sic_file_name, 'w') as sic_f:
            sic_f.write(args.header)
            sic_f.write(df.loc[df['sic'] == sic].sort_values(by=['Ranking']).to_html(escape=False,table_id="EdgarMain",index=False))    
            sic_f.write(args.footer)
        sic_str = "<a href=sic_{}.html>{}</a>".format(sic, sic)
        df.loc[df['sic'] == sic, 'sic'] = sic_str
    file_name = os.path.join(args.dir, 'edgar.html')
    with open(file_name , 'w') as f:
        f.write(args.header)
        f.write(df.sort_values(by=['Ranking']).to_html(escape=False,table_id="EdgarMain",index=False))    
        f.write(args.footer)
    if args.russel3000:
        russel3000_file = os.path.join(args.dir, "russel3000.html");
        with open(russel3000_file , 'w') as f:
            f.write(args.header)
            f.write(r3k.to_html(escape=False,table_id="Russel 3000",index=False))    
            f.write(args.footer)

import sys
import csv
import matplotlib
matplotlib.use('Agg') # fix for matplotlib under multiprocessing
import matplotlib.pyplot as plt
import matplotlib.dates as mdates 
import datetime as dt
import sets
import dateutil
from finpy.utils import get_tickdata
from finpy.equity import Equity
from finpy.portfolio import Portfolio
from finpy.order import Order
import finpy.fpdateutil as du
if __name__ == '__main__':
    """
    python marketsim.py 1000000 orders.csv values.csv
    Where the number represents starting cash and orders.csv is a file of orders organized like this:
    2008-12-3, AAPL, BUY, 130
    2008-12-8, AAPL, SELL, 130
    2008-12-5, IBM, BUY, 50
    values.csv
    2008-12-3, 1000000
    2008-12-4, 1000010
    2008-12-5, 1000250
    """
    cash = sys.argv[1]
    order_file = sys.argv[2]
    value_file = sys.argv[3]
    order_list = []
    dt_timeofday = dt.timedelta(hours=16)
    with open(order_file, 'rU') as csvfile:
        order_reader = csv.reader(csvfile, delimiter=',', skipinitialspace=True)
        for row in order_reader:
            date = dateutil.parser.parse(row[0] + "-16")
            if len(row) == 4:
                o = Order(action=row[2], date=date, tick=row[1], shares=row[3])
            else:
                o = Order(action=row[2], date=date, tick=row[1], shares=row[3], price=row[4])
            order_list.append(o)
    # order_list needs to be sorted. Otherwise the algorithm won't work.
    date_list = [x.date for x in order_list]
    date_list.sort()
    dt_start = date_list[0]     
    dt_end = date_list[-1] 
    tick_set = sets.Set([x.tick for x in order_list])
    ls_symbols = ['$GSPC']
    while(tick_set):
        ls_symbols.append(tick_set.pop())
    ldt_timestamps = du.getNYSEdays(dt_start, dt_end, dt_timeofday)
    all_stocks = get_tickdata(ls_symbols=ls_symbols, ldt_timestamps=ldt_timestamps)
    pf = Portfolio(equities=all_stocks, cash=cash, dates=ldt_timestamps, order_list=order_list)
    pf.sim()
    equity_col = ['buy', 'sell', 'close']
    pf.csvwriter(csv_file=value_file, d=',', cash=False)
    print("The final value of the portfolio using the sample file is -- ", pf.total[-1])
    print("Details of the Performance of the portfolio :")
    print("Data Range :",    ldt_timestamps[0],    "to",    ldt_timestamps[-1])
    print("Sharpe Ratio of Fund :", pf.sharpe_ratio()) 
    print("Sortino Ratio of Fund :", pf.sortino()) 
    print("Sharpe Ratio of $GSPC :", pf.equities['$GSPC'].sharpe_ratio())
    print("Total Return of Fund :", pf.return_ratio())
    print(" Total Return of $GSPC :", pf.equities['$GSPC'].return_ratio())
    print("Standard Deviation of Fund :", pf.std())
    print(" Standard Deviation of $GSPC :", pf.equities['$GSPC'].std())
    print("Average Daily Return of Fund :", pf.avg_daily_return())
    print("Average Daily Return of $GSPC :", pf.equities['$GSPC'].avg_daily_return())
    print("Information Ratio of Fund:", pf.info_ratio(pf.equities['$GSPC']))
    fig = plt.figure()
    ax = fig.add_subplot(111)
    ax.plot(ldt_timestamps, pf.equities['$GSPC'].normalized())
    ax.plot(ldt_timestamps, pf.total/pf.total[0])
    legend = ['$GSPC', "Portfolio"]
    ax.legend(legend, loc=2)
    fig.autofmt_xdate()
    pdf_file = order_file + '.pdf'
    fig.savefig(pdf_file, format='pdf')
    beta, alpha = pf.beta_alpha(pf.equities['$GSPC'])
    print("Beta of the fund is ", beta, ". Alpha of the fund is ", alpha)
    

import pandas as pd
import re
import os
import json
from dyplot.bar import Bar
from bs4 import BeautifulSoup
import xml.etree.ElementTree as ET
from contextlib import closing
import sqlite3
import logging
logger = logging.getLogger(__name__)

class company():
    def __init__(self, ticker, conn, debug = True):
        self.ticker = ticker
        self.url = ""
        self.debug = debug
        with closing(conn.cursor()) as cursor:
            row = cursor.execute("SELECT * FROM COMPANY WHERE ticker = '{}'".format(self.ticker)).fetchone()
        self.ranking = row[0]
        self.cik = row[1]
        self.name = row[3]
        self.sic = row[4]
        self.sicDescription = row[5]
        self.latest_filing_date = row[6]
        self.latest_report_date = row[7]
        self.latest_primaryDocument = row[8]
        self.latest_accessionNumber = row[9]
        self.latest_form = row[10] 
        self.latest_filing_url = "https://www.sec.gov/Archives/edgar/data/{}/{}/{}.htm".format(self.cik, self.latest_accessionNumber.replace('-', ''), self.latest_primaryDocument)
        self.latest_inline_xbrl = "https://www.sec.gov/ix?doc=/Archives/edgar/data/{}/{}/{}".format(self.cik, self.latest_accessionNumber.replace('-', ''), self.latest_primaryDocument)
        self.fact_json_file = os.path.join(os.path.join(os.environ['FINPYDATA'], "edgar", "api", "xbrl", "companyfacts",'{}.json'.format(self.ticker)))
        self.edgar_html = "<a href={}_edgar.html>{}</a>".format(self.ticker, self.ticker)
        try:
            with open(self.fact_json_file, 'r') as file:
                self.fact_json = json.load(file)
        except:
            print("Error loading {} json file".format(ticker))

    def get_cik(self):
        return self.cik

    def get_ticker(self):
        return self.ticker

    def get_concept_json(self, concept, accounting='us-gaap'):
        try:
            concept_json = self.fact_json['facts'][accounting][concept]
        except:
            raise ValueError("concept {} does not exists for {}".format(concept, self.ticker))
        return concept_json

    def get_concept_quaterly_df(self, concept, accounting='us-gaap', units='USD'):
        concept_json = self.get_concept_json(concept, accounting)
        df = pd.DataFrame.from_records(concept_json['units'][units])
        df = df[df['frame'].notna()]
        df['start'] = pd.to_datetime(df.loc[:]['start'])
        df['end'] = pd.to_datetime(df.loc[:]['end'])
        qf = df[df['frame'].str.contains('Q')]
        qf.loc[:]['frame'] = qf['frame'].str[2:]
        qf = qf.set_index(pd.PeriodIndex(qf['frame'], freq='Q'))
        qf = qf.reindex(pd.PeriodIndex(pd.date_range( qf.iloc[0]['start'], qf.iloc[-1]['end'],freq = 'Q')))
        nan_indexes = qf[qf['start'].isna()].index.tolist()
        row_above_nan = [x-1 for x in nan_indexes]
        row_below_nan = [x+1 for x in nan_indexes]
        qf.loc[qf['start'].isna(),'start'] = list(qf.loc[row_above_nan, 'end'] + pd.Timedelta(1, 'd'))
        qf.loc[qf['end'].isna(),'end'] = list(qf.loc[row_below_nan, 'start'] - pd.Timedelta(1, 'd'))
        val_nan_row = qf.loc[qf['val'].isna()].index.tolist()
        row3_above_val_nan = [x - 3 for x in val_nan_row]
        start = list(qf.loc[row3_above_val_nan,'start'])
        end = list(qf.loc[val_nan_row,'end'])
        year_period = pd.DataFrame({'start': start, 'end': end})
        merged = df.merge(year_period, how='outer', indicator=True)
        merged[merged['_merge'] =='both'].val
        qf.loc[val_nan_row, 'val'] = 0
        qf.loc[val_nan_row, 'val'] = list(merged[merged['_merge'] =='both'].val) - qf['val'].rolling(4).sum()[val_nan_row]
        qf.loc[val_nan_row, 'accn'] = list(merged[merged['_merge'] =='both'].accn)
        qf.loc[val_nan_row, 'filed'] = list(merged[merged['_merge'] =='both'].filed)
        qf = qf.rename(columns={'val': concept})
        return qf

    def get_concept_yearly_df(self, concept, accounting='us-gaap', units='USD'):
        concept_json = self.get_concept_json(concept, accounting)
        try:
            df = pd.DataFrame.from_records(concept_json['units'][units])
        except:    
            raise ValueError("unit does not exists in concept {} for {}".format(concept, self.ticker))
        try:
            df = df[df['frame'].notna()]
        except:    
            raise ValueError("frame does not exists in concept {} for {}".format(concept, self.ticker))
        df['start'] = pd.to_datetime(df.loc[:]['start'])
        df['end'] = pd.to_datetime(df.loc[:]['end'])
        df['timedelta'] = df['end'] - df['start']
        one_year = pd.Timedelta(days=300)
        df = df[(df['timedelta'] >= one_year)].sort_values(by='start')
        df.loc[:]['frame'] = df['frame'].str[2:]
        df = df.set_index(pd.PeriodIndex(df['frame'], freq='Y'))
        df = df.rename(columns={'val': concept})
        return df

    def plot_concept_quaterly(self, concept, accounting='us-gaap', type = "Bar"):
        qf = self.get_concept_quaterly_df(concept, accounting)
        g = Bar(height=qf[concept], label=concept)
        g.set_xticklabels(list(qf[concept].index.strftime("%YQ%q")), "categories")
        g.option["axis"]["x"]["tick"]["rotate"] = 90 
        return(g.savefig(html_file="c3_bar.html", width="800px", height="800px"))

    def plot_concept_yearly(self, concept, accounting='us-gaap', type = "Bar"):
        qf = self.get_concept_yearly_df(concept, accounting)
        if self.debug:
            print(qf)
        g = Bar(height=qf[concept], label=concept)
        g.set_xticklabels(list(qf[concept].index.strftime("%Y")), "categories")
        g.option["axis"]["x"]["tick"]["rotate"] = 90 
        return(g.savefig(html_file="c3_bar.html", width="800px", height="800px"))

    def get_concepts(self, concept, duplicated_list=[], accounting='us-gaap', drop_columns=["accn", "fy", "fp", "frame", "timedelta"]):
        """
        The argument of concept should be in the following example format.
        concept = [{"name" : "NetIncomeLoss", "units" : 'USD'},
                    {"name" : "ProfitLoss", "units" : 'USD'},
                    {"name" : "RevenueFromContractWithCustomerExcludingAssessedTax", "units" : 'USD'},
                    {"name" : "Revenues", "units" : 'USD'}
                  ]
        duplicated_list = [
                     "Net Income": ["NetIncomeLoss", "ProfitLoss"],
                     "Revenues" : ["RevenueFromContractWithCustomerExcludingAssessedTax", "Revenues"]
                   ] 
        """
        concepts = {}
        for i in concept:
            try:
                df = self.get_concept_yearly_df(i['name'], accounting, i['units'])
            except ValueError as e:    
                logger.error(e.args)
                continue
            df = df.drop(columns=drop_columns)
            concepts[i['name']] = df
        if duplicated_list:
            concepts = self.remove_duplicated_concepts(concepts, duplicated_list)
        return concepts 

    def remove_duplicated_concepts(self, concepts, duplicated_list=[]):
        """
            concepts should be from the function of get_concepts.
            It is a dictionary of dataframes. The keys of the dictionary are the concepts of edgar. The dataframes has the facts 
            of these concepts.
            The following is an example of duplicated lists.
            duplicated_list = [ 
              ["NetIncomeLoss", "ProfitLoss"],
              ["RevenueFromContractWithCustomerExcludingAssessedTax", "Revenues"]
            ]
            For example, it checks the latest date of NetIncomeLoss and ProfitIncomeLoss from the duplicated list.
            It removes the keys with an earlier date and renames the key. 
        """
        for l in duplicated_list:
            dl = []
            for i in l:
                if (i in concepts) and (not concepts[i].empty):
                    dl.append(i)
            if not dl:        
                logger.error("All items in {} do not exist in {}.".format(l, self.ticker))
            if len(dl) > 1:
                p = dl[0]
                for i in range(1, len(dl)):
                    if ((concepts[dl[i]]['end'][-1] > concepts[p]['end'][-1]) or ((concepts[dl[i]]['end'][-1] == concepts[p]['end'][-1]) and (concepts[dl[i]]['end'][0] < concepts[p]['end'][0]))):
                        del concepts[p]
                        p = i
                    else:
                        del concepts[dl[i]]
        return concepts

    def get_latest_filing(self, cik_json, forms = ["10-Q", "10-K"]):
        filings_recent = zip(cik_json['filings']['recent']['form'],
                             cik_json['filings']['recent']['accessionNumber'],
                             cik_json['filings']['recent']['filingDate'],
                             cik_json['filings']['recent']['reportDate'],
                             cik_json['filings']['recent']['primaryDocument']
                            )
        for i in filings_recent:
            if i[0] in forms:
                return(i)


    def find_form_accessionNumbers(self, form):
        """
        All the sec forms:
        13F-HR
        8-K
        10-K
        10-Q
        """
        form_accessionNumbers = []
        accessionNumber = zip(self.cik_json['filings']['recent']['form'],
                              self.cik_json['filings']['recent']['accessionNumber'],
                              self.cik_json['filings']['recent']['filingDate'])
        for i in accessionNumber:
            if i[0] == form:
                form_accessionNumbers.insert(0, (i[1], i[2]))
        return form_accessionNumbers

    def get_all_forms(self, form):
        form_accessionNumbers = self.find_form_accessionNumbers(form)
        for a in form_accessionNumbers:
            aN = a[0].replace('-', '')
            if not os.path.isdir(os.path.join(self.concept_dir, aN)):
                print("form accession number " + a[0] + "of " + self.cik + " is never processed. fetching it...") 
                os.makedirs(os.path.join(self.concept_dir, aN))
                if form == "13F-HR":
                    f13_html = self.edgar_root + self.edgar_data + self.cik + "/" + aN + "/" + a[0] + "-index.html"
                    if self.debug:
                        print(f13_html)
                    f13_req = requests.get(f13_html, headers = self.hdr)
                    soup = BeautifulSoup(f13_req.text, 'html.parser')
                    match_re = "/xslForm13F_X01/" + "([\-\d]*|form13fInfoTable).xml"
                    pattern = re.compile(match_re)
                    xml_path = ""
                    for link in soup.find_all('a'):
                        href = link.get('href')
                        if pattern.search(href):
                            xml_path = href
                            html_file_name = link.contents[0]
                            break
                    if xml_path == "":
                        continue
                    f13_xml = self.edgar_root + xml_path
                    if self.debug:
                        print(f13_xml)
                    f13_xml_req = requests.get(f13_xml, headers = self.hdr)
                    f13_final_html = os.path.join(self.concept_dir, aN, html_file_name)
                    with open(f13_final_html , 'w') as file: 
                        file.write(f13_xml_req.text) 
                    with open(os.path.join(self.concept_dir, aN, "13F-HR"), 'w') as file: 
                        pass
            else:    
                print("form accession number %s of %s has been processed.", a[0], self.cik) 

import asyncio
import aiohttp
from aiolimiter import AsyncLimiter
import time
from datetime import date
import datetime
import pandas as pd
import re
import os
import sqlite3
import json
from contextlib import closing

async def async_download_url(url, hdr, limiter, semaphore):
    s = time.perf_counter()
    async with aiohttp.ClientSession(headers=hdr) as session:
        await semaphore.acquire()
        async with limiter:
            async with session.get(url) as resp:
                content = await resp.text()
                semaphore.release()
                return content
    
class download():
    def __init__(self, ticker_info, name, email, debug = True):
        self.ranking = ticker_info['ranking']
        self.cik = ticker_info['cik']
        self.ticker = ticker_info['ticker']
        self.name = ticker_info['name']
        self.sic = ticker_info['sic']
        self.sicDescription = ticker_info['sicDescription']
        self.latest_filing_date = ticker_info['latest_filing_date']
        self.latest_report_date = ticker_info['latest_report_date']
        self.latest_primaryDocument = ticker_info['latest_primaryDocument']
        self.latest_accessionNumber = ticker_info['latest_accessionNumber']
        self.latest_form = ticker_info['latest_form']
        self.company_ticker_json_file = os.path.join(os.path.join(os.environ['FINPYDATA'], "edgar", "files", "company_tickers.json"))
        self.company_ticker_json_db = os.path.join(os.path.join(os.environ['FINPYDATA'], "edgar", "files", "company_tickers.db"))
        self.hdr = {'User-Agent' : name + email}
        self.url = ""
        self.debug = debug
        
    @classmethod
    async def async_create(cls, ticker_info, name, email, nodownload, debug, limiter, semaphore, r):
        self = cls(ticker_info, name, email, debug)
        print(self.ticker, self.latest_filing_date)
        if (self.latest_filing_date == None) or (date.today() > (self.latest_filing_date + datetime.timedelta(days=90))):
            await self.async_get_cik_json(nodownload, limiter, semaphore)
        try:    
            await self.async_get_fact_json(limiter, semaphore)
        except:
            print("Error getting {} fact json: ".format(self.ticker))
        with closing(sqlite3.connect(self.company_ticker_json_db)) as conn:
            with closing(conn.cursor()) as cursor:
                cursor.execute("""
                  UPDATE COMPANY SET 
                  sic = ?, 
                  sicDescription = ?, 
                  latest_filing_date = ?, 
                  latest_report_date = ?, 
                  latest_primaryDocument = ?, 
                  latest_accessionNumber = ?, 
                  latest_form = ? where ticker = ?""", \
                  (self.sic, self.sicDescription, self.latest_filing_date, self.latest_report_date, self.latest_primaryDocument, self.latest_accessionNumber, self.latest_form , self.ticker))
            conn.commit()
        r[self.ticker] = self
        return self

    async def async_get_cik_json(self, nodownload, limiter, semaphore):
        self.edgar_root = "https://www.sec.gov/"
        self.edgar_data = "Archives/edgar/data/"
        url_str = 'https://data.sec.gov/submissions/CIK{}.json'.format(self.cik)
        if not os.path.isdir(os.path.join(os.environ['FINPYDATA'], "edgar", "submissions")):
            os.makedirs(os.path.join(os.environ['FINPYDATA'], "edgar", "submissions"))
        self.cik_json_file = os.path.join(os.environ['FINPYDATA'], "edgar", "submissions", self.ticker + '.json')
        if not os.path.exists(self.cik_json_file) or (date.fromtimestamp(os.path.getmtime(self.cik_json_file)) != date.today()) or not nodownload:   
            print("Get cjk json of {}. URL:{}".format(self.ticker, url_str))
            content = await async_download_url(url_str, self.hdr, limiter, semaphore)
            with open(self.cik_json_file, 'w') as file:
                file.write(content)
            cik_json = json.loads(content)
        else:
            with open(self.cik_json_file, 'r') as file:
                try: 
                    cik_json = json.load(file)
                except:
                    print("Error loading json file ", self.cik_json_file)
                    exit()
        self.sic = cik_json["sic"]
        self.sicDescription = cik_json["sicDescription"] 
        filings_recent = zip(cik_json['filings']['recent']['form'],
                             cik_json['filings']['recent']['accessionNumber'],
                             cik_json['filings']['recent']['filingDate'],
                             cik_json['filings']['recent']['reportDate'],
                             cik_json['filings']['recent']['primaryDocument']
                            )
        fin_forms = {"8-K", "10-Q", "10-K", "20-K", "20-F", "40-F"}
        for i in filings_recent:
            if i[0] in fin_forms:
                self.latest_form = i[0]
                self.latest_accessionNumber = i[1]
                self.latest_filing_date = date.fromisoformat(i[2])
                self.latest_report_date = date.fromisoformat(i[3])
                self.latest_primaryDocument = i[4]
                print(self.ticker, "the latest filing and report date of 10-Q or 10-K", self.latest_filing_date, self.latest_report_date)
                break

    def get_all_forms(self, form):
        form_accessionNumbers = self.find_form_accessionNumbers(form)
        for a in form_accessionNumbers:
            aN = a[0].replace('-', '')
            if not os.path.isdir(os.path.join(self.concept_dir, aN)):
                print("form accession number " + a[0] + "of " + self.cik + " is never processed. fetching it...") 
                os.makedirs(os.path.join(self.concept_dir, aN))
                if form == "13F-HR":
                    f13_html = self.edgar_root + self.edgar_data + self.cik + "/" + aN + "/" + a[0] + "-index.html"
                    if self.debug:
                        print(f13_html)
                    f13_req = requests.get(f13_html, headers = self.hdr)
                    soup = BeautifulSoup(f13_req.text, 'html.parser')
                    match_re = "/xslForm13F_X01/" + "([\-\d]*|form13fInfoTable).xml"
                    pattern = re.compile(match_re)
                    xml_path = ""
                    for link in soup.find_all('a'):
                        href = link.get('href')
            
    async def async_get_fact_json(self, limiter, semaphore):
        url_str = 'https://data.sec.gov/api/xbrl/companyfacts/CIK{}.json'.format(self.cik)
        if not os.path.isdir(os.path.join(os.environ['FINPYDATA'], "edgar", "api", "xbrl", "companyfacts")):
            os.makedirs(os.path.join(os.environ['FINPYDATA'], "edgar", "api", "xbrl", "companyfacts"));
        self.fact_json_file = os.path.join(os.path.join(os.environ['FINPYDATA'], "edgar", "api", "xbrl", "companyfacts",'{}.json'.format(self.ticker)))
        if self.debug:
            print(self.fact_json_file)
        if os.path.isfile(self.fact_json_file):
            print("TICKER latest_filing_date, fact_json_file_time")
            print(self.ticker, self.latest_filing_date, date.fromtimestamp(os.path.getmtime(self.fact_json_file)))
        try:    
            if not os.path.isfile(self.fact_json_file) or self.latest_filing_date > date.fromtimestamp(os.path.getmtime(self.fact_json_file)):
                if self.debug:
                    print(url_str)
                content = await async_download_url(url_str, self.hdr, limiter, semaphore)
                with open(self.fact_json_file, 'w') as file:
                    file.write(content)
        except:
            print("Error: {}, latest_filing_date: {}".format(self.ticker, self.latest_filing_date))

"""
(c) 2013 Tsung-Han Yang
This source code is released under the Apache license.  
Created on April 1, 2013
"""
import datetime as dt
import pandas as pd
import numpy as np
import finpy.utils.fpdateutil as du
import finpy.data.dataaccess as da
import finpy.utils.utils as ut
from .fincommon import FinCommon
from finpy.edgar.company import company
import os
import sqlite3

def get_tickdata(ls_symbols, ldt_timestamps, csv_col = [], fill=True, df=pd.DataFrame, actions=True, concepts=[]):
    """
        To get all price data of all tickers in ls_symbols within the list of ldt_timestamps
        :param ls_symbols: A list with all tickers
        :param ldt_timestamps: A list with all trading days within the time frame.
        :param fill: Whether to fill invalid data. Default is True.
    """
    c_dataobj = da.DataAccess("Yahoo", cachestalltime=0)
    if csv_col:
        ls_keys = csv_col
    else:    
        ls_keys = ['open', 'high', 'low', 'actual_close', 'close', 'volume']
    ldf_data = c_dataobj.get_data(ldt_timestamps, ls_symbols, ls_keys, actions)
    d_data = dict(list(zip(ls_symbols, ldf_data)))
    if fill == True:
        for s_key in ls_symbols:
            d_data[s_key] = d_data[s_key].fillna(method = 'ffill')
            d_data[s_key] = d_data[s_key].fillna(method = 'bfill')
            d_data[s_key] = d_data[s_key].fillna(1.0)
    stocks = dict()
    for s in ls_symbols:
        stocks[s] = df(index=ldt_timestamps, data=d_data[s])
        stocks[s]['shares'] = np.nan
        stocks[s].loc[ldt_timestamps[0],'shares'] = 0
    if len(concepts) != 0:
        company_ticker_json_db = os.path.join(os.path.join(os.environ['FINPYDATA'], "edgar", "files", "company_tickers.db"))
        try:
            conn = sqlite3.connect(company_ticker_json_db)
        except:
            print("Please run downlaod_edgar.py to create company db")
        for s in ls_symbols:
            c = company("NVDA", conn)
            facts =  c.get_concepts(concepts)

    return stocks 

"""
(c) 2013 Tsung-Han Yang
This source code is released under the Apache license.  
blacksburg98@yahoo.com
Created on April 1, 2013
"""
import numpy as np
from finpy.utils import utils as ut
class FinCommon():
    """
    This class has some common functions used by both Equity and Portfolio.
    This is an abstract class.
    """
    def avg_daily_return(self):
        """ 
        Average of the daily_return list 
            :return np.average(self.daily_return()):
        """
        return np.average(self.daily_return())

    def beta_alpha(self, benchmark):
        """
        benchmark is an Equity representing the market. 
        It can be S&P 500, Russel 2000, or your choice of market indicator.
        This function uses polyfit in numpy to find the closest linear equation.
            :return beta:
            :return alpha:
        """
        beta, alpha = np.polyfit(benchmark.normalized(), self.normalized(), 1)
        return beta, alpha

    def beta(self, benchmark):
        """
        benchmark is an Equity representing the market. 
        This function uses cov in numpy to calculate beta.
        """
        benchmark_close = benchmark.normalized() 
        C = np.cov(benchmark_close, self.normalized())/np.var(benchmark_close)
        beta = C[0][1]/C[0][0]
        return beta

    def sharpe_ratio(self, rf_tick="$TNX"):
        """
        Return the Original Sharpe Ratio.
        https://en.wikipedia.org/wiki/Sharpe_ratio
        rf_tick is Ten-Year treasury rate ticker at Yahoo.

        """
        return self.mean_excess_return(rf_tick)/self.excess_risk(rf_tick)

    def info_ratio(self, benchmark, rf_tick="$TNX"):
        """
        Information Ratio
        https://en.wikipedia.org/wiki/Information_ratio
        Information Ratio is defined as active return divided by active risk,
        where active return is the difference between the return of the security
        and the return of a selected benchmark index, and active risk is the
        standard deviation of the active return.
        """
        return self.mean_active_return(benchmark)/self.active_risk(benchmark)

    def appraisal_ratio(self, benchmark, rf_tick="$TNX"):
        """
        Appraisal Ratio
        https://en.wikipedia.org/wiki/Appraisal_ratio
        Appraisal Ratio is defined as residual return divided by residual risk,
        where residual return is the difference between the return of the security
        and the return of a selected benchmark index, and residual risk is the
        standard deviation of the residual return.
        """
        return self.mean_residual_return(benchmark, rf_tick)/self.residual_risk(benchmark, rf_tick)

    def excess_return(self, rf_tick="$TNX"):
        """
        An active return is the difference between the benchmark and the actual return.
        """
        return self.daily_return() - ut.riskfree_return(self.ldt_timestamps(), rf_tick="$TNX")

    def mean_excess_return(self, rf_tick="$TNX"):
        return np.mean(self.excess_return(rf_tick))

    def excess_risk(self, rf_tick="$TNX"):
        """
        $FVX is another option. Five-Year treasury rate.
        An excess risk is the standard deviation of the excess return.
        """
        return np.std(self.excess_return(rf_tick))

    def active_return(self, benchmark):
        """
        An active return is the difference between the benchmark and the actual return.
        """
        return self.daily_return() - benchmark.daily_return()

    def mean_active_return(self, benchmark):
        return np.mean(self.active_return(benchmark))

    def active_risk(self, benchmark):
        """
        An active risk is the standard deviation of the active return.
        """
        return np.std(self.active_return(benchmark))

    def residual_return(self, benchmark, rf_tick="$TNX"):
        """
        A residual return is the excess return minus beta times the benchmark excess return.
        """
        beta = self.beta(benchmark)
        return  self.excess_return(rf_tick="$TNX") - beta * benchmark.excess_return(rf_tick="$TNX")

    def residual_risk(self, benchmark, rf_tick="$TNX"):
        """
        Residual Risk is the standard deviation of the residual return.
        """
        return np.std(self.residual_return(benchmark, rf_tick))

    def mean_residual_return(self, benchmark, rf_tick="$TNX"):
        return np.mean(self.residual_return(benchmark, rf_tick))



class Transaction():
    def __init__(self, buy_date, buy_price, sell_date=None, sell_price=None):
        self.buy_date = buy_date
        self.sell_date = sell_date
        self.buy_price = buy_price
        self.sell_price = sell_price


"""
(c) 2013 Tsung-Han Yang
This source code is released under the Apache license.  
blacksburg98@yahoo.com
Created on April 1, 2013
"""
import datetime as dt
import pandas as pd
import numpy as np
import random 
import csv
from .order import Order
from .fincommon import FinCommon
import finpy.utils.fpdateutil as du
from finpy.utils import utils as ut
from finpy.financial.equity import get_tickdata

class Portfolio():
    """
    Portfolio has three items.
    equities is a panda Panel of equity data. 
    Reference by ticker. self.equities['AAPL']
    cash is a pandas series with daily cash balance.
    total is the daily balance.
    order_list is a list of Order
    """
    def __init__(self, equities, cash, dates, order_list=None):
        self.equities = pd.concat(equities, names=["tick", "date"])
        self.equities.sort_index(inplace=True)
#        self.equities = self.equities.reorder_levels(order=["date", "tick"])
        """
            :var equities: is a Panel of equities.
        """ 
        if order_list == None:
            self.order = pd.DataFrame(columns=['tick', 'date', 'action', 'shares', 'price'])
            self.order = self.order.set_index(["tick","date"])
        else:
            ol = order_list
            ol.sort(key=lambda x: x.date)
            self.order = pd.DataFrame.from_records([s.to_dict() for s in ol])
            self.order = self.order.set_index(["tick","date"])
            xi = self.order[self.order["price"].isnull()].index
            self.order.loc[xi, "price"] = self.equities.loc[xi, "close"]
        self.cash = pd.Series(index=dates)
        self.cash[0] = cash
        self.total = pd.Series(index=dates)
        self.total[0] = self.dailysum(dates[0])
        self.dates = dates

    def dailysum(self, date):
        " Calculate the total balance of the date."
        equities_total = np.nansum(self.equities.xs(key=date, level=1)['shares'] * self.equities.xs(key=date, level=1)['close'])
        total = equities_total + self.cash[date]
        return total

    def buy(self, shares, tick, price, date, update_ol=False):
        """
        Portfolio Buy 
        Calculate total, shares and cash upto the date.
        Before we buy, we need to update share numbers. "
        """
        self.cal_total(date)
        last_valid = self.equities.loc[(tick,slice(None)),'shares'].last_valid_index()[1]
        self.equities.loc[(tick, slice(last_valid, date)), 'shares'] = self.equities.loc[(tick, last_valid), 'shares']
        self.equities.loc[(tick, date), 'shares'] += shares
        self.cash[date] -= price*shares
        self.total[date] = self.dailysum(date)
        if update_ol:
            self.order = self.order.append(pd.DataFrame({"action": "buy", "shares" : shares, "price": self.equities.loc[(tick, date), 'close']}, [(tick, date)]))

    def sell(self, shares, tick, price, date, update_ol=False):
        """
        Portfolio sell 
        Calculate shares and cash upto the date.
        """
        self.cal_total(date)
        last_valid = self.equities.loc[(tick,slice(None)),'shares'].last_valid_index()[1]
        self.equities.loc[(tick, slice(last_valid, date)), 'shares'] = self.equities.loc[(tick, last_valid), 'shares']
        self.equities.loc[(tick, date), 'shares'] -= shares
        self.cash[date] += price*shares
        self.total[date] = self.dailysum(date)
        if update_ol:
            self.order = self.order.append(pd.DataFrame({"action": "sell", "shares" : shares, "price": self.equities.loc[(tick, date), 'close']}, [(tick, date)]))

    def fillna_cash(self, date):
        " fillna on cash up to date "
        update_start = self.cash.last_valid_index()
        update_end = date
        self.cash[update_start:update_end] = self.cash[update_start]
        return update_start, update_end 

    def fillna(self, date):
        """
        fillna cash and all equities.
        return update_start and update_end.
        """
        update_start, update_end = self.fillna_cash(date)
        for tick in self.equities.index.unique(0).tolist():
            self.equities.loc[(tick, slice(update_start, update_end)),'shares'] = self.equities.loc[(tick, update_start), 'shares']
        return update_start, update_end

    def cal_total(self, date=None):
        """
        Calculate total up to "date".
        """
        if date == None:
            equities_sum = pd.Series(index=self.ldt_timestamps())
            each_total = self.equities.loc[(slice(None),slice(None)),'close'] * self.equities.loc[(slice(None),slice(None)),'shares']
            equities_sum = each_total.groupby(level=1).sum()
            self.total = self.cash + equities_sum       
        else:
            start, end = self.fillna(date)
            equities_total_df = self.equities.loc[(slice(None),slice(start,end)),'shares'] * self.equities.loc[(slice(None),slice(start,end)),'close']
            equities_total = equities_total_df.groupby(level=1).sum()
            self.total[start:end ] = equities_total + self.cash[start:end]

    def put_orders(self):
        """
        Put the order list to the DataFrame.
        Update shares, cash columns of each Equity
        """
        for o in self.order:
            if o.action.lower() == "buy":
                self.buy(date=o.date, shares=np.float(o.shares), price=np.float(o.price), tick=o.tick)
            elif o.action.lower() == "sell":
                self.sell(shares=np.float(o.shares), tick=o.tick, price=np.float(o.price), date=o.date)

    def sim(self, ldt_timestamps=None):
        """
        Go through each day and calculate total and cash.
        """
        self.put_orders()
        if ldt_timestamps == None:
            ldt_timestamps = self.ldt_timestamps()
        dt_end = ldt_timestamps[-1]
        self.cal_total()

    def csvwriter(self, equity_col=None, csv_file="pf.csv", total=True, cash=True, d=','):
        """
        Write the content of the Portfolio to a csv file.
        If total is True, the total is printed to the csv file.
        If cash is True, the cash is printed to the csv file.
        equity_col specify which columns to print for an equity.
        The specified columns of each equity will be printed.
        """
        lines = []
        l = []
        l.append("Date")
        if total:
            l.append("Total")
        if cash:
            l.append("Cash")
        if equity_col != None:
            for e in self.equities:
                for col in equity_col:
                    label = e + col
                    l.append(label)
        lines.append(l)
        for i in self.ldt_timestamps():
            l = []
            l.append(i.strftime("%Y-%m-%d"))
            if total:
                l.append(round(self.total[i], 2))
            if cash:
                l.append(round(self.cash[i], 2))
            if equity_col != None:
                for e in self.equities.index.droplevel(1).drop_duplicates():
                    for col in equity_col:
                        l.append(round(self.equities.loc[(e, i), col], 2))
            lines.append(l)
        with open(csv_file, 'w') as fp:
            cw = csv.writer(fp, lineterminator='\n', delimiter=d)
            for line in lines:
                cw.writerow(line)

    def write_order_csv(self, csv_file="pf_order.csv", d=','):
        self.order.reorder_levels(["date", "tick"]).to_csv(path_or_buf = csv_file, sep = d, header = False, columns = ["action", "shares"])

    def daily_return(self,tick=None):
        """
        Return the return rate of each day, a list.
            :param tick: The ticker of the equity.
            :type string:
        """
        if tick == None:
            total = self.total
        else:
            total = self.equities.loc[(tick,slice(None)),'close'].droplevel(0)
        daily_rtn = total/total.shift(1)-1
        daily_rtn[0] = 0
        return np.array(daily_rtn)

    def avg_daily_return(self, tick=None):
        " Average of the daily_return list "
        return np.average(self.daily_return(tick))

    def std(self, tick=None):
        " Standard Deviation of the daily_return "
        return np.std(self.daily_return(tick))

    def normalized(self, tick=None):
        start = self.ldt_timestamps()[0]
        if tick == None:
            return self.total/self.total[0]
        else:
            return (self.equities.loc[(tick, slice(None)), 'close']/self.equities.loc[(tick, start), 'close']).droplevel(0)
    def normalized_price(self, tick):
        self.equities.loc[(tick, slice(None)),'open'] = self.equities.loc[(tick, slice(None)),'open'] * self.equities.loc[(tick, slice(None)),'close']/self.equities.loc[(tick, slice(None)),'actual_close']
        self.equities.loc[(tick, slice(None)),'high'] = self.equities.loc[(tick, slice(None)),'high'] * self.equities.loc[(tick, slice(None)),'close']/self.equities.loc[(tick, slice(None)),'actual_close']
        self.equities.loc[(tick, slice(None)),'low'] = self.equities.loc[(tick, slice(None)),'low'] * self.equities.loc[(tick, slice(None)),'close']/self.equities.loc[(tick, slice(None)),'actual_close']

    def sortino(self, k=252, tick=None):
        """
        Return Sortino Ratio. 
        You can overwirte the coefficient with k.
        The default is 252.
        """
        daily_rtn = self.daily_return(tick)
        negative_daily_rtn = daily_rtn[daily_rtn < 0]
        sortino_dev = np.std( negative_daily_rtn)
        sortino = (self.avg_daily_return(tick) / sortino_dev) * np.sqrt(k)
        return sortino

    def return_ratio(self, tick=None):
        " Return the return ratio of the period "
        if tick == None:
            return self.total[-1]/self.total[0]
        else:
            return self.equities.loc[(tick, self.ldt_timestamps()[-1]), 'close']/self.equities.loc[(tick, self.ldt_timestamps()[0]), 'close']

    def moving_average(self, window=20, tick=None):
        """
        Return an array of moving average. Window specified how many days in
        a window.
        """
        if tick == None:
            ma = pd.stats.moments.rolling_mean(self.total, window=window)
        else:
            ma = self.equities[tick].stats.moments.rolling_mean(window=window)
        ma[0:window] = ma[window]
        return ma

    def drawdown(self, window=10):
        """
        Find the peak within the retrospective window.
        Drawdown is the difference between the peak and the current value.
        """
        ldt_timestamps = self.ldt_timestamps()
        pre_timestamps = ut.pre_timestamps(ldt_timestamps, window)
        # ldf_data has the data prior to our current interest.
        # This is used to calculate moving average for the first window.
        merged_data = self.total[pd.Index(pre_timestamps[0]), ldt_timestamps[-1]]
        total_timestamps = merged_data.index
        dd = pd.Series(index=ldt_timestamps)
        j = 0
        for i in range(len(pre_timestamps), len(total_timestamps)):
            win_start = total_timestamps[i - window]
            win_end = total_timestamps[i]
            ts_value = merged_data[win_start:win_end]
            current = merged_data[win_end]
            peak = np.amax(ts_value)
            dd[j] = (peak-current)/peak
            j += 1
        return dd

    def random_choose_tick(self, exclude=[]):
        """
        Randomly return a ticker in the portfolio.
        The items in exclude list are not in the select pool.
        """
        ex_set = set(exclude)
        pf_set = set([x for x in self.equities])
        sel_ls = [s for s in pf_set - ex_set]
        return random.choice(sel_ls) 

    def equities_long(self, date):
        """
        Return the list of long equities on the date.
        "Long equities" means the number of shares of the equity is greater than 0.
        """
        return [x for x in self.equities if self.equities[x].shares[date] > 0]

    def ldt_timestamps(self):
        """
        Return an array of datetime objects.
        """
        ldt_index = self.total.index
        dt_start = ldt_index[0] 
        dt_end = ldt_index[-1] 
        dt_timeofday = dt.timedelta(hours=16)
        ldt_timestamps = du.getNYSEdays(dt_start, dt_end, dt_timeofday)
        return ldt_timestamps

    def excess_return(self, rf_tick="$TNX", tick=None):
        """
        An excess return is the difference between an asset's return and the riskless rate. 
        """
        return self.daily_return(tick=tick) - ut.riskfree_return(self.ldt_timestamps(), rf_tick=rf_tick)

    def mean_excess_return(self, rf_tick="$TNX", tick=None):
        return np.mean(self.excess_return(rf_tick=rf_tick, tick=tick))

    def residual_return(self, benchmark, rf_tick="$TNX", tick=None):
        """
        A residual return is the excess return minus beta times the benchmark excess return.
        """
        beta = self.beta(benchmark, tick)
        return  self.excess_return(rf_tick=rf_tick, tick=tick) - beta * self.excess_return(rf_tick=rf_tick, tick=benchmark)

    def mean_residual_return(self, benchmark, rf_tick="$TNX", tick=None):
        return np.mean(self.residual_return(benchmark=benchmark, rf_tick=rf_tick, tick=tick))

    def residual_risk(self, benchmark, rf_tick="$TNX", tick=None):
        """
        Residual Risk is the standard deviation of the residual return.
        """
        return np.std(self.residual_return(benchmark=benchmark, rf_tick=rf_tick, tick=tick))

    def active_return(self, benchmark, tick=None):
        """
        An active return is the difference between the benchmark and the actual return.
        """
        return self.daily_return(tick=tick) - self.daily_return(tick=benchmark)

    def mean_active_return(self, benchmark, tick=None):
        return np.mean(self.active_return(benchmark, tick))

    def beta_alpha(self, benchmark):
        """
        benchmark is an Equity representing the market. 
        It can be S&P 500, Russel 2000, or your choice of market indicator.
        This function uses polyfit in numpy to find the closest linear equation.
        """
        beta, alpha = np.polyfit(self.daily_return(tick=benchmark), self.daily_return(), 1)
        return beta, alpha

    def beta(self, benchmark, tick=None):
        """
        benchmark is an Equity representing the market. 
        This function uses cov in numpy to calculate beta.
        """
        benchmark_return = self.daily_return(tick=benchmark) 
        C = np.cov(benchmark_return, self.daily_return(tick=tick))/np.var(benchmark_return)
        beta = C[0][1]/C[0][0]
        return beta

    def excess_risk(self, rf_tick="$TNX", tick=None):
        """
        $FVX is another option. Five-Year treasury rate.
        An excess risk is the standard deviation of the excess return.
        """
        return np.std(self.excess_return(rf_tick=rf_tick, tick=tick))

    def active_risk(self, benchmark, tick=None):
        """
        An active risk is the standard deviation of the active return.
        """
        return np.std(self.active_return(benchmark, tick))

    def info_ratio(self, benchmark, rf_tick="$TNX", tick=None):
        """
        Information Ratio
        https://en.wikipedia.org/wiki/Information_ratio
        Information Ratio is defined as active return divided by active risk,
        where active return is the difference between the return of the security
        and the return of a selected benchmark index, and active risk is the
        standard deviation of the active return.
        """
        return self.mean_active_return(benchmark=benchmark, tick=tick)/self.active_risk(benchmark=benchmark, tick=tick)

    def appraisal_ratio(self, benchmark, rf_tick="$TNX", tick=None):
        """
        Appraisal Ratio
        https://en.wikipedia.org/wiki/Appraisal_ratio
        Appraisal Ratio is defined as residual return divided by residual risk,
        where residual return is the difference between the return of the security
        and the return of a selected benchmark index, and residual risk is the
        standard deviation of the residual return.
        """
        return self.mean_residual_return(benchmark, rf_tick, tick)/self.residual_risk(benchmark, rf_tick, tick)

    def sharpe_ratio(self, rf_tick="$TNX", tick=None):
        """
        Return the Original Sharpe Ratio.
        https://en.wikipedia.org/wiki/Sharpe_ratio
        rf_tick is Ten-Year treasury rate ticker at Yahoo.

        """
        return self.mean_excess_return(rf_tick=rf_tick, tick=tick)/self.excess_risk(rf_tick=rf_tick, tick=tick)

    def up_ratio(self, date, tick, days=10):
        """
        Return the ratio of the past up days.
        This function only applies to equities.
        """
        ldt_index = self.ldt_timestamps()
        last = date
        first = date-days
        up = 0.0
        dn = 0.0
        for i in range(first, last+1):
            if self.equities.loc[(tick, ldt_index[i]), 'close'] < self.equities.loc[(tick, ldt_index[i-1]), 'close']:
                dn += 1
            else:
                up += 1
        ratio = up / (dn + up)
        return ratio

    def dn_ratio(self, date,tick , days=10):
        """
        Return the ratio of the past down days. 
        This function only applies to equities.
        """
        ratio = 1.0 - self.up_ratio(date=date, tick=tick, days=days)
        return ratio

    def rolling_normalized_stdev(self, tick, window=50):
        """
        Return the rolling standard deviation of normalized price.
        This function only applies to equities.
        """
        ldt_timestamps = self.ldt_timestamps()
        pre_timestamps = ut.pre_timestamps(ldt_timestamps, window)
        # ldf_data has the data prior to our current interest.
        # This is used to calculate moving average for the first window.
        ldf_data = get_tickdata([tick], pre_timestamps)
        pre_data = pd.concat(ldf_data, names=["tick", "date"])
        merged_data = pd.concat([pre_data.loc[(tick, slice(None)), 'close'], self.equities.loc[(tick,slice(None)),'close']])
        all_timestamps = pre_timestamps.append(ldt_timestamps)
        merged_daily_rtn = (self.equities.loc[(tick,slice(None)),'close']/self.equities.loc[(tick,slice(None)),'close'].shift(1)-1)
        merged_daily_rtn[0] = 0
        sigma = merged_daily_rtn.rolling(window).std()
        return sigma.droplevel(0)[self.ldt_timestamps()]

    def max_rise(self, tick, date, window=20):
        """
        Find the maximum change percentage between the current date and the bottom of the retrospective window.

            :param tick: ticker
            :type tick: string
            :param date: date to calculate max_rise
            :type date: datetime
            :param window: The days of window to calculate max_rise.
            :type window: int
        """
        ldt_timestamps = self.ldt_timestamps() 
        pre_timestamps = ut.pre_timestamps(ldt_timestamps, window)
        first = pre_timestamps[0]
        # ldf_data has the data prior to our current interest.
        # This is used to calculate moving average for the first window.
        try:
            self.equities.loc[(tick, first), 'close']
            merged_data = self.equties.loc[(tick, slice(None)), 'close']
        except:
            ldf_data = get_tickdata([tick], pre_timestamps)
            pre_data = pd.concat(ldf_data, names=["tick", "date"])
            merged_data = pd.concat([pre_data.loc[(tick, slice(None)), 'close'], self.equities.loc[(tick,slice(None)),'close']])
        if(isinstance(date , int)):
            int_date = ldt_timestamps[date]
        else:
            int_date = date
        merged_data = merged_data.droplevel(0)   
        c = merged_data.index.get_loc(int_date)
        m = merged_data[c-window:c].min()
        r = (merged_data[c]-m)/merged_data[c]
        return r

    def max_fall(self, tick, date, window=20):
        """
        Find the change percentage between the top and the bottom of the retrospective window.

            :param tick: ticker
            :type tick: string
            :param date: date to calculate max_rise
            :type date: datetime
            :param window: The days of window to calculate max_rise.
            :type window: int
        """
        ldt_timestamps = self.ldt_timestamps() 
        pre_timestamps = ut.pre_timestamps(ldt_timestamps, window)
        first = pre_timestamps[0]
        # ldf_data has the data prior to our current interest.
        # This is used to calculate moving average for the first window.
        try:
            self.equities.loc[(tick, first), 'close']
            merged_data = self.equties.loc[(tick, slice(None)), 'close']
        except:
            ldf_data = get_tickdata([tick], pre_timestamps)
            pre_data = pd.concat(ldf_data, names=["tick", "date"])
            merged_data = pd.concat([pre_data.loc[(tick, slice(None)), 'close'], self.equities.loc[(tick,slice(None)),'close']])
        if(isinstance(date , int)):
            int_date = ldt_timestamps[date]
        else:
            int_date = date
        merged_data = merged_data.droplevel(0)   
        c = merged_data.index.get_loc(int_date)
        mx = merged_data[c-window:c].max()
        mn = merged_data[c-window:c].min()
        r = (mx-mn)/merged_data[c]
        return r

    def moving_average(self, tick, window=20):
        """
        Return an array of moving average. Window specified how many days in
        a window.

            :param tick: ticker
            :type tick: string
            :param window: The days of window to calculate moving average.
            :type window: int
        """
        mi = self.bollinger_band(tick=tick, window=window, mi_only=True)
        return mi

    def bollinger_band(self, tick, window=20, k=2, mi_only=False):
        """
        Return four arrays for Bollinger Band. The upper band at k times an N-period
        standard deviation above the moving average. The lower band at k times an N-period
        below the moving average.

            :param tick: ticker
            :type tick: string
            :param window: The days of window to calculate Bollinger Band.
            :type window: int
            :param k: k * 
            :return bo: bo['mi'] is the moving average. bo['lo'] is the lower band.
               bo['hi'] is the upper band. bo['ba'] is a seris of the position of the current 
               price relative to the bollinger band.
            :type bo: A dictionary of series.
        """
        ldt_timestamps = self.ldt_timestamps()
        pre_timestamps = ut.pre_timestamps(ldt_timestamps, window)
        # ldf_data has the data prior to our current interest.
        # This is used to calculate moving average for the first window.
        ldf_data = get_tickdata([tick], pre_timestamps)
        pre_data = pd.concat(ldf_data, names=["tick", "date"])
        merged_data = pd.concat([pre_data.loc[(tick, slice(None)), 'close'], self.equities.loc[(tick,slice(None)),'close']]).droplevel(0)
        bo = dict()
        bo['mi'] = merged_data.rolling(window).mean()[ldt_timestamps] 
        if mi_only:
            return bo['mi']
        else:
            sigma = merged_data.rolling(window).std()
            bo['hi'] = bo['mi'] + k * sigma[ldt_timestamps] 
            bo['lo'] = bo['mi'] - k * sigma[ldt_timestamps] 
            bo['ba'] = (merged_data[ldt_timestamps] - bo['mi']) / (k * sigma[ldt_timestamps])
            return bo

    def RSI(self, tick):
        """
        Relative Strength Index
        http://stockcharts.com/school/doku.php?id=chart_school:technical_indicators:relative_strength_index_rsi
        This function uses roughly 250 prior points to calculate RS.

            :param tick: The ticker to calculate RSI
            :type tick: string
            :return rsi[ldt_timestamps]: RSI series
        """
        ldt_timestamps = self.ldt_timestamps()
        pre_timestamps = ut.pre_timestamps(ldt_timestamps, 250)
        ldf_data = get_tickdata([tick], pre_timestamps)
        merged_data = pd.concat([ldf_data[tick]['close'], self.equities[tick]['close']])
        delta = merged_data.diff()
        gain = pd.Series(delta[delta > 0], index=delta.index).fillna(0)
        loss = pd.Series(delta[delta < 0], index=delta.index).fillna(0).abs()
        avg_gain = pd.Series(index=delta.index)
        avg_loss = pd.Series(index=delta.index)
        rsi = pd.Series(index=delta.index)
        avg_gain[14] = gain[1:15].mean()
        avg_loss[14] = loss[1:15].mean()
        for i in range(15, len(delta.index)):
            avg_gain[i] = (avg_gain[i-1]*13+gain[i])/14
            avg_loss[i] = (avg_loss[i-1]*13+loss[i])/14
            if avg_loss[i] == 0:
                rsi[i] = 100
            else:
                rs = avg_gain[i]/avg_loss[i]
                rsi[i] = 100 - 100/(1+rs)
        return(rsi[ldt_timestamps])

import pandas as pd
def sp500():
    table=pd.read_html('https://en.wikipedia.org/wiki/List_of_S%26P_500_companies')
    df = table[0]
    return list(df["Symbol"].str.replace('.', '-', regex=False))

def russel3000(format = 'list'):
    table = pd.read_csv('https://www.ishares.com/us/products/239714/ishares-russell-3000-etf/1467271812596.ajax?fileType=csv&fileName=IWV_holdings&dataType=fund', header=9, index_col=False)
    table = table[table.Ticker.str.match('^[A-Z]*$')]
    table = table[~table["Exchange"].str.contains("NO MARKET")]
    table["Ticker"] = table["Ticker"].str.replace('BRKB', 'BRK-B', regex=False)
    table["Ticker"] = table["Ticker"].str.replace('^BFB$', 'BF-B', regex=True)
    table["Ticker"] = table["Ticker"].str.replace('^BFA$', 'BF-A', regex=True)
    if format == 'list':
        return list(table["Ticker"])
    else:
        table.insert(0, 'Ranking', table.index)
        return table

def custom(file):
    f = open(file, "r")
    l = f.read().splitlines()
    return l

'''
(c) 2011, 2012 Georgia Tech Research Corporation
This source code is released under the New BSD license.  Please see
http://wiki.quantsoftware.org/index.php?title=QSTK_License
for license details.

Created on Jan 16, 2013

@author: Sourabh Bajaj
@contact: sourabhbajaj90@gmail.com
@summary: EventProfiler

'''

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

import finpy.dataaccess as da
import finpy.fpdateutil as du


def eventprofiler(df_events, all_stocks, i_lookback=20, i_lookforward=20,
                s_filename='study', b_market_neutral=True, b_errorbars=True,
                s_market_sym='SPY', out_pict=False):
    ''' Event Profiler for an event matix'''
    df_tmpclose = {}
    df_tmprets = {}
    for x in all_stocks:
        df_tmpclose[x] = all_stocks[x]['close'].copy()
        df_tmprets[x] = pd.Series(all_stocks[x].daily_return(), index=df_tmpclose[x].index)
    df_close = pd.DataFrame(df_tmpclose)
    df_rets = pd.DataFrame(df_tmprets)
    if b_market_neutral == True:
        df_rets = df_rets - df_rets[s_market_sym]
        del df_rets[s_market_sym]
        del df_events[s_market_sym]

    df_close = df_close.reindex(columns=df_events.columns)

    # Removing the starting and the end events
    df_events.values[0:i_lookback, :] = np.nan
    df_events.values[-i_lookforward:, :] = np.nan
    # Number of events
    i_no_events = np.nansum(df_events.values)
#    i_no_events = 0
#    for i in df_events:
#        for j in df_events[i]:
#            if j == 1:
#                i_no_events += 1
    na_event_rets = "False"

    # Looking for the events and pushing them to a matrix
    for i, s_sym in enumerate(df_events.columns):
        for j, dt_date in enumerate(df_events.index):
            if df_events[s_sym][dt_date] == 1:
                na_ret = df_rets[s_sym][j - i_lookback:j + 1 + i_lookforward]
                if type(na_event_rets) == type(""):
                    na_event_rets = na_ret
                else:
                    na_event_rets = np.vstack((na_event_rets, na_ret))

    # Computing daily rets and retuns
    na_event_rets = np.cumprod(na_event_rets + 1, axis=1)
    na_event_rets = (na_event_rets.T / na_event_rets[:, i_lookback]).T

    # Study Params
    na_mean = np.mean(na_event_rets, axis=0)
    na_std = np.std(na_event_rets, axis=0)
    li_time = list(range(-i_lookback, i_lookforward + 1))

    # Plotting the chart
    if out_pict:
        plt.clf()
        plt.axhline(y=1.0, xmin=-i_lookback, xmax=i_lookforward, color='k')
        if b_errorbars == True:
            plt.errorbar(li_time[i_lookback:], na_mean[i_lookback:],
                        yerr=na_std[i_lookback:], ecolor='#AAAAFF',
                        alpha=0.1)
        plt.plot(li_time, na_mean, linewidth=3, label='mean', color='b')
        plt.xlim(-i_lookback - 1, i_lookforward + 1)
        if b_market_neutral == True:
            plt.title('Market Relative mean return of ' +\
                    str(i_no_events) + ' events')
        else:
            plt.title('Mean return of ' + str(i_no_events) + ' events')
        plt.xlabel('Days')
        plt.ylabel('Cumulative Returns')
        plt.savefig(s_filename, format='pdf')
    return i_no_events

'''
(c) 2011, 2012 Georgia Tech Research Corporation
This source code is released under the New BSD license.  Please see
http://wiki.quantsoftware.org/index.php?title=QSTK_License
for license details.

Created on Jan 1, 2011

@author:Drew Bratcher
@contact: dbratcher@gatech.edu
@summary: Contains tutorial for backtester and report.

'''


import datetime as dt
from datetime import timedelta
import time as t
import numpy as np
import os
import pandas as pd


def _cache_dates():
    ''' Caches dates '''
    try:
        # filename = os.environ['QS'] + "/qstkutil/NYSE_dates.txt"
        filename = os.path.join(os.path.dirname(__file__), 'NYSE_dates.txt')
    except KeyError:
        print("Please be sure you have NYSE_dates.txt in the finpy/utils directory")
    with open(filename) as f:
        datestxt = [x.strip('\n') for x in f]
    dates = []
    for i in datestxt:
        dates.append(dt.datetime.strptime(i, "%m/%d/%Y"))
    return pd.Series(index=dates, data=dates)

GTS_DATES = _cache_dates()



def getMonthNames():
    return(['JAN','FEB','MAR','APR','MAY','JUN','JUL','AUG','SEP','OCT','NOV','DEC'])

def getYears(funds):
    years=[]
    for date in funds.index:
        if(not(date.year in years)):
            years.append(date.year)
    return(years)

def getMonths(funds,year):
    months=[]
    for date in funds.index:
        if((date.year==year) and not(date.month in months)):
            months.append(date.month)
    return(months)

def getDays(funds,year,month):
    days=[]
    for date in funds.index:
        if((date.year==year) and (date.month==month)):
            days.append(date)
    return(days)

def getDaysBetween(ts_start, ts_end):
    days=[]
    for i in range(0,(ts_end-ts_start).days):
        days.append(ts_start+timedelta(days=1)*i)
    return(days)

def getFirstDay(funds,year,month):
    for date in funds.index:
        if((date.year==year) and (date.month==month)):
            return(date)
    return('ERROR')

def getLastDay(funds,year,month):
    return_date = 'ERROR'
    for date in funds.index:
        if((date.year==year) and (date.month==month)):
            return_date = date
    return(return_date)

def getNextOptionClose(day, trade_days, offset=0):
    #get third friday in month of day
    #get first of month
    year_off=0
    if day.month+offset > 12:
        year_off = 1
        offset = offset - 12
    first = dt.datetime(day.year+year_off, day.month+offset, 1, hour=16)
    #get weekday
    day_num = first.weekday()
    #get first friday (friday - weekday) add 7 if less than 1
    dif = 5 - day_num
    if dif < 1:
        dif = dif+7
    #move to third friday
    dif = dif + 14
    friday = first+dt.timedelta(days=(dif-1))
    #if friday is a holiday, options expire then
    if friday in trade_days:
        month_close = first + dt.timedelta(days=dif)
    else:
        month_close = friday
    #if day is past the day after that
    if month_close < day:
        return_date = getNextOptionClose(day, trade_days, offset=1)
    else:
        return_date = month_close
    return(return_date)

def getLastOptionClose(day, trade_days):
    start = day
    while getNextOptionClose(day, trade_days)>=start:
        day= day - dt.timedelta(days=1)
    return(getNextOptionClose(day, trade_days))


def getNYSEoffset(mark, offset):
    ''' Returns NYSE date offset by number of days '''
    mark = mark.replace(hour=0, minute=0, second=0, microsecond=0)
    
    i = GTS_DATES.index.searchsorted(mark, side='right')
    # If there is no exact match, take first date in past
    if GTS_DATES[i] != mark:
        i -= 1
        
    ret = GTS_DATES[i + offset]

    ret = ret.replace(hour=16)

    return ret


def getNYSEdays(startday = dt.datetime(1964,7,5), endday = dt.datetime(2020,12,31),
    timeofday = dt.timedelta(0)):
    """
    @summary: Create a list of timestamps between startday and endday (inclusive)
    that correspond to the days there was trading at the NYSE. This function
    depends on a separately created a file that lists all days since July 4,
    1962 that the NYSE has been open, going forward to 2020 (based
    on the holidays that NYSE recognizes).

    @param startday: First timestamp to consider (inclusive)
    @param endday: Last day to consider (inclusive)
    @return list: of timestamps between startday and endday on which NYSE traded
    @rtype datetime
    """
    dates = GTS_DATES[startday:endday]
    return(dates)

def getNextNNYSEdays(startday, days, timeofday):
    """
    @summary: Create a list of timestamps from startday that is days days long
    that correspond to the days there was trading at  NYSE. This function
    depends on the file used in getNYSEdays and assumes the dates within are
    in order.
    @param startday: First timestamp to consider (inclusive)
    @param days: Number of timestamps to return
    @return list: List of timestamps starting at startday on which NYSE traded
    @rtype datetime
    """
    try:
        # filename = os.environ['QS'] + "/qstkutil/NYSE_dates.txt"
        filename = os.path.join(os.path.dirname(__file__), 'NYSE_dates.txt')
    except KeyError:
        print("Please be sure to set the value for QS in config.sh or\n")
        print("in local.sh and then \'source local.sh\'.\n")

    datestxt = np.loadtxt(filename,dtype=str)
    dates=[]
    for i in datestxt:
        if(len(dates)<days):
            if((dt.datetime.strptime(i,"%m/%d/%Y")+timeofday)>=startday):
                dates.append(dt.datetime.strptime(i,"%m/%d/%Y")+timeofday)
    return(dates)

def getPrevNNYSEday(startday, timeofday):
    """
    @summary: This function returns the last valid trading day before the start
    day, or returns the start day if it is a valid trading day. This function
    depends on the file used in getNYSEdays and assumes the dates within are
    in order.
    @param startday: First timestamp to consider (inclusive)
    @param days: Number of timestamps to return
    @return list: List of timestamps starting at startday on which NYSE traded
    @rtype datetime
    """
    try:
        # filename = os.environ['QS'] + "/qstkutil/NYSE_dates.txt"
        filename = os.path.join(os.path.dirname(__file__), 'NYSE_dates.txt')
    except KeyError:
        print("Please be sure to set the value for QS in config.sh or\n")
        print("in local.sh and then \'source local.sh\'.\n")

    datestxt = np.loadtxt(filename,dtype=str)

    #''' Set return to first day '''
    dtReturn = dt.datetime.strptime( datestxt[0],"%m/%d/%Y")+timeofday

    #''' Loop through all but first '''
    for i in datestxt[1:]:
        dtNext = dt.datetime.strptime(i,"%m/%d/%Y")

        #''' If we are > startday, then use previous valid day '''
        if( dtNext > startday ):
            break

        dtReturn = dtNext + timeofday

    return(dtReturn)

def ymd2epoch(year, month, day):
    """
    @summary: Convert YMD info into a unix epoch value.
    @param year: The year
    @param month: The month
    @param day: The day
    @return epoch: number of seconds since epoch
    """
    return(t.mktime(dt.date(year,month,day).timetuple()))

def epoch2date(ts):
    """
    @summary Convert seconds since epoch into date
    @param ts: Seconds since epoch
    @return thedate: A date object
    """
    tm = t.gmtime(ts)
    return(dt.date(tm.tm_year,tm.tm_mon,tm.tm_mday))


def _trade_dates(dt_start, dt_end, s_period):
    '''
    @summary: Generate dates on which we need to trade
    @param c_strat: Strategy config class
    @param dt_start: Start date
    @param dt_end: End date
    '''

    ldt_timestamps = getNYSEdays(dt_start,
                dt_end, dt.timedelta(hours=16) )


    # Use pandas reindex method instead
    # Note, dates are index as well as values, we select based on index
    # but return values since it is a numpy array of datetimes instead of
    # pandas specific.
    ts_dates = pd.Series(index=ldt_timestamps, data=ldt_timestamps)

    # These are the dates we want
    if s_period[:2] == 'BW':
        # special case for biweekly

        dr_range = pd.DateRange(dt_start, dt_end,
                                timeRule=s_period[1:])
        dr_range = np.asarray(dr_range)
        li_even = np.array(list(range(len(dr_range))))
        dr_range = dr_range[li_even[li_even % 2 == 0]]
    else:
        dr_range = pd.DateRange(dt_start, dt_end,
                                timeRule=s_period)
        dr_range = np.asarray(dr_range)


    # Warning, we MUST copy the date range, if we modify it it will be returned
    # in it's modified form the next time we use it.
    dr_range = np.copy(dr_range)
    dr_range += pd.DateOffset(hours=16)
    ts_dates = ts_dates.reindex( dr_range, method='bfill' )
    ldt_dates = ts_dates[ts_dates.notnull()].values

    #Make unique
    sdt_unique = set()
    ldt_dates = [x for x in ldt_dates
                 if x not in sdt_unique and not sdt_unique.add(x)]

    return ldt_dates


"""
(c) 2013 Tsung-Han Yang
This source code is released under the Apache license.  
blacksburg98@yahoo.com
Created on November 24, 2014
"""
import datetime as dt
from . import fpdateutil as du
import numpy as np
import pandas as pd
import finpy.data.dataaccess as da

def riskfree_return(ldt_timestamps, rf_tick="$TNX"):
    """
    Default is $TNX. Ten-year treasury rate
    $FVX is another option. Five-Year treasury rate.
    """
    c_dataobj = da.DataAccess('Yahoo', cachestalltime=0)
    ls_keys = ['open', 'high', 'low', 'close', 'volume', 'actual_close']
    ldf_data = c_dataobj.get_data(ldt_timestamps, [rf_tick], ls_keys)
    rf = (ldf_data[0]['close']/100)/365
    return rf

def pre_timestamps(ldt_timestamps, window):
    """
    Return an list of timestamps.
    Start roughly from ldt_timestamps[0] - window.
    End at ldt_timestamps[0] - 1
    """
    dt_timeofday = dt.timedelta(hours=16)
    days_delta = dt.timedelta(days=(np.ceil(window*7/5)+20))
    dt_start = ldt_timestamps[0] - days_delta
    dt_end = ldt_timestamps[0] - dt.timedelta(days=1)
    pre_timestamps = du.getNYSEdays(dt_start, dt_end, dt_timeofday)
    return pre_timestamps 

def get_max_draw_down(ts_vals):
    """
    @summary Returns the max draw down of the returns.
    @param ts_vals: 1d numpy array or fund list
    @return Max draw down

    """
    MDD = 0
    DD = 0
    peak = -99999
    for value in ts_vals:
        if (value > peak):
            peak = value
        else:
            DD = (peak - value) / peak
        if (DD > MDD):
            MDD = DD
    return MDD



### intelligetn investor
import { httpsGET } from './httpsGET';
import { CompanyData } from '@/src/types/CompanyData';

const COMPANY_FACTS_URL = 'https://data.sec.gov/api/xbrl/companyfacts/CIK';

export async function fetchCompanyData(cik: string, email: string) {
  const url = `${COMPANY_FACTS_URL}${"0".repeat(10 - cik.toString().length)}${cik}.json`;
  const data = await httpsGET(url, 'data.sec.gov', email);
  const parsedData: CompanyData = JSON.parse(data) ;
  return parsedData;
}



import { App, normalizePath, TFile, Notice } from 'obsidian';
import { IntelligentInvestorSettings } from '@/src/settings/IntelligentInvestorSettings';
import { useState, useEffect } from 'react';
import { CompanyData, Share } from '@/src/types/CompanyData';
import getUnits from '@/src/components/getUnits';

// import { Line } from 'react-chartjs-2';
// import { Chart as ChartJS, CategoryScale, LinearScale, PointElement, LineElement, Title, Tooltip, Legend } from 'chart.js';
// ChartJS.register(CategoryScale, LinearScale, PointElement, LineElement, Title, Tooltip, Legend);

interface CompanyDataDisplayProps {
  app: App;
  settings: IntelligentInvestorSettings;
  companyData: CompanyData;
}

export default function CompanyDataDisplay({ app, settings, companyData }: CompanyDataDisplayProps) {
  const [formType, setFormType] = useState("10-Q");
  const [companyMetric, setCompanyMetric] = useState("Assets");
  const [years, setYears] = useState<number[]>([]);
  const [startYear, setStartYear] = useState<number>(0);
  const [endYear, setEndYear] = useState<number>(0);
  const [displayData, setDisplayData] = useState<Share[]>([]);
  const fiscalQuarterOrder = { Q1: 1, Q2: 2, Q3: 3, Q4: 4, FY: 5 } as any;

  useEffect(() => {
    const units = getUnits(companyData, companyMetric);
    const years = Array.from(
      new Set(
        units
          .map((report: any) => report.fy)
          .sort((a: number, b: number) => a - b)
      )
    ) as number[];
    setYears(years);

    if (startYear === 0) {
      setStartYear(years[0]);
    }
    if (endYear === 0) {
      setEndYear(years[years.length - 1]);
    }

    const data = units
      .filter((datum: any) => {
        return datum.form === formType && datum.fy >= startYear && datum.fy <= endYear
      })
      .slice()
      .sort((a: any, b: any) => {
        if (a.fy !== b.fy) {
          return a.fy - b.fy;
        }

        if (fiscalQuarterOrder[a.fp] !== fiscalQuarterOrder[b.fp]) {
          return fiscalQuarterOrder[a.fp] - fiscalQuarterOrder[b.fp];
        }

        const endDateComparison = new Date(a.end).getTime() - new Date(b.end).getTime();
        if (endDateComparison !== 0) {
          return endDateComparison;
        }

        return a.frame ? 1 : -1;
      });

    console.log("Displaying Data: ", data);

    setDisplayData(data);
  }, [formType, companyMetric, startYear, endYear]);

  const exportCompanyData = async () => {
    console.log("Exporting company data");
    const csvContent = displayData.map((datum) => {
      const row = [datum.fp, datum.fy, datum.filed, datum.end, datum.frame, datum.val];
      return row.join(',');
    }).join('\n');

    const exportDirectory = settings.exportDirectory;
    const filePath = normalizePath(`${exportDirectory}/${companyData.entityName} ${companyMetric}.csv`);
    const existingFile = app.vault.getAbstractFileByPath(filePath);

    if (existingFile instanceof TFile) {
        await app.vault.modify(existingFile, csvContent);
    } else {
        await app.vault.create(filePath, csvContent);
    }

    new Notice(`Company data exported to ${filePath}`);
  }

  const handleSelectFormType = (e: React.ChangeEvent<HTMLSelectElement>) => {
    setFormType(e.target.value);
  }

  const handleSelectMetric = (e: React.ChangeEvent<HTMLSelectElement>) => {
    setCompanyMetric(e.target.value);
  }

  const handleSelectStartYear = (e: React.ChangeEvent<HTMLSelectElement>) => {
    setStartYear(Number(e.target.value));
  }

  const handleSelectEndYear = (e: React.ChangeEvent<HTMLSelectElement>) => {
    setEndYear(Number(e.target.value));
  }

  return (
    <div>
      <div className="company-data-control">
        <label className="control-label">Company Metric</label>
        <select
          className="select-metric-control"
          onChange={handleSelectMetric}
        >
          {Object.keys(companyData.facts["us-gaap"]).map((metric, index) => (
            <option key={index} value={metric}>{metric}</option>
          ))}
        </select>
      </div>
      <div className="company-data-controls">
        <div className="company-data-control">
          <label className="control-label">Select Form Type</label>
          <select
            className="select-control"
            onChange={handleSelectFormType}
          >
            <option value="10-Q">10-Q</option>
            <option value="10-K">10-K</option>
          </select>
        </div>
        <div className="company-data-control">
          <label className="control-label">
            Start Year
          </label>
          <select
            className="select-control"
            value={startYear}
            onChange={handleSelectStartYear}
          >
            {years.map((year, index) => (
              <option key={index} value={year}>{year}</option>
            ))}
          </select>
        </div>
        <div className="company-data-control">
          <label className="control-label">
            End Year
          </label>
          <select
            className="select-control"
            value={endYear}
            onChange={handleSelectEndYear}
          >
            {years.map((year, index) => (
              <option key={index} value={year}>{year}</option>
            ))}
          </select>
        </div>
        <button className="export-button" onClick={exportCompanyData}>
          Export
        </button>
      </div>
      <div className="company-data-container">
        <table className="company-data-table">
          <thead>
            <tr>
              <th>FP</th>
              <th>FY</th>
              <th>Filed</th>
              <th>End</th>
              <th>Frame</th>
              <th>Value</th>
            </tr>
          </thead>
          <tbody>
            {displayData.map((data: any, index: number) => (
              <tr key={index} className={index % 2 === 0 ? "company-data-row-light" : "company-data-row-dark"}>
                <td className="company-data-cell">{data.fp}</td>
                <td className="company-data-cell">{data.fy}</td>
                <td className="company-data-cell">{data.filed}</td>
                <td className="company-data-cell">{data.end}</td>
                <td className="company-data-cell">{data.frame}</td>
                <td className="company-data-cell">{data.val}</td>
              </tr>
            ))}
          </tbody>
        </table>
      </div>
    </div>
  );
}

import {
  CompanyData,
  UsGaap,
  DefinedContributionPlanEmployerMatchingContributionPercentUnits,
  CommonStockParOrStatedValuePerShareUnits,
  NumberOfOperatingSegmentsUnits,
  EntityCommonStockSharesOutstandingUnits,
  EntityPublicFloatUnits
} from '@/src/types/CompanyData';

const DefinedContributionPlanEmployerMatchingContributionPercent = [
  "EffectiveIncomeTaxRateReconciliationAtFederalStatutoryIncomeTaxRate",
  "RevenueRemainingPerformanceObligationPercentage",
  "FinanceLeaseWeightedAverageDiscountRatePercent",
  "LineOfCreditFacilityUnusedCapacityCommitmentFeePercentage",
  "OperatingLeaseWeightedAverageDiscountRatePercent",
  "ShareBasedCompensationArrangementByShareBasedPaymentAwardFairValueAssumptionsExpectedDividendRate",
  "DefinedContributionPlanEmployerMatchingContributionPercent",
  "RestructuringAndRelatedCostNumberOfPositionsEliminatedPeriodPercent"
]
const CommonStockParOrStatedValuePerShare = [
  "ShareBasedCompensationArrangementByShareBasedPaymentAwardOptionsForfeituresAndExpirationsInPeriodWeightedAverageExercisePrice",
  "ShareBasedCompensationArrangementByShareBasedPaymentAwardOptionsExercisableWeightedAverageExercisePrice",
  "PreferredStockParOrStatedValuePerShare",
  "EarningsPerShareBasicAndDiluted",
  "ShareBasedCompensationArrangementsByShareBasedPaymentAwardOptionsExercisesInPeriodWeightedAverageExercisePrice",
  "ShareBasedCompensationArrangementByShareBasedPaymentAwardOptionsOutstandingWeightedAverageExercisePrice","TemporaryEquityParOrStatedValuePerShare",
  "ShareBasedCompensationArrangementByShareBasedPaymentAwardOptionsGrantsInPeriodWeightedAverageGrantDateFairValue",
  "EarningsPerShareBasic",
  "EarningsPerShareDiluted"
]
const NumberOf = [
  "NumberOfReportingUnits",
  "NumberOfOperatingSegments"
]
const EntityCommonStockSharesOutstanding = [
  "AntidilutiveSecuritiesExcludedFromComputationOfEarningsPerShareAmount",
  "CommonStockSharesAuthorized",
  "CommonStockSharesIssued",
  "CommonStockSharesOutstanding",
  "ConversionOfStockSharesConverted1",
  "PreferredStockSharesAuthorized",
  "PreferredStockSharesIssued",
  "ConversionOfStockSharesIssued1",
  "PreferredStockSharesOutstanding",
  "ShareBasedCompensationArrangementByShareBasedPaymentAwardOptionsExercisableNumber",
  "ShareBasedCompensationArrangementByShareBasedPaymentAwardOptionsForfeituresAndExpirationsInPeriod",
  "ShareBasedCompensationArrangementByShareBasedPaymentAwardOptionsOutstandingNumber",
  "StockIssuedDuringPeriodSharesStockOptionsExercised",
  "TemporaryEquitySharesAuthorized",
  "TemporaryEquitySharesIssued",
  "TemporaryEquitySharesOutstanding",
  "WeightedAverageNumberOfShareOutstandingBasicAndDiluted",
  "WeightedAverageNumberOfDilutedSharesOutstanding",
  "ShareBasedCompensationArrangementByShareBasedPaymentAwardOptionsGrantsInPeriodGross",
  "WeightedAverageNumberOfSharesOutstandingBasic"
]

export default function getUnits(companyData: CompanyData, metric: string): any {
  if (DefinedContributionPlanEmployerMatchingContributionPercent.includes(metric)) {
    const units = companyData.facts["us-gaap"][metric as keyof UsGaap].units as DefinedContributionPlanEmployerMatchingContributionPercentUnits;
    return units.pure;
  } else if (CommonStockParOrStatedValuePerShare.includes(metric)) {
    const units = companyData.facts["us-gaap"][metric as keyof UsGaap].units as CommonStockParOrStatedValuePerShareUnits;
    return units["USD/shares"];
  } else if (NumberOf.includes(metric)) {
    const units = companyData.facts["us-gaap"][metric as keyof UsGaap].units as NumberOfOperatingSegmentsUnits;
    return units.Segment;
  } else if (EntityCommonStockSharesOutstanding.includes(metric)) {
    const units = companyData.facts["us-gaap"][metric as keyof UsGaap].units as EntityCommonStockSharesOutstandingUnits;
    return units.shares;
  } else {
    const units = companyData.facts["us-gaap"][metric as keyof UsGaap].units as EntityPublicFloatUnits;
    return units.USD;
  }
}


### sec api



import os
import requests
import pickle
import pandas as pd
# ------------------------------------------------------------
def download_company_facts(cik: str):

    url = f'https://data.sec.gov/api/xbrl/companyfacts/CIK{cik}.json'

    headers = { 'User-Agent': os.environ.get('SEC_GOV_USER_AGENT') }

    response = requests.get(url, headers=headers)

    response.raise_for_status()

    return response
# ------------------------------------------------------------
# symbol = 'hood'
# cik = '0001783879'

def download_and_save_company_facts(symbol: str, cik: str):

    response = download_company_facts(cik)

    file = os.path.join('data', symbol, 'company_facts_response.pkl')

    os.makedirs(os.path.dirname(file), exist_ok=True)

    with open(file, 'wb') as f:
        pickle.dump(response.json(), f)

    df_all = pd.DataFrame()

    taxonomies = response.json()['facts']

    for taxonomy in taxonomies:
        
        facts = list(response.json()['facts'][taxonomy].keys())

        facts.sort()

        for fact in facts:

            units = response.json()['facts'][taxonomy][fact]['units'].keys()

            for unit in units:
                print(f'{taxonomy:<10}: {fact:<50}: {unit:<10}')

                tmp = pd.DataFrame(response.json()['facts'][taxonomy][fact]['units'][unit])

                tmp['taxonomy'] = taxonomy

                tmp['fact'] = fact

                tmp['unit'] = unit

                df_all = pd.concat([df_all, tmp])

    df_all = df_all[['filed', 'fy', 'fp', 'start', 'end', 'frame', 'form', 'taxonomy', 'fact', 'unit', 'accn', 'val']]
            
    file = os.path.join('data', symbol, 'df_all_facts.pkl')

    os.makedirs(os.path.dirname(file), exist_ok=True)

    df_all.to_pickle(file)



#### company valuations

import datetime
import time

import requests
import pandas as pd
from bs4 import BeautifulSoup
from dateutil.relativedelta import relativedelta
from pymongo.errors import DocumentTooLarge

import mongodb

AAPL_CIK = "0000320193"
BABA_CIK = "0001577552"
ATKR_CIK = "0001666138"
META_CIK = "0001326801"
_8K_URL = "https://www.sec.gov/Archives/edgar/data/320193/000114036123023909/ny20007635x4_8k.htm"

def make_edgar_request(url):
    """
    Make a request to EDGAR (Electronic Data Gathering, Analysis and Retrieval)
    :param url:
    :return: response
    """
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/113.0.0.0 Safari/537.36",
        "Accept-Encoding": "gzip, deflate, br",
    }
    return requests.get(url, headers=headers)


def download_cik_ticker_map():
    """
    Get a mapping of cik (Central Index Key, id of company on edgar) and ticker on the exchange.
    It saves this mapping in mongodb.
    """
    CIK_TICKER_URL = "https://www.sec.gov/files/company_tickers_exchange.json"
    response = make_edgar_request(CIK_TICKER_URL)
    r = response.json()
    r["_id"] = "cik_ticker"
    mongodb.upsert_document("cik_ticker", r)


def get_df_cik_ticker_map():
    """
    Create DataFrame from cik ticker document on mongodb.
    :return: DataFrame
    """
    try:
        cik_ticker = mongodb.get_collection_documents("cik_ticker").next()
    except StopIteration:
        print("cik ticker document not found")
        return
    df = pd.DataFrame(cik_ticker["data"], columns=cik_ticker["fields"])
    # add leading 0s to cik (always 10 digits)
    df["cik"] = df.apply(lambda x: add_trailing_to_cik(x["cik"]), axis=1)
    return df


def company_from_cik(cik):
    """
    Get company info from cik
    :param cik: company id on EDGAR
    :return: DataFrame row with company information (name, ticker, exchange)
    """
    df = get_df_cik_ticker_map()
    try:
        return df[df["cik"] == cik].iloc[0]
    except IndexError:
        return None

def cik_from_ticker(ticker):
    """
    Get company cik from ticker
    :param ticker: company ticker
    :return: cik (company id on EDGAR)
    """
    df = get_df_cik_ticker_map()
    try:
        cik = df[df["ticker"] == ticker]["cik"].iloc[0]
    except:
        cik = -1
    return cik

def download_all_cik_submissions(cik):
    """
    Get list of submissions for a single company.
    Upsert this list on mongodb (each download contains all the submissions).
    :param cik: cik of the company
    :return:
    """
    url = f"https://data.sec.gov/submissions/CIK{cik}.json"
    response = make_edgar_request(url)
    r = response.json()
    r["_id"] = cik
    mongodb.upsert_document("submissions", r)


def download_submissions_documents(cik, forms_to_download=("10-Q", "10-K", "8-K"), years=5):
    """
    Download all documents for submissions forms 'forms_to_download' for the past 'max_history' years.
    Insert them on mongodb.
    :param cik: company cik
    :param forms_to_download: a tuple containing the form types to download
    :param years: the max number of years to download
    :return:
    """
    try:
        submissions = mongodb.get_document("submissions", cik)
    except StopIteration:
        print(f"submissions file not found in mongodb for {cik}")
        return
    cik_no_trailing = submissions["cik"]
    filings = submissions["filings"]["recent"]
    for i in range(len(filings["filingDate"])):
        filing_date = filings['filingDate'][i]
        difference_in_years = relativedelta(datetime.date.today(),
                                            datetime.datetime.strptime(filing_date, "%Y-%m-%d")).years
        # as the document are ordered cronologically when we reach the max history we can return
        if difference_in_years > years:
            return
        form_type = filings['form'][i]
        if form_type not in forms_to_download:
            continue
        accession_no_symbols = filings["accessionNumber"][i].replace("-","")
        primary_document = filings["primaryDocument"][i]
        url = f"https://www.sec.gov/Archives/edgar/data/{cik_no_trailing}/{accession_no_symbols}/{primary_document}"
        # if we already have the document, we don't download it again
        if mongodb.check_document_exists("documents", url):
            continue
        print(f"{filing_date} ({form_type}): {url}")
        download_document(url, cik, form_type, filing_date)
        # insert a quick sleep to avoid reaching edgar rate limit
        time.sleep(0.2)


def download_document(url, cik, form_type, filing_date, updated_at=None):
    """
    Download and insert submission document
    :param url:
    :param cik:
    :param form_type:
    :param filing_date:
    :return:
    """
    response = make_edgar_request(url)
    r = response.text
    doc = {"html": r, "cik": cik, "form_type": form_type, "filing_date": filing_date, "updated_at": updated_at, "_id": url}
    try:
        mongodb.insert_document("documents", doc)
    except DocumentTooLarge:
        # DocumenTooLarge is raised by mongodb when uploading files larger than 16MB
        # To avoid this it is better to save this kind of files in a separate storate like S3 and retriving them when needed.
        # Another options could be using mongofiles: https://www.mongodb.com/docs/database-tools/mongofiles/#mongodb-binary-bin.mongofiles
        # for management of large files saved in mongo db.
        print("Document too Large (over 16MB)", url)


def download_financial_data(cik):
    """
    Download financial data for a company.
    Upsert document on mongodb (each requests returns the entire history)
    :param cik:
    :return:
    """
    url = f"https://data.sec.gov/api/xbrl/companyfacts/CIK{cik}.json"
    response = make_edgar_request(url)
    try:
        r = response.json()
        r["_id"] = cik
        r["url"] = url
        mongodb.upsert_document("financial_data", r)
    # ETFs, funds, trusts do not have financial information
    except:
        print(f"ERROR {cik} - {response} - {url}")
        print(company_from_cik(cik))

def get_filing_from_index(url):
    """
    Get the document url from the filing index page.
    This is a filing index page:
    https://www.sec.gov/Archives/edgar/data/320193/000114036123023909/0001140361-23-023909-index.htm
    The document url we want is the first url in the Document Format Files table.
    :param url: filing index page url
    :return:
    """
    index_page = make_edgar_request(url)
    soup = BeautifulSoup(index_page.text, "html.parser")
    table = soup.find("table", {"class": "tableFile", "summary": "Document Format Files"})
    return table.find("a")["href"]


def add_trailing_to_cik(cik_no_trailing):
    return "{:010d}".format(cik_no_trailing)


def get_size_in_bytes(size_string):
    size = int(size_string.split()[0])
    unit = size_string.split()[1].upper()

    if unit == "MB":
        return size * 1024 * 1024
    elif unit == "KB":
        return size * 1024
    else:
        raise ValueError("Invalid size unit. Must be either MB or KB.")


def get_latest_filings(form_type, start_date):
    """
    Get new filings (for all companies) since 'start_date' (yyyy-mm-dd).
    Insert new submission documents on mongodb.
    Insert new financial data on mongodb.

    Used to update submissions documents and financial data in our db.
    :param form_type: form that we want to request (you can pass multiple forms delimited by commas 10-K,10-Q,...
    :param start_date: date from where we want to retrieve new submissions
    :return:
    """

    start_idx = 0
    entries_per_request = 100
    done = False

    cik_df = get_df_cik_ticker_map()
    ciks = list(cik_df["cik"].unique())

    while not done:
        url = f"https://www.sec.gov/cgi-bin/browse-edgar?action=getcurrent&type={form_type}&datea={start_date}&" \
              f"start={start_idx}&count={entries_per_request}&output=atom"
        print(f"{url}")
        response = make_edgar_request(url)

        soup = BeautifulSoup(response.text, 'xml')
        entries = soup.findAll("entry")

        # If the response contains less entry than what we requested it means we are done
        if len(entries) < entries_per_request:
            done = True

        for entry in entries:
            index_url = entry.find("link")["href"]
            entry_form_type = entry.find("category")["term"]
            entry_updated_at = entry.find("updated").text.split("T")[0]
            entry_summary = entry.find("summary").text.replace("<b>",";").replace("</b>","").replace("\n", "")
            filed_date = entry_summary.split(';')[1].split(":")[1].strip()
            size = get_size_in_bytes(entry_summary.split(';')[3].split(":")[1].strip())
            start_cik = index_url.find('data/') + 5
            end_cik = index_url.find('/', start_cik)
            cik = add_trailing_to_cik(int(index_url[start_cik: end_cik]))

            if cik not in ciks:
                print(f"{cik} not present in cik map - skip")
                continue

            if size > 16 * 1024 * 1024:
                print(f"SKIP {cik} because of size {size}")
                continue
            url = get_filing_from_index(index_url)
            url = f"https://www.sec.gov/{url.replace('/ix?doc=/','')}"

            # if we already have the document on mongodb we can skip
            if mongodb.check_document_exists("documents", url):
                continue

            download_document(url, cik, entry_form_type, filed_date, entry_updated_at)

            # if entry_form_type in ["10-Q", "10-Q/A" "10-K", "10-K/A"]:
            #     download_financial_data(cik)

        start_idx += entries_per_request


if __name__ == '__main__':
    apple_tiker = "AAPL"
    cik = cik_from_ticker(apple_tiker)
    download_all_cik_submissions(cik)
    # get_latest_filings("10-K", "2023-01-01")
    # download_cik_ticker_map()
    # download_all_cik_submissions("0001326801")
    # download_submissions_documents("0001326801")




import requests
import urllib3
from bs4 import BeautifulSoup

currency_country = {
    "AUD": "Australia",
    "BRL": "Brazil",
    "CAD": "Canada",
    "CHF": "Switzerland",
    "CLP": "Chile",
    "CNY": "China",
    "COP": "Colombia",
    "CZK": "Czech Republic",
    "DKK": "Denmark",
    "EGP": "Egypt",
    "EUR": "Germany",
    "GBP": "United Kingdom",
    "HKD": "Hong Kong",
    "HUF": "Hungary",
    "IDR": "Indonesia",
    "ILS": "Israel",
    "INR": "India",
    "ISK": "Iceland",
    "JPY": "Japan",
    "KRW": "South Korea",
    "KZT": "Kazakhstan",
    "MXN": "Mexico",
    "MYR": "Malaysia",
    "NGN": "Nigeria",
    "NOK": "Norway",
    "NZD": "New Zealand",
    "PHP": "Philippines",
    "PLN": "Poland",
    "QAR": "Qatar",
    "RUB": "Russia",
    "SGD": "Singapore",
    "THB": "Thailand",
    "TRY": "Turkey",
    "TWD": "Taiwan",
    "USD": "United States",
    "ZAR": "South Africa",
}
country_url = {
    "Australia": "https://www.investing.com/rates-bonds/australia-10-year-bond-yield",
    "Austria": "https://www.investing.com/rates-bonds/austria-10-year-bond-yield",
    "Belgium": "https://www.investing.com/rates-bonds/belguim-10-year-bond-yield",
    "Brazil": "https://www.investing.com/rates-bonds/brazil-10-year-bond-yield",
    "Canada": "https://www.investing.com/rates-bonds/canada-10-year-bond-yield",
    "Chile": "https://www.investing.com/rates-bonds/chile-10-year-bond-yield",
    "China": "https://www.investing.com/rates-bonds/china-10-year-bond-yield",
    "Colombia": "https://www.investing.com/rates-bonds/colombia-10-year-bond-yield",
    "Cyprus": "https://www.investing.com/rates-bonds/cyprus-10-year",
    "Czech Republic": "https://www.investing.com/rates-bonds/czech-republic-10-year-bond-yield",
    "Denmark": "https://www.investing.com/rates-bonds/denmark-10-year-bond-yield",
    "Egypt": "https://www.investing.com/rates-bonds/egypt-10-year-bond-yield",
    "Finland": "https://www.investing.com/rates-bonds/finland-10-year-bond-yield",
    "France": "https://www.investing.com/rates-bonds/france-10-year-bond-yield",
    "Germany": "https://www.investing.com/rates-bonds/germany-10-year-bond-yield",
    "Greece": "https://www.investing.com/rates-bonds/greece-10-year-bond-yield",
    "Hong Kong": "https://www.investing.com/rates-bonds/hong-kong-10-year-bond-yield",
    "Hungary": "https://www.investing.com/rates-bonds/hungary-10-year-bond-yield",
    "Iceland": "https://www.investing.com/rates-bonds/iceland-10-year-bond-yield",
    "India": "https://www.investing.com/rates-bonds/india-10-year-bond-yield",
    "Indonesia": "https://www.investing.com/rates-bonds/indonesia-10-year-bond-yield",
    "Ireland": "https://www.investing.com/rates-bonds/ireland-10-year-bond-yield",
    "Israel": "https://www.investing.com/rates-bonds/israel-10-year-bond-yield",
    "Italy": "https://www.investing.com/rates-bonds/italy-10-year-bond-yield",
    "Japan": "https://www.investing.com/rates-bonds/japan-10-year-bond-yield",
    "Kazakhstan": "https://www.investing.com/rates-bonds/kazakhstan-10-year",
    "Malaysia": "https://www.investing.com/rates-bonds/malaysia-10-year-bond-yield",
    "Malta": "https://www.investing.com/rates-bonds/malta-10-year",
    "Mauritius": "https://www.investing.com/rates-bonds/mauritius-10-year",
    "Mexico": "https://www.investing.com/rates-bonds/mexico-10-year",
    "Netherlands": "https://www.investing.com/rates-bonds/netherlands-10-year-bond-yield",
    "New Zealand": "https://www.investing.com/rates-bonds/new-zealand-10-years-bond-yield",
    "Nigeria": "https://www.investing.com/rates-bonds/nigeria-10-year",
    "Norway": "https://www.investing.com/rates-bonds/norway-10-year-bond-yield",
    "Philippines": "https://www.investing.com/rates-bonds/philippines-10-year-bond-yield",
    "Poland": "https://www.investing.com/rates-bonds/poland-10-year-bond-yield",
    "Portugal": "https://www.investing.com/rates-bonds/portugal-10-year-bond-yield",
    "Qatar": "https://www.investing.com/rates-bonds/qatar-10-year-bond-yield",
    "Russia": "https://www.investing.com/rates-bonds/russia-10-year-bond-yield",
    "Singapore": "https://www.investing.com/rates-bonds/singapore-10-year-bond-yield",
    "South Africa": "https://www.investing.com/rates-bonds/south-africa-10-year-bond-yield",
    "South Korea": "https://www.investing.com/rates-bonds/south-korea-10-year-bond-yield",
    "Spain": "https://www.investing.com/rates-bonds/spain-10-year-bond-yield",
    "Switzerland": "https://www.investing.com/rates-bonds/switzerland-10-year-bond-yield",
    "Taiwan": "https://www.investing.com/rates-bonds/taiwan-10-year-bond-yield",
    "Thailand": "https://www.investing.com/rates-bonds/thailand-10-year-bond-yield",
    "Turkey": "https://www.investing.com/rates-bonds/turkey-10-year-bond-yield",
    "United Kingdom": "https://www.investing.com/rates-bonds/uk-10-year-bond-yield",
    "United States": "https://www.investing.com/rates-bonds/u.s.-10-year-bond-yield",
    "Vietnam": "https://www.investing.com/rates-bonds/vietnam-10-year-bond-yield"
}

def get_10y_bond_yield(currency):
    urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

    if currency not in currency_country:
        return None, None

    url = country_url[currency_country[currency]]
    headers = {
        'accept': 'text/plain, */*; q=0.01',
        'accept-encoding': 'gzip, deflate, utf-8',
        'accept-language': 'en,it-IT;q=0.9,it;q=0.8,en-US;q=0.7',
        'cache-control': 'no-cache',
        'origin': 'https://www.investing.com',
        'pragma': 'no-cache',
        'sec-ch-ua': '".Not/A)Brand";v="99", "Google Chrome";v="103", "Chromium";v="103"',
        'sec-ch-ua-mobile': '?0',
        'sec-ch-ua-platform': '"Windows"',
        'sec-fetch-dest': 'empty',
        'sec-fetch-mode': 'cors',
        'sec-fetch-site': 'same-origin',
        'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/103.0.0.0 Safari/537.36',
        'upgrade-insecure-requests': '1',
    }

    bondyield = None
    retries = 0
    max_retries = 3
    while bondyield is None:
        response = request_with_retries(url, headers=headers)

        # with open("response.html", "w", encoding="utf-8") as f:
        #     f.write(response.text)

        soup = BeautifulSoup(response.text, 'html.parser')
        span = soup.select_one('dd[data-test="prevClose"]')
        try:
            bondyield = round(float(span.text) / 100, 5)
        except:
            print("ERROR in getting riskfree", url)
            retries += 1
            if retries >= max_retries:
                break

    return bondyield, currency_country[currency]

def request_with_retries(url, headers=None):

    urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

    resp = None
    max_retry = 5
    retry = 0

    while resp is None and retry < max_retry:
        try:
            if headers is not None:
                resp = requests.get(url, verify=False, headers=headers)
            else:
                resp = requests.get(url, verify=False)
        except:
            print(f"{url} conn err - retry")
        retry += 1
    return resp

from configparser import ConfigParser

from pymongo import MongoClient
import os

DB_NAME = 'company_eval'

def get_mongodb_client():
    """
    Get mongodb client
    :return: mongodb client
    """

    # Get credentials
    parser = ConfigParser()
    _ = parser.read(os.path.join("credentials.cfg"))
    username = parser.get("mongo_db", "username")
    password = parser.get("mongo_db", "password")

    # Set connection string
    LOCAL_CONNECTION = "mongodb://localhost:27017"
    ATLAS_CONNECTION = f"mongodb+srv://{username}:{password}@cluster0.3dxfmjo.mongodb.net/?" \
                       f"retryWrites=true&w=majority"
    ATLAS_OLD_CONNECTION = f"mongodb://{username}:{password}@cluster0.3dxfmjo.mongodb.net:27017/?" \
                          f"retryWrites=true&w=majority&tls=true"
    # print(ATLAS_CONNECTION)

    connection_string = LOCAL_CONNECTION

    # Create a connection using MongoClient
    client = MongoClient(connection_string)

    return client

def get_collection(collection_name):
    db = get_mongodb_client()[DB_NAME]
    return db[collection_name]

def get_file_size(file_name):
    file_stats = os.stat(file_name)
    print(f'File Size in Bytes is {file_stats.st_size}')
    return file_stats.st_size

def get_dict_size(data):
    import sys
    print("The size of the dictionary is {} bytes".format(sys.getsizeof(data)))
    return sys.getsizeof(data)

def upsert_document(collection_name, data):
    collection = get_collection(collection_name)
    collection.replace_one({"_id":data["_id"]}, data, upsert=True)

def insert_document(collection_name, data):
    collection = get_collection(collection_name)
    collection.insert_one(data)

def get_document(collection_name, document_id):
    collection = get_collection(collection_name)
    return collection.find({"_id": document_id}).next()

def check_document_exists(collection_name, document_id):
    collection = get_collection(collection_name)
    return collection.count_documents({"_id": document_id}, limit=1) > 0

def get_collection_documents(collection_name):
    collection = get_collection(collection_name)
    return collection.find({})

import time
import traceback
from datetime import datetime

import pandas as pd
import pymongo
from bs4 import BeautifulSoup

import mongodb
from edgar_utils import company_from_cik, AAPL_CIK, download_all_cik_submissions, download_submissions_documents
from openai_interface import summarize_section
from postgresql import get_df_from_table, country_to_region, area_to_repr_country


def restructure_parsed_10k(doc):
    """
    Look for and select only the sections specified in result dictionary.
    :param doc: mongo document from "documents" collection
    :return: a dictionary containing the parsed document sections titles and their text.
    """
    result = {
        "business": {"text":"", "links":[]}, # important
        "risk": {"text":"", "links":[]},       # important
        "unresolved": {"text":"", "links":[]},
        "property": {"text":"", "links":[]},
        # "MD&A": {"text":"", "links":[]},     # important
        "legal": {"text":"", "links":[]},
        "foreign": {"text":"", "links":[]},
        # "notes": {"text":"", "links":[]},
        "other": {"text":"", "links":[]}
    }

    for s in doc["sections"]:

        found = None
        if ("business" in s.lower() or "overview" in s.lower() or "company" in s.lower() or "general" in s.lower() or "outlook" in s.lower())\
                and not "combination" in s.lower():
            found = "business"
        elif "propert" in s.lower() and not "plant" in s.lower() and not "business" in s.lower():
            found = "property"
        elif "foreign" in s.lower() and "jurisdiction" in s.lower():
            found = "foreign"
        elif "legal" in s.lower() and "proceeding" in s.lower():
            found = "legal"
        # elif "management" in s.lower() and "discussion" in s.lower():
        #     found = "MD&A"
        # elif "supplementa" in s.lower() or ("note" in s.lower() and "statement" not in s.lower()):
        #     found = "notes"
        elif "information" in s.lower() and "other" in s.lower():
            found = "other"
        elif "unresolved" in s.lower():
            found = "unresolved"
        elif "risk" in s.lower():
            found = "risk"

        if found is not None:
            result[found]["text"] += doc["sections"][s]["text"]
            result[found]["links"].append({
                "title": s,
                "link": doc["sections"][s]["link"] if "link" in doc["sections"][s] else None
            })

    return result

def restructure_parsed_10q(doc):
    result = {
        "risk": {"text":"", "links":[]},  # important
        "MD&A": {"text":"", "links":[]},  # important
        "legal": {"text":"", "links":[]},
        "other": {"text":"", "links":[]},
        "equity": {"text":"", "links":[]},
        "defaults": {"text":"", "links":[]},
    }

    for s in doc["sections"]:

        found = None
        if "legal" in s.lower() and "proceeding" in s.lower():
            found = "legal"
        elif "management" in s.lower() and "discussion" in s.lower():
            found = "MD&A"
        elif "information" in s.lower() and "other" in s.lower():
            found = "other"
        elif "risk" in s.lower():
            found = "risk"
        elif "sales" in s.lower() and "equity" in s.lower():
            found = "equity"
        elif "default" in s.lower():
            found = "defaults"

        if found is not None:
            result[found]["text"] += doc["sections"][s]["text"]
            result[found]["links"].append({
                "title": s,
                "link": doc["sections"][s]["link"] if "link" in doc["sections"][s] else None
            })

    return result

def restructure_parsed_8k(doc):

    result = {}

    for s in doc["sections"]:
        if "financial statements and exhibits" in s.lower():
            continue
        result[s] = doc["sections"][s]

    return result

def sections_summary(doc, verbose=False):
    """
    Summarize all sections of a document using openAI API.
    Upsert summary on mongodb (overwrite previous one, in case we make changes to openai_interface)

    This method is configured to use gpt-3.5-turbo. At the moment this model has two different version,
    a version with 4k token and a version with 16k tokens. That are used with based on the length of a sections.

    :param doc: a parsed_document from mongodb
    :param verbose: passed to langchain verbose
    :return:
    """

    company = company_from_cik(doc["cik"])
    result = {"_id": doc["_id"],
              "name": company["name"],
              "ticker": company["ticker"],
              "form_type": doc["form_type"],
              "filing_date": doc["filing_date"]}

    total_cost = 0
    total_start_time = time.time()

    if "10-K" in doc["form_type"]:
        new_doc = restructure_parsed_10k(doc)
    elif doc["form_type"] == "10-Q":
        new_doc = restructure_parsed_10q(doc)
    elif doc["form_type"] == "8-K":
        new_doc = restructure_parsed_8k(doc)
    else:
        print(f"form_type {doc['form_type']} is not yet implemented")
        return


    for section_title, section in new_doc.items():

        section_links = section["links"] if "links" in section else None
        section_text = section["text"]

        start_time = time.time()
        if len(section_text) < 250:
            continue

        if section_title in ["business", "risk", "MD&A"]:
            chain_type = "refine"

            if len(section_text) > 25000:
                model = "gpt-3.5-turbo-16k"
            else:
                model = "gpt-3.5-turbo"
        else:
            if len(section_text) < 25000:
                chain_type = "refine"
                model = "gpt-3.5-turbo"
            elif len(section_text) < 50000:
                chain_type = "map_reduce"
                model = "gpt-3.5-turbo"
            else:
                chain_type = "map_reduce"
                model = "gpt-3.5-turbo-16k"

        original_len = len(section_text)

        # get summary from openAI model
        print(f"{section_title} original_len: {original_len} use {model} w/ chain {chain_type}")
        summary, cost = summarize_section(section_text, model, chain_type, verbose)

        result[section_title] = {"summary":summary, "links": section_links}

        summary_len = len(''.join(summary))
        reduction = 100 - round(summary_len / original_len * 100, 2)

        total_cost += cost
        duration = round(time.time() - start_time, 1)

        print(f"{section_title} original_len: {original_len} summary_len: {summary_len} reduction: {reduction}% "
              f"cost: {cost}$ duration:{duration}s used {model} w/ chain {chain_type}")

    mongodb.upsert_document("items_summary", result)

    total_duration = round(time.time() - total_start_time, 1)

    print(f"\nTotal Cost: {total_cost}$, Total duration: {total_duration}s")


def extract_segments(doc):
    """
    Extract segments information (industry, geographical) from document
    :param url: url of the document, used as id on mongodb
    :return: list of dictionaries {"date": date, "segment":{"axis":"member", ...}, "value": number, "measure": "measure/metric"}
    """

    # doc = mongodb.get_document("documents", url)
    page = doc["html"]
    soap = BeautifulSoup(page, features="html.parser")

    ix_resources = soap.find("ix:resources")

    if ix_resources is None:
        return

    contexts = ix_resources.findAll("xbrli:context")

    axis = [
        "srt:ProductOrServiceAxis",
        "us-gaap:StatementBusinessSegmentsAxis",
        "srt:ConsolidationItemsAxis",
        "srt:StatementGeographicalAxis",
    ]

    result = []

    for c in contexts:

        context_id = c["id"]
        s = c.find("xbrli:segment")

        if s is not None:

            members = s.find_all("xbrldi:explicitmember")
            if len(members) == 0:
                continue

            include = True
            for m in members:
                if m["dimension"] not in axis:
                    include = False
                    break
            if not include:
                continue

            try:
                period = c.find("xbrli:enddate").text
            except:
                period = c.find("xbrli:instant").text
            period = datetime.strptime(period, "%Y-%m-%d").date()

            element = soap.find("ix:nonfraction", attrs={"contextref": context_id})
            if element is None or "name" not in element.attrs:
                continue

            try:
                value = float(element.text.replace(",",""))
            except:
                continue

            segment = {}
            for m in members:
                segment[m["dimension"]] = m.text

            result.append({
                "date": period,
                "segment": segment,
                "value": value,
                "measure": element["name"]
            })

    return result

def map_geographic_area(string):
    if "other" in string and ("region" in string or "countr" in string or "continent" in string):
        return "Global"
    elif "foreign" in string:
        return "Global"
    elif "europe" in string:
        return "Western Europe"
    elif "asia" in string:
        return "Asia"
    elif "emea" in string:
        return "EMEA" # 70% western europe, 15% middle east, 15% africa
    elif "apac" in string:
        return "APAC" # 90% asia, 10% australia
    elif "lacc" in string:
        return "LACC" # 50% central & south america, 40% canada, 10% caribbean
    elif "centralandsouthamerica" in string or "southamerica" in string or "americas" in string:
        return "Central and South America"
    elif "africa" in string:
        return "Africa"
    elif "middleeast" in string:
        return "Middle East"
    elif "northamerica" in string:
        return "North America"

def geography_distribution(segments, ticker):

    df = pd.DataFrame(segments)

    if df.empty:
        return df

    df["segment"] = df["segment"].astype(str)

    # filter by geography segments
    df = df[(df["segment"].str.contains('srt:StatementGeographicalAxis'))&
        ~(df["segment"].str.contains("srt:ProductOrServiceAxis"))&
        ~(df["segment"].str.contains("us-gaap:StatementBusinessSegmentsAxis"))]

    # print(df.to_markdown())

    # filter by measure
    measures = list(df["measure"].unique())

    selected_measure = None

    for m in measures:
        if "revenue" in m.lower() and ticker in m.lower():
            selected_measure = m
            break

    if selected_measure is None:
        for m in [
            "Revenues",
            "RevenueFromContractWithCustomerExcludingAssessedTax",
            "RevenueFromContractWithCustomerIncludingAssessedTax",
            "SalesRevenueNet",
            "OperatingIncomeLoss",
            "IncomeLossFromContinuingOperationsBeforeInterestExpenseInterestIncomeIncomeTaxesExtraordinaryItemsNoncontrollingInterestsNet",
            "IncomeLossFromContinuingOperationsBeforeIncomeTaxesMinorityInterestAndIncomeLossFromEquityMethodInvestments",
            "IncomeLossFromContinuingOperationsBeforeIncomeTaxesExtraordinaryItemsNoncontrollingInterest",
            "IncomeLossFromContinuingOperationsBeforeIncomeTaxesForeign",
            "IncomeLossFromContinuingOperationsBeforeIncomeTaxesDomestic",
            "NetIncomeLoss",
            "NetIncomeLossAvailableToCommonStockholdersBasic",
            "NetIncomeLossAvailableToCommonStockholdersDiluted",
            "ComprehensiveIncomeNetOfTax",
            "IncomeLossFromContinuingOperations",
            "ProfitLoss",
            "IncomeLossFromContinuingOperationsIncludingPortionAttributableToNoncontrollingInterest",
            "IncomeLossFromSubsidiariesNetOfTax"
        ]:
            if f"us-gaap:{m}" in measures:
                selected_measure = f"us-gaap:{m}"
                break

    df = df[df["measure"] == selected_measure]

    df = df[df.groupby(["segment","measure"])['date'].transform('max') == df['date']]\
        .drop(["date","measure"], axis=1)

    # print(df.to_markdown())


    # if only 'srt:StatementGeographicalAxis'
    df["segment"] = df["segment"].apply(lambda x:
                                        x[x.find("'srt:StatementGeographicalAxis':")+len("'srt:StatementGeographicalAxis':"):]
                                        .split("}")[0].split(",")[0].split(":")[1].split("'")[0])

    # MAP SEGMENTS
    # 1st try and match countries
    country_stats = get_df_from_table("damodaran_country_stats", most_recent=True)[["country","alpha_2_code"]]
    df = pd.merge(df, country_stats, left_on="segment", right_on="alpha_2_code", how="left").drop("alpha_2_code", axis=1)

    # 2st try and map regions
    df["area"] = df["segment"].apply(lambda x: map_geographic_area(x.lower()))

    # manage the rest
    df = df[~(df["country"].isna())|~(df["area"].isna())]

    df["value"] /= df["value"].sum()
    df["country_area"] = df["country"].fillna(df["area"])

    aggregate_areas_df = pd.DataFrame([
        {"country_area": "EMEA", "part_area": "Western Europe", "area_percent": 0.7},
        {"country_area": "EMEA", "part_area": "Middle East", "area_percent": 0.15},
        {"country_area": "EMEA", "part_area": "Africa", "area_percent": 0.15},
        {"country_area": "APAC", "part_area": "Asia", "area_percent": 0.9},
        {"country_area": "APAC", "part_area": "Australia & New Zealand", "area_percent": 0.1},
        {"country_area": "LACC", "part_area": "Central and South America", "area_percent": 0.5},
        {"country_area": "LACC", "part_area": "Canada", "area_percent": 0.4},
        {"country_area": "LACC", "part_area": "Caribbean", "area_percent": 0.1},
    ])

    # print(df.to_markdown())
    df = pd.merge(df, aggregate_areas_df, how="left", left_on="country_area", right_on="country_area")
    df["part_area"] = df["part_area"].fillna(df["country_area"])
    df["area_percent"] = df["area_percent"].fillna(1)
    df = df.drop("country_area", axis=1)
    df = df.rename(columns={"part_area":"country_area"})
    df["value"] = df["value"] * df["area_percent"]

    df["region"] = df["country_area"].apply(lambda x: country_to_region[x] if x in country_to_region else "Global")
    df["country_representative"] = df["country_area"].apply(lambda x: area_to_repr_country[x] if x in area_to_repr_country else None)
    df["country"] = df["country"].fillna(df["country_representative"])

    # print(df.to_markdown())

    return df.drop(["segment","country_representative","area", "area_percent"], axis=1)

def try_geo_segments():

    # url = "https://www.sec.gov/Archives/edgar/data/1666138/000166613822000128/atkr-20220930.htm" # ATKR 10-k
    # url = "https://www.sec.gov/Archives/edgar/data/320193/000032019322000108/aapl-20220924.htm" # AAPL 10-K
    # url = "https://www.sec.gov/Archives/edgar/data/1800/000162828023004026/abt-20221231.htm" # ABT 10-K
    # url = "https://www.sec.gov/Archives/edgar/data/2098/000156459023003422/acu-10k_20221231.htm" # ACU 10-K
    # url = "https://www.sec.gov/Archives/edgar/data/4447/000162828023005059/hes-20221231.htm" # HES 10-K

    docs = mongodb.get_collection_documents("documents")
    for doc in docs:
        if doc["form_type"] != "10-K":
            continue

        print(doc["_id"])
        ticker = doc["_id"].split("/")[-1].split("-")[0]
        segments = extract_segments(doc)
        geography_distribution(segments, ticker)


    # segments = extract_segments(url)
    # geography_distribution(segments, "hes")

def get_last_document(cik, form_type):

    download_all_cik_submissions(cik)
    download_submissions_documents(cik, forms_to_download=("10-K", "10-Q", "8-K",), years=1)

    collection = mongodb.get_collection("documents")
    docs = collection.find({"cik": cik, "form_type": form_type})

    last_doc = None
    last_date = None
    for doc in docs:
        filing_date = datetime.strptime(doc["filing_date"], "%Y-%m-%d")
        if last_date is None or filing_date > last_date:
            last_date = filing_date
            last_doc = doc

    return last_doc

def get_recent_docs(cik, filing_date):
    collection = mongodb.get_collection("documents")
    docs = collection.find({"cik": cik, "filing_date": {"$gte":filing_date}})

    # sort by date asc
    docs = docs.sort("filing_date", pymongo.ASCENDING)

    return docs


if __name__ == '__main__':

    # doc = get_last_document(AAPL_CIK, "10-K")
    # print(doc["_id"])

    try_geo_segments()

import datetime
import traceback

import pandas as pd
import mongodb
from edgar_utils import company_from_cik, cik_from_ticker, download_financial_data
from postgresql import get_df_from_table, get_generic_info
from qualitative_analysis import get_last_document, extract_segments, geography_distribution, get_recent_docs, \
    sections_summary
from utils import parse_document, find_auditor
from valuation_helper import convert_currencies, get_target_info, get_normalized_info, get_dividends_info, \
    get_final_info, calculate_liquidation_value, dividends_valuation, fcff_valuation, get_status, summary_valuation, \
    r_and_d_amortization, get_growth_ttm, capitalize_rd, debtize_op_leases, get_roe_roc, get_spread_from_dscr, \
    company_complexity, company_share_diluition, get_company_type, currency_bond_yield, get_industry_data
from yahoo_finance import get_current_price_from_yahoo

EARNINGS_TTM = "EARNINGS_TTM"
EARNINGS_NORM = "EARNINGS_NORM"
GROWTH_FIXED = "GROWTH_FIXED"
GROWTH_TTM = "GROWTH_TTM"
GROWTH_NORM = "GROWTH_NORM"

STATUS_OK = "OK"
STATUS_NI = "NI"
STATUS_KO = "KO"

def build_financial_df(doc, measure, unit="USD", tax="us-gaap"):

    """
    Build a DataFrame from a company financial document (containing all history and all measures).
    DataFrame is built on a specific subsection of the document, identified with taxonomy, measure, unit
    :param doc: company financial document
    :param measure: measure we are interest in
    :param unit: unit of measure (usually is a single one for each measure)
    :param tax: taxonomy
    :return: DataFrame
    """

    try:
        data = doc["facts"][tax][measure]["units"][unit]
    except:
        return None

    df = pd.DataFrame(data)
    df["val"] = pd.to_numeric(df["val"])

    # for income statement or cashflow statement measures we have a start and end date to represent a period.
    # for example Revenues, we need to know the period (start-end) in which they have been generated.
    # for balance sheet measures we have only end date as they are snapshot in time.
    # for example Cash, we just know the amount in a certain date, there is no start-date period concept.

    try:
        if "start" in df.columns:
            df["start"] = pd.to_datetime(df["start"])

        df["end"] = pd.to_datetime(df["end"])
        df["filed"] = pd.to_datetime(df["filed"])
    except:
        return None

    # print(measure, unit, tax)
    # print(df)

    try:
        df = df[~df.frame.isna()]
    except:
        df = df[0:0]

    return df

def get_ttm_from_df(df):
    """
    Compute TTM (trailing twelve months) value from a DataFrame containing quarterly and annual values.
    :param df: DataFrame containing quarterly and annual values
    :return: ttm value, year of last annual value in DataFrame
    """

    # create a copy as we are going to edit and filter it
    ttm_df = df.copy()

    # Get last annual value
    try:
        # Keep only annual and quarterly periods
        ttm_df["period"] = (ttm_df["end"] - ttm_df["start"]).dt.days
        ttm_df = ttm_df[~(ttm_df.frame.str.contains("Q")) | ((ttm_df.frame.str.contains("Q")) & (ttm_df.period < 100))]
        last_yearly_row = ttm_df[ttm_df.period > 100].iloc[-1]
    except:
        return None, None

    # Get quarterly values AFTER the annual value
    post_quarterly_rows = ttm_df[ttm_df.index > last_yearly_row.name]

    # Get corresponding quarterly values BEFORE the annual value
    pre_frames = list(post_quarterly_rows.frame)
    pre_frames = [x[:2] + str(int(x[2:6]) - 1) + x[6:] for x in pre_frames]
    pre_quarterly_rows = ttm_df[ttm_df.frame.isin(pre_frames)]

    # TTM = annual value + quarterly values after - corresponding quarterly values before
    ttm = last_yearly_row.val + post_quarterly_rows.val.sum() - pre_quarterly_rows.val.sum()

    return ttm, last_yearly_row.end

def get_most_recent_value_from_df(df):
    """
    Get most recent value and date in DataFrame (last row)
    :param df: DataFrame containing quarterly and annual values
    :return: most recent value and date in DataFrame
    """
    return {"date":df.iloc[-1]["end"], "value":df.iloc[-1]["val"]}

def get_last_annual_report_date_and_fy(df):

    if df is None:
        return None, None

    year_df = df[~df.frame.str.contains("Q")]
    dates = list((year_df.frame.str.replace("CY", "")).astype(int))

    last_annual_report_date = year_df.iloc[-1].end if len(year_df) > 0 else None
    last_annual_report_fy = dates[-1] if len(dates) > 0 else None

    return last_annual_report_date, last_annual_report_fy

def get_quarter_of_annual_report(df, last_annual_report_date, last_annual_report_fy):

    if df is None:
        return None, None

    last_annual_report_row = df[df.end == last_annual_report_date]
    if last_annual_report_row.empty:
        return None, None

    # frame is a string CYXXXXQXI, we want the X between Q and I
    try:
        quarter_of_annual_report = last_annual_report_row.iloc[0]["frame"][7]
    except:
        print(last_annual_report_row)
        return None, None

    year_bs = int(last_annual_report_row.frame.iloc[0][2:6])
    years_diff = year_bs - last_annual_report_fy

    return quarter_of_annual_report, years_diff

def get_yearly_values_from_df(df, instant=False, quarter_of_annual_report=None, years_diff=0):

    """
    Get yearly data from DataFrame
    :param df: DataFrame containing quarterly and annual values
    :param instant: bool that indicates if the measure is instantaneous (snapshot), if True it means the measure is a
    balance sheet measure, otherwise it's an income statement/cashflow statement measure (period instead of snapshot)
    :param quarter_of_annual_report: in case of instant measure, we also need the quarter when the annual report is
    released
    :param years_diff: used when the fiscal year ends in a different solar year
    :return: dict {
        "dates": [date1, date2, ..., dateN],
        "values": [val1, val2, ..., valN],
        "last_annual_report_date": date
    }
    """

    # create a copy as we are going to edit and filter it
    year_df = df.copy()

    # income statement / cashflow statement
    if not instant:

        # get only annual frames
        year_df = year_df[~year_df.frame.str.contains("Q")]
        dates = list((year_df.frame.str.replace("CY", "")).astype(int))

        return {"dates": dates,
                "values": list(year_df.val)}

    # balance sheet
    else:

        # keep only only rows with quarters of annual reports
        year_df = year_df[year_df.frame.str.contains(f"Q{quarter_of_annual_report}I")]
        year_df["frame"] = year_df.frame.str.replace("CY", "").str.replace(f"Q{quarter_of_annual_report}I","").astype(int) - years_diff

        return {"dates": list(year_df.frame),
                "values": list(year_df.val)}

def get_values_from_measures(doc, measures, get_ttm=True, get_most_recent=True, get_yearly=True, instant=False,
                             quarter_of_annual_report=None, years_diff=0, debug=False, unit="USD", tax="us-gaap"):

    """
    Retrieve requested financial values from company financial document (containing all history and all measures).
    Measures are interpreted in a hierarchical way, meaning that if we have a value for 2020 for the first measure,
    and a value for 2020 for the second measure, we are going to keep the first.
    This is done in order to account for different possible measures that represent the same metric but could be present
    in a company but not in another. The hierarchy is useful in case a company has more than one measure and we need
    to choose which one to keep
    :param doc: company financial document
    :param measures: measures we are interest in (in order of "importance")
    :param get_ttm: bool, whether to compute ttm value
    :param get_most_recent: bool, whether to compute most recent value
    :param get_yearly: bool, whether to compute yearly values
    :param instant: bool, indicates if the measures are instantaneous (snapshot, balance sheet) or not (period,
    income statement / cashflow statement)
    :param quarter_of_annual_report: quarter of annual report, used for instant measures
    :param years_diff: difference between fiscal year and solar year, used for instant measures
    :param debug: bool, print debug statements
    :param unit: unit for build_financial_df
    :param tax: taxonomy for build_financial_df
    :return: most recent value, ttm value, yearly values (0 or empty if not requested)
    """

    ttm = 0
    ttm_year = None
    most_recent = 0
    most_recent_date = None
    yearly = {"dates": [], "values": []}

    for m in measures:

        # Build the DataFrame
        df = build_financial_df(doc, m, unit, tax)

        # The df is None if the company does not have the measure m in its financial data
        if df is None or df.empty:
            continue

        if get_ttm:

            # Get TTM
            ttm_value_tmp, ttm_year_tmp = get_ttm_from_df(df)

            if ttm_value_tmp is not None:

                # We override ttm if we have a more recent value
                if ttm_year is None or ttm_year_tmp > ttm_year:
                    ttm = ttm_value_tmp
                    ttm_year = ttm_year_tmp

                if debug:
                    print(m, ttm_year_tmp, ttm_value_tmp)

        if get_most_recent:

            # Get most recent value
            most_recent_tmp = get_most_recent_value_from_df(df)

            if most_recent_tmp["value"] is not None:

                # We override most_recent_value if we have a more recent value
                if most_recent_date is None or most_recent_tmp["date"] > most_recent_date:
                    most_recent_date = most_recent_tmp["date"]
                    most_recent = most_recent_tmp["value"]

                if debug:
                    print(m, most_recent_tmp["date"], most_recent_tmp["value"])

        if get_yearly:

            # Get yearly values
            yearly_tmp = get_yearly_values_from_df(df, instant, quarter_of_annual_report, years_diff)

            if yearly_tmp is not None:

                # for each date
                for i, d in enumerate(yearly_tmp["dates"]):

                    # if we don't have it already (hierarchical), we add the values
                    if d not in yearly["dates"]:
                        yearly["dates"].append(d)
                        yearly["values"].append(yearly_tmp["values"][i])

                if debug:
                    print(m, yearly_tmp)

    # sort dates and values from the least recent to the most recent
    sort = sorted(zip(yearly["dates"], yearly["values"]))
    yearly["dates"] = [x for x, _ in sort]
    yearly["values"] = [x for _, x in sort]

    if debug:
        print("ttm", ttm)
        print("most recent", most_recent)
        print("yearly", yearly)

    return {"date":most_recent_date, "value":most_recent}, {"date":ttm_year, "value":ttm}, yearly

def merge_subsets_yearly(superset, subsets, must_include=None):

    """
    Sum multiple measures into a single one. Superset is the measure that should already represent the sum. Subsets are
    its components.
    This method is used when we don't have the aggregated measure or we don't have it for all the years where we have
    the disaggregated measures.
    For example we have Total Assets for 2020,2021,2022 + Current Assets and Non-Current Assets from 2019 to 2022.
    In this case we can build Total Assets also for 2019.
    :param superset: aggregated measure
    :param subsets: disaggregated measures to be summed
    :param must_include: we can pass a tuple in order to consider a summed value iff we all the measures in subsets
    with indexes included in 'must_include' have values. In the example above for example we can say to add the 2019
    value for Total Assets iff we have Current Assets for 2019. If we have only Non-Current assets we will not add the
    2019 value for Total Assets.
    :return:
    """

    to_add = {"dates":[],"values":[]}

    # if no must_include
    if must_include is None:

        # for each subset
        for s in subsets:

            # for each date in the subset
            for i, d in enumerate(s["dates"]):

                # if that date is not in superset
                if d not in superset["dates"]:

                    # if it's the first subset with that date, append the value
                    if d not in to_add["dates"]:
                        to_add["dates"].append(d)
                        to_add["values"].append(s["values"][i])

                    # else add the value to the existing one
                    else:
                        idx = to_add["dates"].index(d)
                        to_add["values"][idx] += s["values"][i]

    # if must_include
    else:

        if not isinstance(must_include, tuple):
            raise Exception("must_include must be a tuple")

        # get dates for the first must_include (all others in must_include must have the same dates for the date
        # to be included)
        tmp_dates = subsets[must_include[0]]["dates"]

        remove_dates = []

        # for each date
        for d in tmp_dates:

            # for each index in must_include
            for m in must_include:

                # if the subset does not have the date we remove it
                s = subsets[m]
                if d not in s["dates"]:
                    remove_dates.append(d)

        # keep only the dates where we have values for every must_include subset
        must_include_dates = [x for x in tmp_dates if x not in remove_dates and x not in superset["dates"]]

        # if none return
        if len(must_include_dates) == 0:
            return

        # set to 0 the values for each date
        for m in must_include_dates:
            to_add["dates"].append(m)
            to_add["values"].append(0)

        # for each subset, add the value for the dates
        for s in subsets:
            for i, d in enumerate(s["dates"]):
                if d in to_add["dates"]:
                    idx = to_add["dates"].index(d)
                    to_add["values"][idx] += s["values"][i]

    for i, d in enumerate(to_add["dates"]):
        superset["dates"].append(d)
        superset["values"].append(to_add["values"][i])

    # sort date and values in superset
    sort = sorted(zip(superset["dates"], superset["values"]))
    superset["dates"] = [x for x, _ in sort]
    superset["values"] = [x for _, x in sort]

def merge_subsets_most_recent(superset, subsets):

    """
    Sum multiple most recent (or ttm) measures into a single one. Superset is the measure that should already represent the sum. Subsets are
    its components.
    This method is used when we don't have the aggregated measure or we don't have it for all the years where we have
    the disaggregated measures.
    For example we have Total Assets for 2020,2021,2022 + Current Assets and Non-Current Assets from 2019 to 2022.
    In this case we can build Total Assets also for 2019.
    :param superset: aggregated measure
    :param subsets: disaggregated measures to be summed
    :return:
    """

    replace = False
    for s in subsets:
        if superset["date"] is None or (s["date"] is not None and s["date"] > superset["date"]):
            replace = True
            break

    if replace:

        dates = [x["date"] for x in subsets if x["date"] is not None]

        # we are here if neither the superset nor the subsets have any value
        if len(dates) == 0:
            return

        d = max(dates)

        superset["date"] = d
        superset["value"] = 0

        for s in subsets:
            if s["date"] == d:
                superset["value"] += s["value"]

def extract_shares(doc, quarter_of_annual_report, years_diff):
    """
    Extract number of shares from company financial document
    :param doc: company financial document
    :return: number of common shares outstanding (most recent and annual)
    """

    df = build_financial_df(doc, "EntityCommonStockSharesOutstanding", unit="shares", tax="dei")

    debug = False

    if debug:
        print(df.to_markdown())

    try:
        most_recent_shares = get_most_recent_value_from_df(df)
    except:
        most_recent_shares = {"date":None, "value":0}

    measures = ["CommonStockSharesOutstanding"]

    mr_common_shares, _, yearly_common_shares = get_values_from_measures(
        doc, measures, instant=True, quarter_of_annual_report=quarter_of_annual_report,
        years_diff=years_diff, get_ttm=False,
        get_most_recent=True, debug=debug, unit="shares")

    measures = ["WeightedAverageNumberOfSharesOutstandingBasic"]

    mr_average_shares, _, yearly_average_shares = get_values_from_measures(
        doc, measures, instant=False, quarter_of_annual_report=quarter_of_annual_report,
        years_diff=years_diff, get_ttm=False,
        get_most_recent=True, debug=debug, unit="shares")

    merge_subsets_most_recent(most_recent_shares, [mr_common_shares])
    merge_subsets_most_recent(most_recent_shares, [mr_average_shares])

    try:
        yearly_shares = get_yearly_values_from_df(df, instant=True, quarter_of_annual_report=quarter_of_annual_report,
        years_diff=years_diff)

        merge_subsets_yearly(yearly_common_shares, [yearly_average_shares])
        merge_subsets_yearly(yearly_shares, [yearly_common_shares])

    except:
        merge_subsets_yearly(yearly_common_shares, [yearly_average_shares])
        yearly_shares = yearly_common_shares

    # in some filings the company report shares with a wrong unit of measure (million shares instead of thousand shares)

    try:
        max_num_shares = max(yearly_shares["values"])
    except:
        raise NoSharesException()

    yearly_shares["values"] = [x * 1000 if x / max_num_shares < 0.01 else x for x in yearly_shares["values"]]
    if most_recent_shares["value"] / max_num_shares < 0.01:
        most_recent_shares["value"] *= 1000
    return {
        "mr_shares": most_recent_shares,
        "shares": yearly_shares,
    }

class NoSharesException(Exception):
    pass

def extract_income_statement(doc):
    """
    Extract income statement measures from company financial document.
    Measures include:
    - revenue
    - R&D
    - net income
    - interest expense
    - gross profit
    - depreciation and amortization
    - EBIT
    :param doc: company financial document
    :return: dict with ttm and yearly measures
    """

    measures = [
        "Revenues",
        "RevenueFromContractWithCustomerExcludingAssessedTax",
        "RevenueFromContractWithCustomerIncludingAssessedTax",
        "SalesRevenueNet"
    ]
    _, ttm_revenue, yearly_revenue = get_values_from_measures(doc, measures, get_most_recent=False, debug=False)

    last_annual_report_date = None
    last_annual_report_fy = None
    for m in measures:
        df = build_financial_df(doc, m)
        if df is not None and not df.empty and "frame" in df.columns:
            annual_rd, annual_fy = get_last_annual_report_date_and_fy(df)
            if last_annual_report_date is None or (annual_rd is not None and annual_rd > last_annual_report_date):
                last_annual_report_date = annual_rd
                last_annual_report_fy = annual_fy

    #### R and D ####
    measures = ["ResearchAndDevelopmentExpense"]
    _, _, yearly_rd = get_values_from_measures(
        doc, measures, get_ttm=False, get_most_recent=False, debug=False)

    measures = ["ResearchAndDevelopmentExpenseExcludingAcquiredInProcessCost"]
    _, _, yearly_rd_not_inprocess = get_values_from_measures(
        doc, measures, get_ttm=False, get_most_recent=False, debug=False)

    measures = ["ResearchAndDevelopmentInProcess"]
    _, _, yearly_rd_inprocess = get_values_from_measures(
        doc, measures, get_ttm=False, get_most_recent=False, debug=False)

    merge_subsets_yearly(yearly_rd, [yearly_rd_not_inprocess, yearly_rd_inprocess])

    #### Net Income ####
    measures = [
        "NetIncomeLoss",
        "NetIncomeLossAvailableToCommonStockholdersBasic",
        "NetIncomeLossAvailableToCommonStockholdersDiluted",
        "ComprehensiveIncomeNetOfTax",
        "IncomeLossFromContinuingOperations",

        # including minority interest
        "ProfitLoss",
        "IncomeLossFromContinuingOperationsIncludingPortionAttributableToNoncontrollingInterest",
        "IncomeLossFromSubsidiariesNetOfTax"
    ]

    _, ttm_net_income, yearly_net_income = get_values_from_measures(doc, measures, get_most_recent=False, debug=False)

    #### Interest Expenses ####
    measures = [
        "InterestExpense",
        "InterestAndDebtExpense",
        "InterestPaid",
        "InterestPaidNet",
        "InterestCostsIncurred"]

    _, ttm_interest_expenses, _ = get_values_from_measures(doc, measures, get_most_recent=False, get_yearly=False,
                                                                                  debug=False)

    # Probably we don't need yearly interest expenses
    # measures = ["InterestExpenseDebt",
    #             "InterestExpenseDebtExcludingAmortization"]
    # _, ttm_ie_debt, yearly_ie_debt = get_values_from_measures(doc, measures, get_most_recent=False,
    #                                                           debug=False)
    #
    # measures = ["InterestExpenseLongTermDebt"]
    # _, ttm_ie_debt_lt, yearly_ie_debt_lt = get_values_from_measures(doc, measures, get_most_recent=False,
    #                                                                 debug=False)
    #
    # measures = ["InterestExpenseShortTermBorrowings"]
    # _, ttm_ie_debt_st, yearly_ie_debt_st = get_values_from_measures(doc, measures, get_most_recent=False,
    #                                                                 debug=False)
    # merge_subsets(yearly_ie_debt, [yearly_ie_debt_lt, yearly_ie_debt_st])
    #
    #
    # measures = ["InterestExpenseBorrowings"]
    # _, ttm_ie_borrowings, yearly_ie_borrowings = get_values_from_measures(doc, measures, get_most_recent=False,
    #                                                                       debug=False)
    # measures = ["InterestExpenseDeposits"]
    # _, ttm_ie_deposits, yearly_ie_deposits = get_values_from_measures(doc, measures, get_most_recent=False,
    #                                                                   debug=False)
    # measures = ["InterestExpenseOther"]
    # _, ttm_ie_others, yearly_ie_others = get_values_from_measures(doc, measures, get_most_recent=False,
    #                                                               debug=False)
    # measures = ["InterestExpenseRelatedParty"]
    # _, ttm_ie_related, yearly_ie_related = get_values_from_measures(doc, measures, get_most_recent=False,
    #                                                                 debug=False)
    #
    # merge_subsets(yearly_ie_borrowings, [yearly_ie_debt, yearly_ie_deposits, yearly_ie_others, yearly_ie_related])
    # merge_subsets(yearly_interest_expenses, [yearly_ie_borrowings])



    #### Gross Profit ####

    if ttm_interest_expenses == 0:

        measures = ["InterestExpenseBorrowings"]
        _, ttm_ie_borrowings, _ = get_values_from_measures(doc, measures, get_most_recent=False, get_yearly=False,
                                                           debug=False)

        if ttm_ie_borrowings["value"] == 0:

            measures = ["InterestExpenseDebt",
                        "InterestExpenseDebtExcludingAmortization"]
            _, ttm_ie_debt, _ = get_values_from_measures(doc, measures, get_most_recent=False, get_yearly=False,
                                                                      debug=False)

            if ttm_ie_debt["value"] == 0:

                measures = ["InterestExpenseLongTermDebt"]
                _, ttm_ie_debt_lt, _ = get_values_from_measures(doc, measures, get_most_recent=False, get_yearly=False,
                                                                debug=False)
                measures = ["InterestExpenseShortTermBorrowings"]
                _, ttm_ie_debt_st, _ = get_values_from_measures(doc, measures, get_most_recent=False, get_yearly=False,
                                                                debug=False)

                merge_subsets_most_recent(ttm_ie_debt, [ttm_ie_debt_lt, ttm_ie_debt_st])


            measures = ["InterestExpenseDeposits"]
            _, ttm_ie_deposits, _ = get_values_from_measures(doc, measures, get_most_recent=False, get_yearly=False,
                                                                              debug=False)
            measures = ["InterestExpenseOther"]
            _, ttm_ie_others, _ = get_values_from_measures(doc, measures, get_most_recent=False, get_yearly=False,
                                                                          debug=False)
            measures = ["InterestExpenseRelatedParty"]
            _, ttm_ie_related, _ = get_values_from_measures(doc, measures, get_most_recent=False, get_yearly=False,
                                                                            debug=False)

            ttm_ie_borrowings = merge_subsets_most_recent(ttm_ie_borrowings,
                                                          [ttm_ie_debt, ttm_ie_deposits, ttm_ie_others, ttm_ie_related])

        ttm_interest_expenses = ttm_ie_borrowings

    measures = ["Gross Profit"]
    _, ttm_gross_profit, yearly_gross_profit = get_values_from_measures(doc, measures, get_most_recent=False,
                                                                        debug=False)

    #### Depreciation ####
    measures = [
        "DepreciationDepletionAndAmortization",
        "DepreciationAmortizationAndAccretionNet"]
    _, _, yearly_depreciation_amortization = get_values_from_measures(doc, measures, get_most_recent=False, get_ttm=False,
                                                                                                  debug=False)

    measures = ["Depreciation"]
    _, _, yearly_depreciation = get_values_from_measures(doc, measures, get_most_recent=False, get_ttm=False,
                                                                        debug=False)

    measures = ["AmortizationOfFinancingCostsAndDiscounts"]
    _, _, yearly_amortization_fincost_disc = get_values_from_measures(doc, measures, get_most_recent=False, get_ttm=False,
                                                                                                  debug=False)

    measures = ["AmortizationOfDebtDiscountPremium"]
    _, _, yearly_amortization_disc = get_values_from_measures(doc, measures, get_most_recent=False, get_ttm=False,
                                                                                  debug=False)
    measures = ["AmortizationOfFinancingCosts"]
    _, _, yearly_amortization_fincost = get_values_from_measures(doc, measures, get_most_recent=False, get_ttm=False,
                                                                                        debug=False)

    merge_subsets_yearly(yearly_amortization_fincost_disc, [yearly_amortization_disc, yearly_amortization_fincost])

    measures = ["AmortizationOfDeferredCharges"]
    _, _, yearly_amortization_charges = get_values_from_measures(doc, measures, get_most_recent=False, get_ttm=False,
                                                                                        debug=False)
    measures = ["AmortizationOfDeferredSalesCommissions"]
    _, _, yearly_amortization_comm = get_values_from_measures(doc, measures, get_most_recent=False, get_ttm=False,
                                                                                  debug=False)
    measures = ["AmortizationOfIntangibleAssets"]
    _, _, yearly_amortization_intan = get_values_from_measures(doc, measures, get_most_recent=False, get_ttm=False,
                                                                                    debug=False)

    yearly_amortization = {"dates":[], "values":[]}
    merge_subsets_yearly(yearly_amortization, [yearly_amortization_fincost_disc, yearly_amortization_charges,
                                               yearly_amortization_comm, yearly_amortization_intan])
    merge_subsets_yearly(yearly_depreciation_amortization, [yearly_depreciation, yearly_amortization])

    #### EBIT ####
    measures = ["OperatingIncomeLoss",
                "IncomeLossFromContinuingOperationsBeforeInterestExpenseInterestIncomeIncomeTaxesExtraordinaryItemsNoncontrollingInterestsNet",
                "IncomeLossFromContinuingOperationsBeforeIncomeTaxesMinorityInterestAndIncomeLossFromEquityMethodInvestments",
                "IncomeLossFromContinuingOperationsBeforeIncomeTaxesExtraordinaryItemsNoncontrollingInterest",
                "IncomeLossFromContinuingOperationsBeforeIncomeTaxesForeign",
                "IncomeLossFromContinuingOperationsBeforeIncomeTaxesDomestic",
                ]
    _, ttm_ebit, yearly_ebit = get_values_from_measures(doc, measures, get_most_recent=False,
                                                        debug=False)

    return {
        "ttm_revenue": ttm_revenue,
        "ttm_gross_profit": ttm_gross_profit,
        "ttm_ebit": ttm_ebit,
        "ttm_net_income": ttm_net_income,
        "ttm_interest_expenses": ttm_interest_expenses,
        "revenue": yearly_revenue,
        "gross_profit": yearly_gross_profit,
        "rd": yearly_rd,
        "ebit": yearly_ebit,
        "depreciation": yearly_depreciation_amortization,
        "net_income": yearly_net_income,
        "last_annual_report_date": last_annual_report_date,
        "last_annual_report_fy": last_annual_report_fy
    }

def extract_balance_sheet_current_assets(doc, quarter_of_annual_report, years_diff):
    """
    Extract balance sheet measures (Current Assets) from company financial document.
    Measures include:
    - cash
    - inventory
    - other assets
    - receivables
    - securities
    :param doc: company financial document
    :param quarter_of_annual_report: quarter of annual report
    :param years_diff: difference between fiscal year and solar year
    :return: dict with most recent and yearly measures
    """

    # No need for aggregate values
    # #### ASSETS ####
    # measures = ["Assets"]
    # most_recent_assets, _, yearly_assets = get_values_from_measures(
    #     doc, measures, instant=True, last_annual_report_date=last_annual_report_date, get_ttm=False, debug=False)
    #
    # #### Current Assets ####
    # measures = ["AssetsCurrent"]
    # most_recent_current_assets, _, yearly_current_assets = get_values_from_measures(
    #     doc, measures, instant=True, last_annual_report_date=last_annual_report_date, get_ttm=False, debug=False)



    #### Inventory ####

    #### Cash ####
    measures = ["CashCashEquivalentsRestrictedCashAndRestrictedCashEquivalents"]
    most_recent_cash_and_restricted, _, yearly_cash_and_restricted = get_values_from_measures(
        doc, measures, instant=True, quarter_of_annual_report=quarter_of_annual_report, years_diff=years_diff,
        get_ttm=False, debug=False)

    measures = ["CashAndCashEquivalentsAtCarryingValue", "Cash"]
    most_recent_cash, _, yearly_cash = get_values_from_measures(
        doc, measures, instant=True, quarter_of_annual_report=quarter_of_annual_report, years_diff=years_diff,
        get_ttm=False, debug=False)

    measures = [
        "RestrictedCashAndCashEquivalentsAtCarryingValue",
        "RestrictedCashAndCashEquivalents",
        "RestrictedCash",
        "RestrictedCashAndInvestmentsCurrent",
        "RestrictedCashCurrent"
    ]
    most_recent_restrictedcash, _, yearly_restrictedcash = get_values_from_measures(
        doc, measures, instant=True, quarter_of_annual_report=quarter_of_annual_report, years_diff=years_diff,
        get_ttm=False, debug=False)

    merge_subsets_yearly(yearly_cash_and_restricted, [yearly_cash, yearly_restrictedcash], must_include=(0,))

    if most_recent_cash_and_restricted["date"] is None \
            or (most_recent_cash["date"] is not None and most_recent_cash["date"] > most_recent_cash_and_restricted["date"]):
        most_recent_cash_and_restricted["date"] = most_recent_cash["date"]
        most_recent_cash_and_restricted["value"] = most_recent_cash["value"]

        if most_recent_restrictedcash["date"] == most_recent_cash["date"]:
            most_recent_cash_and_restricted["value"] += most_recent_restrictedcash["value"]

    #### Inventory ####
    measures = [
        "InventoryNet",
        "InventoryGross",
        "FIFOInventoryAmount",
        "InventoryLIFOReserve",
        "LIFOInventoryAmount",
    ]
    most_recent_inventory, _, yearly_inventory = get_values_from_measures(
        doc, measures, instant=True, quarter_of_annual_report=quarter_of_annual_report, years_diff=years_diff,
        get_ttm=False, debug=False)

    measures = [
        "RetailRelatedInventory",
        "RetailRelatedInventoryMerchandise"
    ]
    most_recent_inventory_retail, _, yearly_inventory_retail = get_values_from_measures(
        doc, measures, instant=True, quarter_of_annual_report=quarter_of_annual_report, years_diff=years_diff,
        get_ttm=False, debug=False)

    measures = [
        "EnergyRelatedInventory"
    ]
    most_recent_inventory_energy, _, yearly_inventory_energy = get_values_from_measures(
        doc, measures, instant=True, quarter_of_annual_report=quarter_of_annual_report, years_diff=years_diff,
        get_ttm=False, debug=False)

    measures = [
        "PublicUtilitiesInventory"
    ]
    most_recent_inventory_utilities, _, yearly_inventory_utilities = get_values_from_measures(
        doc, measures, instant=True, quarter_of_annual_report=quarter_of_annual_report, years_diff=years_diff,
        get_ttm=False, debug=False)

    measures = [
        "InventoryRealEstate"
    ]
    most_recent_inventory_re, _, yearly_inventory_re = get_values_from_measures(
        doc, measures, instant=True, quarter_of_annual_report=quarter_of_annual_report, years_diff=years_diff,
        get_ttm=False, debug=False)

    measures = [
        "AirlineRelatedInventory"
    ]
    most_recent_inventory_airline, _, yearly_inventory_airline = get_values_from_measures(
        doc, measures, instant=True, quarter_of_annual_report=quarter_of_annual_report, years_diff=years_diff,
        get_ttm=False, debug=False)

    merge_subsets_most_recent(most_recent_inventory,
                              [most_recent_inventory_retail, most_recent_inventory_airline,
                               most_recent_inventory_energy, most_recent_inventory_re, most_recent_inventory_utilities])
    merge_subsets_yearly(yearly_inventory, [yearly_inventory_retail, yearly_inventory_airline, yearly_inventory_energy,
                                            yearly_inventory_re, yearly_inventory_utilities])

    #### Other Assets ####
    measures = [
        "OtherAssetsCurrent",
        "OtherAssetsMiscellaneousCurrent",
        "PrepaidExpenseAndOtherAssetsCurrent",
        "OtherAssetsFairValueDisclosure",
        "OtherAssetsMiscellaneous",
        "PrepaidExpenseAndOtherAssets"
    ]
    most_recent_other_current_assets, _, yearly_other_current_assets = get_values_from_measures(
        doc, measures, instant=True, quarter_of_annual_report=quarter_of_annual_report, years_diff=years_diff,
        get_ttm=False, debug=False)

    measures = ["PrepaidExpenseCurrent"]
    most_recent_prepaid_exp, _, yearly_prepaid_exp = get_values_from_measures(
        doc, measures, instant=True, quarter_of_annual_report=quarter_of_annual_report, years_diff=years_diff,
        get_ttm=False, debug=False)
    measures = ["PrepaidInsurance"]
    most_recent_prepaid_ins, _, yearly_prepaid_ins = get_values_from_measures(
        doc, measures, instant=True, quarter_of_annual_report=quarter_of_annual_report, years_diff=years_diff,
        get_ttm=False, debug=False)
    measures = ["PrepaidTaxes",
                "IncomeTaxesReceivable",
                "IncomeTaxReceivable"]
    most_recent_prepaid_tax, _, yearly_prepaid_tax = get_values_from_measures(
        doc, measures, instant=True, quarter_of_annual_report=quarter_of_annual_report, years_diff=years_diff,
        get_ttm=False, get_yearly=False, debug=False)
    merge_subsets_yearly(yearly_other_current_assets, [yearly_prepaid_exp, yearly_prepaid_ins, yearly_prepaid_tax])

    merge_subsets_most_recent(most_recent_other_current_assets,
                              [most_recent_prepaid_exp, most_recent_prepaid_ins, most_recent_prepaid_tax])

    #### Receivables ####
    measures = [
        "AccountsAndOtherReceivablesNetCurrent",
        "AccountsNotesAndLoansReceivableNetCurrent",
        "ReceivablesNetCurrent",
        "NontradeReceivablesCurrent"]
    most_recent_receivables, _, yearly_receivables = get_values_from_measures(
        doc, measures, instant=True, quarter_of_annual_report=quarter_of_annual_report, years_diff=years_diff,
        get_ttm=False, debug=False)

    measures = ["AccountsReceivableNetCurrent",
                "AccountsReceivableNet",
                "AccountsReceivableGrossCurrent",
                "AccountsReceivableGross"]
    most_recent_ar, _, yearly_ar = get_values_from_measures(
        doc, measures, instant=True, quarter_of_annual_report=quarter_of_annual_report, years_diff=years_diff,
        get_ttm=False, debug=False)

    measures = ["LoansAndLeasesReceivableNetReportedAmount",
                "LoansAndLeasesReceivableNetOfDeferredIncome",
                "LoansReceivableFairValueDisclosure",
                "LoansAndLeasesReceivableGrossCarryingAmount"]
    most_recent_loans_rec, _, yearly_loans_rec = get_values_from_measures(
        doc, measures, instant=True, quarter_of_annual_report=quarter_of_annual_report, years_diff=years_diff,
        get_ttm=False, debug=False)

    measures = ["NotesReceivableNet",
                "NotesReceivableFairValueDisclosure",
                "NotesReceivableGross"]
    most_recent_notes_rec, _, yearly_notes_rec = get_values_from_measures(
        doc, measures, instant=True, quarter_of_annual_report=quarter_of_annual_report, years_diff=years_diff,
        get_ttm=False, debug=False)

    merge_subsets_yearly(yearly_receivables, [yearly_ar, yearly_loans_rec, yearly_notes_rec])
    merge_subsets_most_recent(most_recent_receivables,
                              [most_recent_ar, most_recent_loans_rec, most_recent_notes_rec])

    #### Securities ####
    measures = [
        "MarketableSecurities"
        "AvailableForSaleSecurities"]
    most_recent_securities, _, yearly_securities = get_values_from_measures(
        doc, measures, instant=True, quarter_of_annual_report=quarter_of_annual_report, years_diff=years_diff,
        get_ttm=False, debug=False)

    measures = ["AvailableForSaleSecuritiesDebtSecurities"]
    most_recent_debtsecurities, _, yearly_debtsecurities = get_values_from_measures(
        doc, measures, instant=True, quarter_of_annual_report=quarter_of_annual_report, years_diff=years_diff,
        get_ttm=False, debug=False)

    measures = ["AvailableForSaleSecuritiesEquitySecurities"]
    most_recent_equitysecurities, _, yearly_equitysecurities = get_values_from_measures(
        doc, measures, instant=True, quarter_of_annual_report=quarter_of_annual_report, years_diff=years_diff,
        get_ttm=False, debug=False)

    merge_subsets_yearly(yearly_securities, [yearly_debtsecurities, yearly_equitysecurities])
    merge_subsets_most_recent(most_recent_securities,
                              [most_recent_debtsecurities, most_recent_equitysecurities])

    measures = ["DerivativeAssets",
                "DerivativeAssetsCurrent"]
    most_recent_derivatives, _, yearly_derivatives = get_values_from_measures(
        doc, measures, instant=True, quarter_of_annual_report=quarter_of_annual_report, years_diff=years_diff,
        get_ttm=False, debug=False)

    measures = ["HeldToMaturitySecurities",
                "HeldToMaturitySecuritiesFairValue",
                "HeldToMaturitySecuritiesCurrent",
                ]
    most_recent_held_securities, _, yearly_held_securities = get_values_from_measures(
        doc, measures, instant=True, quarter_of_annual_report=quarter_of_annual_report, years_diff=years_diff,
        get_ttm=False, debug=False)

    measures = ["AvailableForSaleSecuritiesNoncurrent",
                "AvailableForSaleSecuritiesDebtSecuritiesNoncurrent",
                ]
    most_recent_non_curr_sec, _, yearly_non_curr_sec = get_values_from_measures(
        doc, measures, instant=True, quarter_of_annual_report=quarter_of_annual_report, years_diff=years_diff,
        get_ttm=False, debug=False)

    measures = ["MarketableSecuritiesCurrent",
                "AvailableForSaleSecuritiesDebtSecuritiesCurrent"]
    most_recent_marksecurities_cur, _, yearly_marksecurities_cur = get_values_from_measures(
        doc, measures, instant=True, quarter_of_annual_report=quarter_of_annual_report, years_diff=years_diff,
        get_ttm=False, debug=False)

    measures = ["ShortTermInvestments"]
    most_recent_st_inv, _, yearly_st_inv = get_values_from_measures(
        doc, measures, instant=True, quarter_of_annual_report=quarter_of_annual_report, years_diff=years_diff,
        get_ttm=False, debug=False)

    measures = ["MoneyMarketFundsAtCarryingValue"]
    most_recent_mm, _, yearly_mm = get_values_from_measures(
        doc, measures, instant=True, quarter_of_annual_report=quarter_of_annual_report, years_diff=years_diff,
        get_ttm=False, debug=False)

    merge_subsets_yearly(yearly_securities, [yearly_derivatives, yearly_held_securities, yearly_non_curr_sec,
                                             yearly_marksecurities_cur, yearly_st_inv, yearly_mm])
    merge_subsets_most_recent(most_recent_securities,
                              [most_recent_derivatives, most_recent_held_securities, most_recent_non_curr_sec,
                               most_recent_marksecurities_cur, most_recent_st_inv, most_recent_mm])

    # merge_subsets(yearly_current_assets, [yearly_cash_and_restricted, yearly_inventory, yearly_other_current_assets,
    #                                       yearly_receivables, yearly_securities])

    return {
        "mr_cash": most_recent_cash_and_restricted,
        "cash": yearly_cash_and_restricted,
        "mr_inventory": most_recent_inventory,
        "inventory": yearly_inventory,
        "mr_other_assets": most_recent_other_current_assets,
        "other_assets": yearly_other_current_assets,
        "mr_receivables": most_recent_receivables,
        "receivables": yearly_receivables,
        "mr_securities": most_recent_securities,
        "securities": yearly_securities
    }

def extract_balance_sheet_noncurrent_assets(doc, quarter_of_annual_report, years_diff):
    """
    Extract balance sheet measures (Non-Current Assets) from company financial document.
    Measures include:
    - equity investments
    - other financial assets
    - PP&E
    - investment property
    - tax benefits
    :param doc: company financial document
    :param quarter_of_annual_report: quarter of annual report
    :param years_diff: difference between fiscal year and solar year
    :return: dict with most recent measures
    """

    # ##### Non current assets ####
    # measures = ["AssetsNoncurrent",
    #             "NoncurrentAssets"]
    # most_recent_non_curr_asset, _, yearly_non_curr_asset = get_values_from_measures(
    #     doc, measures, instant=True, last_annual_report_date=last_annual_report_date, get_ttm=False, debug=False)

    #### Equity investments ####

    #### Equity Investments ####
    measures = [
        "EquityMethodInvestmentAggregateCost",
        "EquityMethodInvestments",
        "InvestmentOwnedAtCost",
        "Investments",
        "InvestmentsInAffiliatesSubsidiariesAssociatesAndJointVentures",
    ]
    most_recent_equity_investments, _, _ = get_values_from_measures(
        doc, measures, instant=True, quarter_of_annual_report=quarter_of_annual_report, years_diff=years_diff,
        get_ttm=False, get_yearly=False,
        debug=False)

    measures = [
        "EquityMethodInvestmentsFairValueDisclosure",
        "InvestmentOwnedAtFairValue",
        "InvestmentsFairValueDisclosure",
    ]
    most_recent_equity_inv_fv, _, _ = get_values_from_measures(
        doc, measures, instant=True, quarter_of_annual_report=quarter_of_annual_report, years_diff=years_diff,
        get_ttm=False, get_yearly=False, debug=False)

    measures = ["EquitySecuritiesWithoutReadilyDeterminableFairValueAmount", ]
    most_recent_equity_inv_notfv, _, _ = get_values_from_measures(
        doc, measures, instant=True, quarter_of_annual_report=quarter_of_annual_report, years_diff=years_diff,
        get_ttm=False, get_yearly=False, debug=False)

    # merge_subsets_yearly(yearly_equity_investments, [yearly_equity_inv_fv, yearly_equity_inv_notfv])
    merge_subsets_most_recent(most_recent_equity_investments,
                              [most_recent_equity_inv_fv, most_recent_equity_inv_notfv])

    measures = ["MarketableSecuritiesNoncurrent"]
    most_recent_securities_non_curr, _, _ = get_values_from_measures(
        doc, measures, instant=True, quarter_of_annual_report=quarter_of_annual_report, years_diff=years_diff,
        get_ttm=False, get_yearly=False, debug=False)

    # yearly_equity_investments_and_securities = {"dates": [], "values": []}
    # merge_subsets_yearly(yearly_equity_investments_and_securities, [yearly_equity_investments, yearly_securities_non_curr])

    merge_subsets_most_recent(most_recent_equity_investments, [most_recent_securities_non_curr])

    #### Other financial assets ####
    measures = [
        "PrepaidExpenseNoncurrent",
        "PrepaidExpenseOtherNoncurrent",
    ]
    most_recent_prepaid_non_curr, _, _ = get_values_from_measures(
        doc, measures, instant=True, quarter_of_annual_report=quarter_of_annual_report, years_diff=years_diff,
        get_ttm=False, get_yearly=False, debug=False)

    measures = [
        "RestrictedCashAndCashEquivalentsNoncurrent",
        "RestrictedCashAndInvestmentsNoncurrent",
        "RestrictedCashNoncurrent"
    ]
    most_recent_cash_non_curr, _, _ = get_values_from_measures(
        doc, measures, instant=True, quarter_of_annual_report=quarter_of_annual_report, years_diff=years_diff,
        get_ttm=False, get_yearly=False, debug=False)

    measures = ["DerivativeAssetsNoncurrent", ]
    most_recent_derivatives_non_curr, _, _ = get_values_from_measures(
        doc, measures, instant=True, quarter_of_annual_report=quarter_of_annual_report, years_diff=years_diff,
        get_ttm=False, get_yearly=False, debug=False)

    measures = ["EscrowDeposit"]
    most_recent_escrow, _, _ = get_values_from_measures(
        doc, measures, instant=True, quarter_of_annual_report=quarter_of_annual_report, years_diff=years_diff,
        get_ttm=False, get_yearly=False, debug=False)

    # yearly_other_financial_assets = {"dates": [], "values": []}
    # merge_subsets_yearly(yearly_other_financial_assets, [yearly_prepaid_non_curr, yearly_cash_non_curr,
    #                                                      yearly_derivatives_non_curr, yearly_escrow])

    most_recent_other_financial_assets = {"date":None, "value":0}
    merge_subsets_most_recent(most_recent_other_financial_assets,
                              [most_recent_prepaid_non_curr, most_recent_cash_non_curr,
                               most_recent_derivatives_non_curr, most_recent_escrow])

    #### PP&E ####
    measures = [
        "PropertyPlantAndEquipmentNet",
        "PropertyPlantAndEquipmentAndFinanceLeaseRightOfUseAssetAfterAccumulatedDepreciationAndAmortization"
    ]
    most_recent_ppe, _, _ = get_values_from_measures(
        doc, measures, instant=True, quarter_of_annual_report=quarter_of_annual_report, years_diff=years_diff,
        get_ttm=False, get_yearly=False, debug=False)

    #### Investment property ####
    measures = [
        "RealEstateInvestments",
        "RealEstateInvestmentPropertyNet",
        "RealEstateInvestmentPropertyAtCost",
        "RealEstateHeldforsale"
    ]
    most_recent_property, _, _ = get_values_from_measures(
        doc, measures, instant=True, quarter_of_annual_report=quarter_of_annual_report, years_diff=years_diff,
        get_ttm=False, get_yearly=False, debug=False)

    measures = ["InvestmentBuildingAndBuildingImprovements"]
    most_recent_buildings, _, _ = get_values_from_measures(
        doc, measures, instant=True, quarter_of_annual_report=quarter_of_annual_report, years_diff=years_diff,
        get_ttm=False, get_yearly=False, debug=False)

    measures = [
        "LandAndLandImprovements",
        "Land",
    ]
    most_recent_land, _, _ = get_values_from_measures(
        doc, measures, instant=True, quarter_of_annual_report=quarter_of_annual_report, years_diff=years_diff,
        get_ttm=False, get_yearly=False, debug=False)

    # merge_subsets_yearly(yearly_property, [yearly_buildings, yearly_land])
    merge_subsets_most_recent(most_recent_property,
                              [most_recent_buildings, most_recent_land])

    # merge_subsets(yearly_non_curr_asset, [yearly_property, yearly_ppe, yearly_other_financial_assets,
    #                                       yearly_equity_investments_and_securities])
    # merge_subsets(yearly_assets, [yearly_current_assets, yearly_non_curr_asset])

    #### Tax Benefits ####
    measures = [
        "UnrecognizedTaxBenefits",
        "UnrecognizedTaxBenefitsThatWouldImpactEffectiveTaxRate",
        "IncomeTaxesReceivableNoncurrent",
    ]
    most_recent_tax_benefit, _, _ = get_values_from_measures(
        doc, measures, instant=True, quarter_of_annual_report=quarter_of_annual_report, years_diff=years_diff,
        get_ttm=False, get_yearly=False, debug=False)

    return {
        "mr_equity_investments": most_recent_equity_investments,
        "mr_other_financial_assets": most_recent_other_financial_assets,
        "mr_ppe": most_recent_ppe,
        "mr_investment_property": most_recent_property,
        "mr_tax_benefits": most_recent_tax_benefit
    }

def extract_balance_sheet_debt(doc, quarter_of_annual_report, years_diff):
    """
    Extract balance sheet DEBT measures (Current + Non-Current) from company financial document.
    :param doc: company financial document
    :param quarter_of_annual_report: quarter of annual report
    :param years_diff: difference between fiscal year and solar year
    :return: dict with most recent and yearly measures
    """

    # DEBT LONG + SHORT
    measures = ["DebtLongtermAndShorttermCombinedAmount"]
    most_recent_debt, _, yearly_debt = get_values_from_measures(
        doc, measures, instant=True, quarter_of_annual_report=quarter_of_annual_report, years_diff=years_diff,
        get_ttm=False, debug=False)

    measures = ["MortgageLoansOnRealEstate"]
    most_recent_mortgage, _, yearly_mortgage = get_values_from_measures(
        doc, measures, instant=True, quarter_of_annual_report=quarter_of_annual_report, years_diff=years_diff,
        get_ttm=False, debug=False)

    measures = ["OtherBorrowings"]
    most_recent_other_borr, _, yearly_other_borr = get_values_from_measures(
        doc, measures, instant=True, quarter_of_annual_report=quarter_of_annual_report, years_diff=years_diff,
        get_ttm=False, debug=False)

    # DEBT SHORT
    measures = [
        "ShortTermBorrowings",
        "ShorttermDebtAverageOutstandingAmount",
        "ShorttermDebtFairValue",
    ]
    most_recent_debt_st, _, yearly_debt_st = get_values_from_measures(
        doc, measures, instant=True, quarter_of_annual_report=quarter_of_annual_report, years_diff=years_diff,
        get_ttm=False, debug=False)

    measures = [
        "CommercialPaper",
        "CommercialPaperAtCarryingValue",
    ]
    most_recent_cp, _, yearly_cp = get_values_from_measures(
        doc, measures, instant=True, quarter_of_annual_report=quarter_of_annual_report, years_diff=years_diff,
        get_ttm=False, debug=False)

    measures = ["BankOverdrafts"]
    most_recent_overdraft, _, yearly_overdraft = get_values_from_measures(
        doc, measures, instant=True, quarter_of_annual_report=quarter_of_annual_report, years_diff=years_diff,
        get_ttm=False, debug=False)

    measures = ["ShortTermBankLoansAndNotesPayable"]
    most_recent_loans_st, _, yearly_loans_st = get_values_from_measures(
        doc, measures, instant=True, quarter_of_annual_report=quarter_of_annual_report, years_diff=years_diff,
        get_ttm=False, debug=False)

    measures = ["BridgeLoan"]
    most_recent_bridge, _, yearly_bridge = get_values_from_measures(
        doc, measures, instant=True, quarter_of_annual_report=quarter_of_annual_report, years_diff=years_diff,
        get_ttm=False, debug=False)

    # DEBT LONG
    measures = [
        "LongTermDebtAndCapitalLeaseObligationsIncludingCurrentMaturities",
        "LongTermDebt",
        "LongTermDebtFairValue",
        "DebtInstrumentFaceAmount",
        "DebtInstrumentCarryingAmount",
    ]
    most_recent_debt_lt, _, yearly_debt_lt = get_values_from_measures(
        doc, measures, instant=True, quarter_of_annual_report=quarter_of_annual_report, years_diff=years_diff,
        get_ttm=False, debug=False)

    measures = [
        "ConvertibleDebt",
        "ConvertibleNotesPayable",
    ]
    most_recent_convertible, _, yearly_convertible = get_values_from_measures(
        doc, measures, instant=True, quarter_of_annual_report=quarter_of_annual_report, years_diff=years_diff,
        get_ttm=False, debug=False)

    measures = [
        "LineOfCredit",
        "LineOfCreditFacilityFairValueOfAmountOutstanding",
    ]
    most_recent_revolver, _, yearly_revolver = get_values_from_measures(
        doc, measures, instant=True, quarter_of_annual_report=quarter_of_annual_report, years_diff=years_diff,
        get_ttm=False, debug=False)

    measures = ["LoansPayable"]
    most_recent_loans_pay, _, yearly_loans_pay = get_values_from_measures(
        doc, measures, instant=True, quarter_of_annual_report=quarter_of_annual_report, years_diff=years_diff,
        get_ttm=False, debug=False)

    measures = ["SecuredDebt"]
    most_recent_debt_sec, _, yearly_debt_sec = get_values_from_measures(
        doc, measures, instant=True, quarter_of_annual_report=quarter_of_annual_report, years_diff=years_diff,
        get_ttm=False, debug=False)

    measures = ["NotesPayable",
                "SeniorNotes"]
    most_recent_debt_notes, _, yearly_debt_notes = get_values_from_measures(
        doc, measures, instant=True, quarter_of_annual_report=quarter_of_annual_report, years_diff=years_diff,
        get_ttm=False, debug=False)

    measures = ["UnsecuredDebt"]
    most_recent_debt_unsec, _, yearly_debt_unsec = get_values_from_measures(
        doc, measures, instant=True, quarter_of_annual_report=quarter_of_annual_report, years_diff=years_diff,
        get_ttm=False, debug=False)

    # DEBT LONG - CURRENT
    measures = [
        "LongTermDebtAndCapitalLeaseObligationsCurrent",
        "LongTermDebtCurrent",
    ]
    most_recent_debt_lt_cur, _, yearly_debt_lt_cur = get_values_from_measures(
        doc, measures, instant=True, quarter_of_annual_report=quarter_of_annual_report, years_diff=years_diff,
        get_ttm=False, debug=False)

    measures = [
        "ConvertibleDebtCurrent",
        "ConvertibleNotesPayableCurrent",
    ]
    most_recent_convertible_cur, _, yearly_convertible_cur = get_values_from_measures(
        doc, measures, instant=True, quarter_of_annual_report=quarter_of_annual_report, years_diff=years_diff,
        get_ttm=False, debug=False)

    measures = ["LinesOfCreditCurrent"]
    most_recent_revolver_cur, _, yearly_revolver_cur = get_values_from_measures(
        doc, measures, instant=True, quarter_of_annual_report=quarter_of_annual_report, years_diff=years_diff,
        get_ttm=False, debug=False)

    measures = ["NotesPayableCurrent",
                "SeniorNotesCurrent"]
    most_recent_notes_cur, _, yearly_notes_cur = get_values_from_measures(
        doc, measures, instant=True, quarter_of_annual_report=quarter_of_annual_report, years_diff=years_diff,
        get_ttm=False, debug=False)

    measures = ["SecuredDebtCurrent"]
    most_recent_debt_sec_cur, _, yearly_debt_sec_cur = get_values_from_measures(
        doc, measures, instant=True, quarter_of_annual_report=quarter_of_annual_report, years_diff=years_diff,
        get_ttm=False, debug=False)

    measures = ["UnsecuredDebtCurrent"]
    most_recent_debt_unsec_cur, _, yearly_debt_unsec_cur = get_values_from_measures(
        doc, measures, instant=True, quarter_of_annual_report=quarter_of_annual_report, years_diff=years_diff,
        get_ttm=False, debug=False)

    # DEBT LONG - NON CURRENT
    measures = [
        "LongTermDebtAndCapitalLeaseObligations",
        "LongTermDebtNoncurrent",
    ]
    most_recent_debt_lt_noncur, _, yearly_debt_lt_noncur = get_values_from_measures(
        doc, measures, instant=True, quarter_of_annual_report=quarter_of_annual_report, years_diff=years_diff,
        get_ttm=False, debug=False)

    measures = [
        "ConvertibleDebtNoncurrent",
        "ConvertibleLongTermNotesPayable",
    ]
    most_recent_convertible_noncur, _, yearly_convertible_noncur = get_values_from_measures(
        doc, measures, instant=True, quarter_of_annual_report=quarter_of_annual_report, years_diff=years_diff,
        get_ttm=False, debug=False)

    measures = ["LongTermLineOfCredit"]
    most_recent_revolver_noncur, _, yearly_revolver_noncur = get_values_from_measures(
        doc, measures, instant=True, quarter_of_annual_report=quarter_of_annual_report, years_diff=years_diff,
        get_ttm=False, debug=False)

    measures = ["LongTermNotesPayable",
                "SeniorLongTermNotes"]
    most_recent_notes_noncur, _, yearly_notes_noncur = get_values_from_measures(
        doc, measures, instant=True, quarter_of_annual_report=quarter_of_annual_report, years_diff=years_diff,
        get_ttm=False, debug=False)

    measures = ["SecuredLongTermDebt"]
    most_recent_debt_sec_noncur, _, yearly_debt_sec_noncur = get_values_from_measures(
        doc, measures, instant=True, quarter_of_annual_report=quarter_of_annual_report, years_diff=years_diff,
         get_ttm=False, debug=False)

    measures = ["UnsecuredLongTermDebt"]
    most_recent_debt_unsec_noncur, _, yearly_debt_unsec_noncur = get_values_from_measures(
        doc, measures, instant=True, quarter_of_annual_report=quarter_of_annual_report, years_diff=years_diff,
         get_ttm=False, debug=False)

    merge_subsets_yearly(yearly_debt_st, [yearly_cp, yearly_overdraft, yearly_bridge, yearly_loans_st])
    merge_subsets_yearly(yearly_debt_lt, [yearly_convertible, yearly_revolver, yearly_loans_pay, yearly_debt_sec,
                                          yearly_debt_notes, yearly_debt_unsec])
    merge_subsets_yearly(yearly_debt_lt_cur, [yearly_convertible_cur, yearly_revolver_cur, yearly_notes_cur,
                                              yearly_debt_sec_cur, yearly_debt_unsec_cur])
    merge_subsets_yearly(yearly_debt_lt_noncur, [yearly_convertible_noncur, yearly_revolver_noncur, yearly_notes_noncur,
                                                 yearly_debt_sec_noncur, yearly_debt_unsec_noncur])
    merge_subsets_yearly(yearly_debt_lt, [yearly_debt_lt_cur, yearly_debt_lt_noncur])
    merge_subsets_yearly(yearly_debt, [yearly_debt_lt, yearly_debt_st])
    merge_subsets_yearly(yearly_debt, [yearly_debt, yearly_mortgage, yearly_other_borr], (0,))

    merge_subsets_most_recent(most_recent_debt_st, [most_recent_cp, most_recent_overdraft, most_recent_bridge, most_recent_loans_st])
    merge_subsets_most_recent(most_recent_debt_lt, [most_recent_convertible, most_recent_revolver, most_recent_loans_pay, most_recent_debt_sec,
                                          most_recent_debt_notes, most_recent_debt_unsec])
    merge_subsets_most_recent(most_recent_debt_lt_cur, [most_recent_convertible_cur, most_recent_revolver_cur, most_recent_notes_cur,
                                              most_recent_debt_sec_cur, most_recent_debt_unsec_cur])
    merge_subsets_most_recent(most_recent_debt_lt_noncur, [most_recent_convertible_noncur, most_recent_revolver_noncur, most_recent_notes_noncur,
                                                 most_recent_debt_sec_noncur, most_recent_debt_unsec_noncur])
    merge_subsets_most_recent(most_recent_debt_lt, [most_recent_debt_lt_cur, most_recent_debt_lt_noncur])
    merge_subsets_most_recent(most_recent_debt, [most_recent_debt_lt, most_recent_debt_st])

    for m in [most_recent_mortgage, most_recent_other_borr]:
        if m["date"] == most_recent_debt["date"]:
            most_recent_debt["value"] += m["value"]

    return {
        "mr_debt": most_recent_debt,
        "debt": yearly_debt
    }

def extract_balance_sheet_liabilities(doc, quarter_of_annual_report, years_diff, most_recent_debt):
    """
    Extract balance sheet measures (Total Liabilities) from company financial document.
    :param doc: company financial document
    :param quarter_of_annual_report: quarter of annual report
    :param years_diff: difference between fiscal year and solar year
    :return: dict with most recent measures
    """

    measures = [
        "Liabilities",
        "LiabilitiesFairValueDisclosure",
        "LiabilitiesAssumed1",
    ]
    most_recent_liabilities, _, _ = get_values_from_measures(
        doc, measures, instant=True, quarter_of_annual_report=quarter_of_annual_report, years_diff=years_diff,
        get_ttm=False, get_yearly=False, debug=False)

    measures = ["DerivativeLiabilities"]
    most_recent_derivatives_liability, _, yearly_derivatives_liability = get_values_from_measures(
        doc, measures, instant=True, quarter_of_annual_report=quarter_of_annual_report, years_diff=years_diff,
        get_ttm=False, debug=False)

    measures = [
        "AccountsPayableAndAccruedLiabilitiesCurrentAndNoncurrent",
        "AccountsPayableCurrentAndNoncurrent",
    ]
    most_recent_ap, _, yearly_ap = get_values_from_measures(
        doc, measures, instant=True, quarter_of_annual_report=quarter_of_annual_report, years_diff=years_diff,
        get_ttm=False, debug=False)

    measures = ["DueToRelatedPartiesCurrentAndNoncurrent"]
    most_recent_due_related_parties, _, yearly_due_related_parties = get_values_from_measures(
        doc, measures, instant=True, quarter_of_annual_report=quarter_of_annual_report, years_diff=years_diff,
        get_ttm=False, debug=False)

    measures = ["DueToAffiliateCurrentAndNoncurrent"]
    most_recent_due_affiliates, _, yearly_due_affiliates = get_values_from_measures(
        doc, measures, instant=True, quarter_of_annual_report=quarter_of_annual_report, years_diff=years_diff,
        get_ttm=False, debug=False)

    # yearly_liabilities_ex_debt = {"dates": [], "values": []}
    # merge_subsets_yearly(yearly_liabilities_ex_debt, [yearly_derivatives_liability, yearly_ap, yearly_due_related_parties,
    #                                                   yearly_due_affiliates])

    # Current
    measures = ["LiabilitiesCurrent"]
    most_recent_liabilities_cur, _, _ = get_values_from_measures(
        doc, measures, instant=True, quarter_of_annual_report=quarter_of_annual_report, years_diff=years_diff,
        get_ttm=False, get_yearly=False, debug=False)

    measures = [
        "AccountsPayableAndOtherAccruedLiabilitiesCurrent",
        "AccountsPayableAndAccruedLiabilitiesCurrent",
    ]
    most_recent_ap_complete_cur, _, yearly_ap_complete_cur = get_values_from_measures(
        doc, measures, instant=True, quarter_of_annual_report=quarter_of_annual_report, years_diff=years_diff,
        get_ttm=False, debug=False)

    measures = ["AccountsPayableCurrent"]
    most_recent_ap_cur, _, yearly_ap_cur = get_values_from_measures(
        doc, measures, instant=True, quarter_of_annual_report=quarter_of_annual_report, years_diff=years_diff,
        get_ttm=False, debug=False)

    measures = ["AccountsPayableOtherCurrent"]
    most_recent_apother_cur, _, yearly_apother_cur = get_values_from_measures(
        doc, measures, instant=True, quarter_of_annual_report=quarter_of_annual_report, years_diff=years_diff,
        get_ttm=False, debug=False)

    measures = ["AccountsPayableRelatedPartiesCurrent"]
    most_recent_ap_rel_cur, _, yearly_ap_rel_cur = get_values_from_measures(
        doc, measures, instant=True, quarter_of_annual_report=quarter_of_annual_report, years_diff=years_diff,
        get_ttm=False, debug=False)

    measures = ["AccountsPayableTradeCurrent"]
    most_recent_ap_trade_cur, _, yearly_ap_trade_cur = get_values_from_measures(
        doc, measures, instant=True, quarter_of_annual_report=quarter_of_annual_report, years_diff=years_diff,
        get_ttm=False, debug=False)

    merge_subsets_yearly(yearly_ap_complete_cur, [yearly_ap_cur, yearly_apother_cur, yearly_ap_rel_cur,
                                                  yearly_ap_trade_cur])


    measures = ["DueToAffiliateCurrent"]
    most_recent_due_affiliates_cur, _, yearly_due_affiliates_cur = get_values_from_measures(
        doc, measures, instant=True, quarter_of_annual_report=quarter_of_annual_report, years_diff=years_diff,
        get_ttm=False, debug=False)

    measures = ["DueToRelatedPartiesCurrent"]
    most_recent_due_related_cur, _, yearly_due_related_cur = get_values_from_measures(
        doc, measures, instant=True, quarter_of_annual_report=quarter_of_annual_report, years_diff=years_diff,
        get_ttm=False, debug=False)

    measures = ["DerivativeLiabilitiesCurrent"]
    most_recent_derivatives_liability_cur, _, yearly_derivatives_liability_cur = get_values_from_measures(
        doc, measures, instant=True, quarter_of_annual_report=quarter_of_annual_report, years_diff=years_diff,
        get_ttm=False, debug=False)

    # merge_subsets_yearly(yearly_liabilities_cur, [yearly_ap_complete_cur, yearly_due_affiliates_cur, yearly_due_related_cur,
    #                                               yearly_derivatives_liability_cur])

    # Non - Current
    measures = ["LiabilitiesNoncurrent"]
    most_recent_liabilities_noncur, _, _ = get_values_from_measures(
        doc, measures, instant=True, quarter_of_annual_report=quarter_of_annual_report, years_diff=years_diff,
        get_ttm=False, get_yearly=False, debug=False)

    measures = ["DerivativeLiabilitiesNoncurrent"]
    most_recent_derivatives_liability_noncur, _, _ = get_values_from_measures(
        doc, measures, instant=True, quarter_of_annual_report=quarter_of_annual_report, years_diff=years_diff,
        get_ttm=False, get_yearly=False, debug=False)

    measures = ["DueToAffiliateNoncurrent"]
    most_recent_due_affiliates_noncur, _, _ = get_values_from_measures(
        doc, measures, instant=True, quarter_of_annual_report=quarter_of_annual_report, years_diff=years_diff,
         get_ttm=False, get_yearly=False, debug=False)

    measures = ["DueToRelatedPartiesNoncurrent"]
    most_recent_due_related_noncur, _, _ = get_values_from_measures(
        doc, measures, instant=True, quarter_of_annual_report=quarter_of_annual_report, years_diff=years_diff,
        get_ttm=False, get_yearly=False, debug=False)

    # merge_subsets_yearly(yearly_liabilities_noncur, [yearly_derivatives_liability_noncur, yearly_due_affiliates_noncur,
    #                                                  yearly_due_related_noncur])
    # merge_subsets_yearly(yearly_liabilities_ex_debt, [yearly_liabilities_cur, yearly_liabilities_noncur])
    # merge_subsets_yearly(yearly_liabilities, [yearly_liabilities_ex_debt, yearly_debt])

    merge_subsets_most_recent(most_recent_ap_complete_cur, [most_recent_ap_cur, most_recent_apother_cur,
                                                            most_recent_ap_rel_cur, most_recent_ap_trade_cur])
    merge_subsets_most_recent(most_recent_ap, [most_recent_ap_complete_cur])
    merge_subsets_most_recent(most_recent_derivatives_liability, [most_recent_derivatives_liability_cur,
                                                                  most_recent_derivatives_liability_noncur])
    merge_subsets_most_recent(most_recent_due_affiliates, [most_recent_due_affiliates_cur,
                                                                  most_recent_due_affiliates_noncur])
    merge_subsets_most_recent(most_recent_due_related_parties, [most_recent_due_related_cur,
                                                                  most_recent_due_related_noncur])

    merge_subsets_most_recent(most_recent_liabilities, [most_recent_liabilities_cur, most_recent_liabilities_noncur])
    combo_liabilities = {"date":None, "value":0}
    merge_subsets_most_recent(combo_liabilities, [most_recent_ap, most_recent_derivatives_liability,
                                                        most_recent_due_affiliates, most_recent_due_related_parties,
                                                        most_recent_debt])

    if most_recent_liabilities["date"] is None:
        most_recent_liabilities = combo_liabilities
    elif combo_liabilities["date"] is not None:
        if combo_liabilities["date"] > most_recent_liabilities["date"]:
            most_recent_liabilities = combo_liabilities
        elif combo_liabilities["date"] == most_recent_liabilities["date"] and combo_liabilities["value"] > most_recent_liabilities["value"]:
            most_recent_liabilities = combo_liabilities

    # for working capital
    merge_subsets_yearly(yearly_ap_complete_cur, [yearly_ap])
    merge_subsets_yearly(yearly_due_affiliates_cur, [yearly_due_affiliates])
    merge_subsets_yearly(yearly_due_related_cur, [yearly_due_related_parties])


    return {
        "mr_liabilities": most_recent_liabilities,
        "account_payable": yearly_ap_complete_cur,
        "due_to_affiliates": yearly_due_affiliates_cur,
        "due_to_related_parties": yearly_due_related_cur
    }

def extract_balance_sheet_equity(doc, last_annual_report_date, last_annual_report_fy):
    """
    Extract balance sheet EQUITY measures from company financial document.
    :param doc: company financial document
    :param last_annual_report_date: date of last annual report
    :return: dict with most recent and yearly measures
    """

    # measures = ["LiabilitiesAndStockholdersEquity"]
    # most_recent_liabilities_and_equity, _, yearly_liabilities_and_equity = get_values_from_measures(
    #     doc, measures, instant=True, last_annual_report_date=last_annual_report_date, get_ttm=False, debug=False)

    df = build_financial_df(doc, "StockholdersEquityIncludingPortionAttributableToNoncontrollingInterest")
    quarter_of_annual_report, years_diff = get_quarter_of_annual_report(df, last_annual_report_date, last_annual_report_fy)

    if quarter_of_annual_report is None:
        df = build_financial_df(doc, "StockholdersEquity")
        quarter_of_annual_report, years_diff = get_quarter_of_annual_report(df, last_annual_report_date,
                                                                            last_annual_report_fy)

    measures = ["StockholdersEquityIncludingPortionAttributableToNoncontrollingInterest"]
    most_recent_equity, _, yearly_equity = get_values_from_measures(
        doc, measures, instant=True, quarter_of_annual_report=quarter_of_annual_report, years_diff=years_diff,
        get_ttm=False, debug=False)

    measures = ["StockholdersEquity"]
    most_recent_equity_no_mi, _, yearly_equity_no_mi = get_values_from_measures(
        doc, measures, instant=True, quarter_of_annual_report=quarter_of_annual_report, years_diff=years_diff,
        get_ttm=False, debug=False)

    measures = ["MinorityInterest"]
    most_recent_minority_interest, _, yearly_minority_interest = get_values_from_measures(
        doc, measures, instant=True, quarter_of_annual_report=quarter_of_annual_report, years_diff=years_diff,
        get_ttm=False, debug=False)

    merge_subsets_yearly(yearly_equity, [yearly_equity_no_mi, yearly_minority_interest], (0,))

    if most_recent_equity["date"] is None or \
            (most_recent_equity_no_mi["date"] is not None and most_recent_equity_no_mi["date"] > most_recent_equity["date"]):
        merge_subsets_most_recent(most_recent_equity, [most_recent_equity_no_mi, most_recent_minority_interest])

    return {
        "mr_equity": most_recent_equity,
        "equity": yearly_equity,
        "mr_minority_interest": most_recent_minority_interest,
        "quarter_of_annual_report": quarter_of_annual_report,
        "years_diff": years_diff
    }

def extract_cashflow_statement(doc):
    """
    Extract cashflow statement measures from company financial document.
    Measures include:
    - dividends
    - CAPEX
    - net income
    - interest expense
    - gross profit
    - depreciation and amortization
    - EBIT
    :param doc: company financial document
    :return: dict with ttm and yearly measures
    """

    # DIVIDENDS
    measures = [
        "Dividends",
        "DividendsCash",
        "PaymentsOfDividends",
        "PaymentsOfOrdinaryDividends"
    ]
    _, ttm_dividends, yearly_dividends = get_values_from_measures(doc, measures, get_most_recent=False,
                                                                  debug=False)

    measures = [
        "DividendsCommonStock",
        "DividendsCommonStockCash",
        "PaymentsOfDividendsCommonStock"
    ]
    _, ttm_dividends_cs, yearly_dividends_cs = get_values_from_measures(doc, measures, get_most_recent=False,
                                                                        debug=False)

    measures = [
        "DividendsPreferredStock",
        "DividendsPreferredStockCash",
        "PaymentsOfDividendsPreferredStockAndPreferenceStock"
    ]
    _, ttm_dividends_ps, yearly_dividends_ps = get_values_from_measures(doc, measures, get_most_recent=False,
                                                                        debug=False)
    merge_subsets_yearly(yearly_dividends, [yearly_dividends_cs, yearly_dividends_ps])
    merge_subsets_most_recent(ttm_dividends, [ttm_dividends_cs, ttm_dividends_ps])

    # CAPEX

    # Acquisition
    measures = ["BusinessAcquisitionCostOfAcquiredEntityTransactionCosts"]
    _, _, yearly_acquisition_costs = get_values_from_measures(doc, measures, get_most_recent=False, get_ttm=False,
                                                                                  debug=False)

    measures = [
        "PaymentsForPreviousAcquisition",
        "PaymentsForProceedsFromPreviousAcquisition",
    ]
    _, _, yearly_acquisition_adj = get_values_from_measures(doc, measures, get_most_recent=False, get_ttm=False,
                                                                              debug=False)

    measures = [
        "PaymentsToAcquireBusinessesNetOfCashAcquired",
        "PaymentsToAcquireBusinessesGross",
    ]
    _, _, yearly_acquisition = get_values_from_measures(doc, measures, get_most_recent=False, get_ttm=False,
                                                                      debug=False)

    measures = ["PaymentsToAcquireBusinessTwoNetOfCashAcquired"]
    _, _, yearly_acquisition2 = get_values_from_measures(doc, measures, get_most_recent=False, get_ttm=False,
                                                                        debug=False)

    measures = ["PaymentsToAcquireInterestInSubsidiariesAndAffiliates"]
    _, _, yearly_sub_aff = get_values_from_measures(doc, measures, get_most_recent=False, get_ttm=False,
                                                              debug=False)

    measures = ["PaymentsToAcquireAdditionalInterestInSubsidiaries"]
    _, _, yearly_sub = get_values_from_measures(doc, measures, get_most_recent=False, get_ttm=False,
                                                      debug=False)

    measures = ["PaymentsToAcquireBusinessesAndInterestInAffiliates"]
    _, _, yearly_aff = get_values_from_measures(doc, measures, get_most_recent=False, get_ttm=False,
                                                      debug=False)

    merge_subsets_yearly(yearly_sub_aff, [yearly_sub, yearly_aff])

    measures = ["PaymentsToAcquireInterestInJointVenture"]
    _, _, yearly_jv = get_values_from_measures(doc, measures, get_most_recent=False, get_ttm=False,
                                                    debug=False)

    # PP&E
    measures = ["CapitalExpendituresIncurredButNotYetPaid"]
    _, _, yearly_capex_not_paid = get_values_from_measures(doc, measures, get_most_recent=False, get_ttm=False,
                                                                            debug=False)

    measures = ["PaymentsForCapitalImprovements"]
    _, _, yearly_capex_imp = get_values_from_measures(doc, measures, get_most_recent=False, get_ttm=False,
                                                                  debug=False)

    measures = ["PaymentsToAcquireOtherProductiveAssets"]
    _, _, yearly_productive_assets_other = get_values_from_measures(doc, measures, get_most_recent=False, get_ttm=False,
                                                                                              debug=False)

    measures = ["PaymentsToAcquireOtherPropertyPlantAndEquipment"]
    _, _, yearly_ppe_other = get_values_from_measures(doc, measures, get_most_recent=False, get_ttm=False,
                                                                  debug=False)

    measures = ["PaymentsToAcquireProductiveAssets"]
    _, _, yearly_productive_assets = get_values_from_measures(doc, measures, get_most_recent=False, get_ttm=False,
                                                                                  debug=False)

    measures = ["PaymentsToAcquirePropertyPlantAndEquipment"]
    _, _, yearly_ppe = get_values_from_measures(doc, measures, get_most_recent=False, get_ttm=False,
                                                      debug=False)

    measures = ["PaymentsForSoftware"]
    _, _, yearly_sw = get_values_from_measures(doc, measures, get_most_recent=False, get_ttm=False,
                                                    debug=False)

    measures = ["PaymentsToDevelopSoftware"]
    _, _, yearly_sw_dev = get_values_from_measures(doc, measures, get_most_recent=False, get_ttm=False,
                                                            debug=False)

    # Intangibles
    measures = ["PaymentsToAcquireIntangibleAssets"]
    _, _, yearly_intangibles = get_values_from_measures(doc, measures, get_most_recent=False, get_ttm=False,
                                                                      debug=False)

    yearly_capex = {"dates": [], "values": []}
    merge_subsets_yearly(yearly_capex, [yearly_acquisition_costs, yearly_acquisition_adj, yearly_acquisition,
                                        yearly_acquisition2, yearly_sub_aff, yearly_jv, yearly_capex_not_paid,
                                        yearly_capex_imp, yearly_productive_assets_other, yearly_ppe_other,
                                        yearly_productive_assets,
                                        yearly_ppe, yearly_sw, yearly_sw_dev, yearly_intangibles])

    return {
        "ttm_dividends": ttm_dividends,
        "dividends": yearly_dividends,
        "capex": yearly_capex
    }

def extract_operating_leases(doc):
    """
    Extract operating leases measures from company financial document.
    :param doc: company financial document
    :return: dict with most recent measures
    """

    # Last year expenses
    measures = [
        "OperatingLeasePayments",
        "OperatingLeaseCost",
        "OperatingLeaseExpense",
    ]
    _, mr_op_leases_expense, _ = get_values_from_measures(doc, measures, get_ttm=True, get_most_recent=False, get_yearly=False, debug=False)

    # Next year expenses
    measures = [
        "LesseeOperatingLeaseLiabilityPaymentsDueNextTwelveMonths",
        "OperatingLeasesFutureMinimumPaymentsDueCurrent",
        "LesseeOperatingLeaseLiabilityPaymentsDueNextRollingTwelveMonths",
    ]
    mr_op_leases_next_year, _, _ = get_values_from_measures(doc, measures, get_ttm=False, get_yearly=False, debug=False)

    # Next 2year expenses
    measures = [
        "LesseeOperatingLeaseLiabilityPaymentsDueYearTwo",
        "OperatingLeasesFutureMinimumPaymentsDueInTwoYears",
        "LesseeOperatingLeaseLiabilityPaymentsDueInRollingYearTwo",
    ]
    mr_op_leases_next_2year, _, _ = get_values_from_measures(doc, measures, get_ttm=False, get_yearly=False, debug=False)

    # Next 3year expenses
    measures = [
        "LesseeOperatingLeaseLiabilityPaymentsDueYearThree",
        "OperatingLeasesFutureMinimumPaymentsDueInThreeYears",
        "LesseeOperatingLeaseLiabilityPaymentsDueInRollingYearThree",
    ]
    mr_op_leases_next_3year, _, _ = get_values_from_measures(doc, measures, get_ttm=False, get_yearly=False, debug=False)

    # Next 4year expenses
    measures = [
        "LesseeOperatingLeaseLiabilityPaymentsDueYearFour",
        "OperatingLeasesFutureMinimumPaymentsDueInFourYears",
        "LesseeOperatingLeaseLiabilityPaymentsDueInRollingYearFour",
    ]
    mr_op_leases_next_4year, _, _ = get_values_from_measures(doc, measures, get_ttm=False, get_yearly=False, debug=False)

    # Next 5year expenses
    measures = [
        "LesseeOperatingLeaseLiabilityPaymentsDueYearFive",
        "OperatingLeasesFutureMinimumPaymentsDueInFiveYears",
        "LesseeOperatingLeaseLiabilityPaymentsDueInRollingYearFive",
    ]
    mr_op_leases_next_5year, _, _ = get_values_from_measures(doc, measures, get_ttm=False, get_yearly=False, debug=False)

    # After 5year expenses
    measures = [
        "LesseeOperatingLeaseLiabilityPaymentsDueAfterYearFive",
        "OperatingLeasesFutureMinimumPaymentsDueThereafter",
        "LesseeOperatingLeaseLiabilityPaymentsDueAfterRollingYearFive",
    ]
    mr_op_leases_after_5year, _, _ = get_values_from_measures(doc, measures, get_ttm=False, get_yearly=False, debug=False)

    return {
        "mr_op_leases_expense": mr_op_leases_expense,
        "mr_op_leases_next_year": mr_op_leases_next_year,
        "mr_op_leases_next_2year": mr_op_leases_next_2year,
        "mr_op_leases_next_3year": mr_op_leases_next_3year,
        "mr_op_leases_next_4year": mr_op_leases_next_4year,
        "mr_op_leases_next_5year": mr_op_leases_next_5year,
        "mr_op_leases_after_5year": mr_op_leases_after_5year
    }

def extract_options(doc):
    """
    Extract options measures from company financial document.
    :param doc: company financial document
    :return: dict with most recent measures
    """

    # Last year expenses
    measures = [
        "EmployeeServiceShareBasedCompensationNonvestedAwardsTotalCompensationCostNotYetRecognized",
    ]
    mr_sbc, _, _ = get_values_from_measures(doc, measures, get_ttm=False, get_yearly=False, debug=False)

    measures = [
        # options
        "EmployeeServiceShareBasedCompensationNonvestedAwardsTotalCompensationCostNotYetRecognizedStockOptions",
        "ShareBasedCompensationArrangementByShareBasedPaymentAwardOptionsOutstandingIntrinsicValue",
        "ShareBasedCompensationArrangementByShareBasedPaymentAwardOptionsVestedAndExpectedToVestOutstandingAggregateIntrinsicValue",
        "ShareBasedCompensationArrangementByShareBasedPaymentAwardOptionsVestedAndExpectedToVestExercisableAggregateIntrinsicValue",
        "SharebasedCompensationArrangementBySharebasedPaymentAwardOptionsExercisableIntrinsicValue1",
    ]
    mr_sbc_options, _, _ = get_values_from_measures(doc, measures, get_ttm=False, get_yearly=False, debug=False)

    measures = [
        # non-options
        "EmployeeServiceShareBasedCompensationNonvestedAwardsTotalCompensationCostNotYetRecognizedShareBasedAwardsOtherThanOptions",
        "SharebasedCompensationArrangementBySharebasedPaymentAwardEquityInstrumentsOtherThanOptionsAggregateIntrinsicValueOutstanding",
        "SharebasedCompensationArrangementBySharebasedPaymentAwardEquityInstrumentsOtherThanOptionsAggregateIntrinsicValueNonvested",
    ]
    mr_sbc_non_options, _, _ = get_values_from_measures(doc, measures, get_ttm=False, get_yearly=False, debug=False)

    merge_subsets_most_recent(mr_sbc, [mr_sbc_options, mr_sbc_non_options])

    return {
        "mr_sbc": mr_sbc,
    }

def extract_company_financial_information(cik):

    """
    Extract financial data required for valuation from company financial document
    :param cik: company cik
    :return: dict with income statement and balance sheet metrics
    """

    try:
        doc = mongodb.get_document("financial_data", cik)
    except:
        download_financial_data(cik)
        doc = mongodb.get_document("financial_data", cik)

    income_statement_measures = extract_income_statement(doc)
    last_annual_report_date = income_statement_measures["last_annual_report_date"]
    last_annual_report_fy = income_statement_measures["last_annual_report_fy"]

    equity = extract_balance_sheet_equity(doc, last_annual_report_date, last_annual_report_fy)
    quarter_of_annual_report = equity["quarter_of_annual_report"]
    years_diff = equity["years_diff"]

    shares = extract_shares(doc, quarter_of_annual_report, years_diff)
    current_assets = extract_balance_sheet_current_assets(doc, quarter_of_annual_report, years_diff)
    non_current_assets = extract_balance_sheet_noncurrent_assets(doc, quarter_of_annual_report, years_diff)
    debt = extract_balance_sheet_debt(doc, quarter_of_annual_report, years_diff)
    liabilities = extract_balance_sheet_liabilities(doc, quarter_of_annual_report, years_diff, debt["mr_debt"])
    cashflow_statement_measures = extract_cashflow_statement(doc)
    leases = extract_operating_leases(doc)
    options = extract_options(doc)

    return {
        **shares,
        **income_statement_measures,
        **current_assets,
        **non_current_assets,
        **debt,
        **liabilities,
        **equity,
        **cashflow_statement_measures,
        **leases,
        **options
    }

def get_selected_years(data, key, start, end):
    """
    Get the values corresponding to selected years from a dictionary {"key": {"dates":[],"values":[]}}
    :param data: dictionary {"key": {"dates":[],"values":[]}}
    :param key: the key of the dictionary that we want to extract the selected years
    :param start: initial year
    :param end: final year
    :return: list of values corresponding to selected years (or 0 if year not found)
    """

    r = []

    for y in range(start, end + 1, 1):
        try:
            idx = data[key]["dates"].index(y)
            r.append(data[key]["values"][idx] / 1000)
        except ValueError:
            r.append(0)

    return r

def null_valuation(price_per_share=0):

    fcff_value = div_value = liquidation_per_share = -1
    fcff_delta = div_delta = liquidation_delta = 10
    status = STATUS_KO

    return price_per_share, fcff_value, div_value, fcff_delta, div_delta, liquidation_per_share, liquidation_delta, status

def valuation(cik, years=5, recession_probability = 0.5, qualitative=False, debug=False):
    """
    Compute valuation for company. Valuation is done following principles teached by Prof. Damodaran in his Valuation
    Course (FCFF Valuation and Dividends Valuation).
    We build 4 different scenarios for both FCFF and Dividends Valuation:
    1. Earnings TTM & Historical Growth
    2. Earnings Normalized & Historical Growth
    3. Earnings TTM & Growth TTM
    4. Earnings Normalized & Growth Normalized
    Each scenario is also run with a recession hypothesis.
    We compute a median value for FCFF, Recession FCFF, Dividends, Recession Dividends and then compute 2
    Expected Values based on the recession_probability.
    These 2 values are then used to compute the final valuation (value/share) skewing the result towards the lowest
    value (to be conservative).

    :param cik: company cik
    :param years: how many financial years to consider in the valuation
    :param debug:
    :return: price_per_share (current price/share), fcff_value (FCFF EV), div_value (Dividends EV),
    fcff_delta premium(discount) on shares, div_delta premium(discount) on shares, status
    (OK if company is underpriced, NI if company is correctly priced, KO is company is overpriced)
    """

    # Check if we have financial data
    # Check if we have submissions (at least the last 10k)
    try:
        download_financial_data(cik)
        data = extract_company_financial_information(cik)
    except NoSharesException:
        print(cik, "no shares")
        return null_valuation()
    except StopIteration:
        print(cik, "no financial data")
        return null_valuation()

    if debug:
        print(data)
        print()

    # Retrieve company revenues
    try:
        final_year = data["revenue"]["dates"][-1]
        initial_year = final_year - years + 1
    except:
        print(cik, "no revenue")
        return null_valuation()

    # Calculate ERP
    erp = get_df_from_table("damodaran_erp")
    erp = erp[erp["date"] == erp["date"].max()]["value"].iloc[0]

    # Retrieve company info
    company_info = company_from_cik(cik)
    ticker = company_info["ticker"]

    # Get current price per share from yahoo
    price_per_share = get_current_price_from_yahoo(ticker)
    if price_per_share is None:
        print(ticker, "delisted")
        return null_valuation()

    # Get generic info
    try:
        company_name, country, industry, region = get_generic_info(ticker)
    except IndexError:
        print(ticker, "not found in db")
        return null_valuation()

    # Retrieve currency and financial currency from postgreSQL DB
    yahoo_equity_ticker = get_df_from_table("yahoo_equity_tickers", f"where symbol = '{ticker}'", most_recent=True).iloc[0]
    db_curr = yahoo_equity_ticker["currency"]
    db_financial_curr = yahoo_equity_ticker["financial_currency"]

    # Retrieve bond_spread from postgreSQL DB
    damodaran_bond_spread = get_df_from_table("damodaran_bond_spread", most_recent=True)
    damodaran_bond_spread["greater_than"] = pd.to_numeric(damodaran_bond_spread["greater_than"])
    damodaran_bond_spread["less_than"] = pd.to_numeric(damodaran_bond_spread["less_than"])

    # Make sure to retrieve last annual report (10-K on SEC)
    doc = get_last_document(cik, "10-K")

    # Extract business segments and geographic distributions
    if doc is not None:
        segments = extract_segments(doc)
        geo_segments_df = geography_distribution(segments, ticker)
    else:
        geo_segments_df = None

    # Retrieve country statistics from postgreSQL DB
    country_stats = get_df_from_table("damodaran_country_stats", most_recent=True)

    # Compute tax rate, country default spread anc country risk premium based on company country
    tax_rate = 0
    country_default_spread = 0
    country_risk_premium = 0

    if geo_segments_df is None or geo_segments_df.empty:
        try:
            filter_df = country_stats[country_stats["country"] == country.replace(" ", "")].iloc[0]
        except:
            filter_df = country_stats[country_stats["country"] == "Global"].iloc[0]
        tax_rate = float(filter_df["tax_rate"])
        country_default_spread = float(filter_df["adjusted_default_spread"])
        country_risk_premium = float(filter_df["country_risk_premium"])
    else:
        for _, row in geo_segments_df.iterrows():
            percent = row["value"]
            search_key = row["country_area"]

            try:
                filter_df = country_stats[country_stats["country"] == search_key.replace(" ", "")].iloc[0]
            except:
                filter_df = country_stats[country_stats["country"] == "Global"].iloc[0]

            t = float(filter_df["tax_rate"])
            cds = float(filter_df["adjusted_default_spread"])
            crp = float(filter_df["country_risk_premium"])

            tax_rate += t * percent
            country_default_spread += cds * percent
            country_risk_premium += crp * percent

    # Compute final ERP adding country risk premium
    final_erp = float(erp) + country_risk_premium

    # Select alpha_3_code from company country
    try:
        alpha_3_code = country_stats[country_stats["country"] == country.replace(" ", "")].iloc[0]["alpha_3_code"]
    except:
        alpha_3_code = None

    # Retrieve the riskfree rate based on the company financial currency and the country statistics
    riskfree = currency_bond_yield(db_financial_curr, alpha_3_code, country_stats)

    # Check if the riskfree rate exists
    if riskfree == -1:
        print(ticker, "no riskfree")
        return null_valuation(price_per_share)

    if debug:
        print("===== GENERAL INFORMATION =====\n")
        print("ticker", ticker)
        print("cik", cik)
        print("company_name", company_name)
        print("country", country)
        print("region", region)
        print("industry", industry)
        print("financial currency", db_financial_curr)
        print("riskfree", riskfree)
        print("erp", erp)
        print("\n\n")

    # Retrieve shares number
    mr_shares = data["mr_shares"]["value"] / 1000
    shares = get_selected_years(data, "shares", initial_year, final_year)

    # Convert Currency
    fx_rate = None
    if db_curr is None or db_curr.strip() == "":
        return null_valuation(price_per_share)
    if db_financial_curr is None or db_financial_curr.strip() == "":
        return null_valuation(price_per_share)
    # they are different
    if db_curr != db_financial_curr:
        fx_rate = convert_currencies(db_curr, db_financial_curr)
    fx_rate_financial_USD = 1
    if db_financial_curr != "USD":
        fx_rate_financial_USD = convert_currencies("USD", db_financial_curr)

    # Retrieve financial data
    ttm_revenue = data["ttm_revenue"]["value"] / 1000
    ttm_ebit = data["ttm_ebit"]["value"] / 1000
    ttm_net_income = data["ttm_net_income"]["value"] / 1000
    ttm_dividends = data["ttm_dividends"]["value"] / 1000
    ttm_interest_expense = data["ttm_interest_expenses"]["value"] / 1000
    mr_cash = data["mr_cash"]["value"] / 1000
    mr_securities = data["mr_securities"]["value"] / 1000
    mr_debt = data["mr_debt"]["value"] / 1000
    mr_equity = data["mr_equity"]["value"] / 1000
    ebit = get_selected_years(data, "ebit", initial_year, final_year)
    net_income = get_selected_years(data, "net_income", initial_year, final_year)
    dividends = get_selected_years(data, "dividends", initial_year, final_year)
    capex = get_selected_years(data, "capex", initial_year, final_year)
    depreciation = get_selected_years(data, "depreciation", initial_year, final_year)
    equity_bv = get_selected_years(data, "equity", initial_year, final_year)
    cash = get_selected_years(data, "cash", initial_year, final_year)
    securities = get_selected_years(data, "securities", initial_year, final_year)
    debt_bv = get_selected_years(data, "debt", initial_year, final_year)
    revenue = get_selected_years(data, "revenue", initial_year-1, final_year)

    # Compute revenue growth
    revenue_growth = []
    revenue_delta = []
    for i in range(len(revenue) - 1):
        if revenue[i] < 0:
            print("negative revenue")
            return null_valuation(price_per_share)
        revenue_delta.append(revenue[i + 1] - revenue[i])
        try:
            revenue_growth.append(revenue[i + 1] / revenue[i] - 1)
        except:
            revenue_growth.append(0)

    # Drop 1st element we don't need
    revenue = revenue[1:]
    revenue_growth = revenue_growth[1:]

    # Retrieve R&D
    try:
        r_and_d_amortization_years = r_and_d_amortization[industry]
    except:
        print(f"\n#######\nCould not find industry: {industry} mapping. "
              f"Check r_and_d_amortization dictionary.\n#######\n")
        r_and_d_amortization_years = 5

    r_and_d = get_selected_years(data, "rd", final_year - r_and_d_amortization_years, final_year)
    while len(r_and_d) < years:
        r_and_d.insert(0, 0)
    ebit_r_and_d_adj, tax_benefit, r_and_d_unamortized, r_and_d_amortization_cy = \
        capitalize_rd(r_and_d, r_and_d_amortization_years, tax_rate, years)

    # Compute R&D-adjusted values
    ttm_ebit_adj = ttm_ebit + ebit_r_and_d_adj[-1]
    ebit_adj = [sum(x) for x in zip(ebit, ebit_r_and_d_adj)]
    ttm_net_income_adj = ttm_net_income + ebit_r_and_d_adj[-1]
    net_income_adj = [sum(x) for x in zip(net_income, ebit_r_and_d_adj)]
    mr_equity_adj = mr_equity + r_and_d_unamortized[-1]
    equity_bv_adj = [sum(x) for x in zip(equity_bv, r_and_d_unamortized)]
    capex_adj = [sum(x) for x in zip(capex, r_and_d[-years:])]
    depreciation_adj = [sum(x) for x in zip(depreciation, r_and_d_amortization_cy)]
    ebit_after_tax = [sum(x) for x in zip([x * (1 - tax_rate) for x in ebit_adj], tax_benefit)]
    ttm_eps_adj = ttm_net_income_adj / mr_shares

    # Retrieve Operating Leases
    leases = [
        data["mr_op_leases_expense"]["value"] / 1000,
        data["mr_op_leases_next_year"]["value"] / 1000,
        data["mr_op_leases_next_2year"]["value"] / 1000,
        data["mr_op_leases_next_3year"]["value"] / 1000,
        data["mr_op_leases_next_4year"]["value"] / 1000,
        data["mr_op_leases_next_5year"]["value"] / 1000,
        data["mr_op_leases_after_5year"]["value"] / 1000,
    ]
    last_year_leases = max([i for i, x in enumerate(leases) if x != 0], default=-1)
    if last_year_leases != -1:
        ebit_op_adj, int_exp_op_adj, debt_adj, tax_benefit_op, company_default_spread = \
            debtize_op_leases(ttm_interest_expense, ttm_ebit_adj, damodaran_bond_spread, riskfree, country_default_spread,
                          leases, last_year_leases, tax_rate, revenue_growth)

        # Compute OperatingLeases-adjusted values
        ttm_ebit_adj += ebit_op_adj[-1]
        ttm_interest_expense_adj = ttm_interest_expense + int_exp_op_adj
        mr_debt_adj = mr_debt + debt_adj[-1]
        ebit_adj = [sum(x) for x in zip(ebit_adj, ebit_op_adj)]
        debt_bv_adj = [sum(x) for x in zip(debt_bv, debt_adj)]
        ebit_after_tax = [sum(x) for x in zip(ebit_after_tax, tax_benefit_op)]

        ttm_ebit_after_tax = ttm_ebit_adj * (1 - tax_rate) + tax_benefit[-1] + tax_benefit_op[-1]
    # no leases
    else:
        ttm_interest_expense_adj = ttm_interest_expense
        mr_debt_adj = mr_debt
        debt_bv_adj = debt_bv
        company_default_spread = get_spread_from_dscr(12.5, damodaran_bond_spread)
        ttm_ebit_after_tax = ttm_ebit_adj * (1 - tax_rate) + tax_benefit[-1]

    # Compute cost of debt
    cost_of_debt = riskfree + country_default_spread + company_default_spread

    # Compute cash and securities
    mr_cash_and_securities = mr_cash + mr_securities
    cash_and_securities = [sum(x) for x in zip(cash, securities)]

    # Consider EPS/dividends as with most recent number of shares (to account for splits and buybacks)
    eps = [x / mr_shares for x in net_income]
    eps_adj = [x/mr_shares for x in net_income_adj]
    dividends = [x/mr_shares for x in dividends]

    # Compute Working Capital as inventory + receivables + other assets - payables - due to affiliates - due to related
    wc = {}
    for i in ["inventory", "receivables", "other_assets", "account_payable", "due_to_affiliates", "due_to_related_parties"]:
        val = get_selected_years(data, i, initial_year-1, final_year)
        wc[i] = val
    df = pd.DataFrame(wc)
    df["wc"] = df["inventory"] + df["receivables"] + df["other_assets"] - df["account_payable"] \
               - df["due_to_affiliates"] - df["due_to_related_parties"]
    df["delta_wc"] = df["wc"].diff(1)
    df = df.dropna()
    working_capital = df["wc"].to_list()
    delta_wc = df["delta_wc"].to_list()

    # Compute reinvestments
    reinvestment = []
    for i in range(len(capex)):
        reinvestment.append(capex_adj[i] + delta_wc[i] - depreciation_adj[i])

    # Compute equity market value (market cap)
    equity_mkt = mr_shares * price_per_share
    if fx_rate is not None:
        equity_mkt /= fx_rate

    # Compute debt market value
    debt_mkt = ttm_interest_expense_adj * (1 - (1 + cost_of_debt) ** -6) / cost_of_debt + mr_debt_adj / (
                1 + cost_of_debt) ** 6

    # Get company industry data
    target_sales_capital, industry_payout, pbv, unlevered_beta, target_operating_margin, target_debt_equity = \
        get_industry_data(industry, region, geo_segments_df, revenue, ebit_adj, revenue_delta, reinvestment,
                          equity_mkt, debt_mkt, equity_bv_adj, debt_bv_adj, mr_equity_adj, mr_debt_adj)

    # Retrieve minority interest
    mr_original_min_interest = data["mr_minority_interest"]["value"] / 1000
    mr_minority_interest = mr_original_min_interest * pbv

    # Retrieve tax benefits and Share Based Compensation
    mr_tax_benefits = data["mr_tax_benefits"]["value"] / 1000
    mr_sbc = data["mr_sbc"]["value"] / 1000

    if debug:
        print("===== Last Available Data =====\n")
        print("Outstanding Shares", mr_shares)
        print("Price/Share (price currency)", price_per_share)
        print("FX Rate:", 1 if fx_rate is None else fx_rate)
        print("FX Rate USD:", fx_rate_financial_USD)
        print("ttm_revenue", ttm_revenue)
        print("ttm_ebit", ttm_ebit, "=>", ttm_ebit_adj)
        print("ttm_net_income", ttm_net_income, "=>", ttm_net_income_adj)
        print("ttm_dividends", ttm_dividends)
        # print("ttm_eps", ttm_eps, "=>", ttm_eps_adj)
        print("ttm_interest_expense", ttm_interest_expense, "=>", ttm_interest_expense_adj)
        print("tax_credit", mr_tax_benefits)
        # print("minority_interest", mr_original_min_interest, "=>", mr_minority_interest)
        # print("cash&securities", mr_cash_and_securities)
        # print("BV of debt", mr_debt, "=>", mr_debt_adj)
        # print("BV of equity", mr_equity, "=>", mr_equity_adj)
        print("\n\n")
        print("===== Historical Data =====\n")
        print("initial_year", initial_year)
        print("revenue", revenue)
        print("revenue_delta", revenue_delta)
        print("ebit", ebit, "=>", ebit_adj)
        # print("ebit_after_tax_adj", ebit_after_tax)
        print("net_income", net_income, "=>", net_income_adj)
        # print("eps", eps, "=>", eps_adj)
        print("dividends", dividends)
        print("working_capital", working_capital)
        print("delta_WC", delta_wc)
        print("capex", capex, "=>", capex_adj)
        print("depreciation", depreciation, "=>", depreciation_adj)
        print("shares_outstanding", shares)
        print("equity_bv", equity_bv, "=>", equity_bv_adj)
        print("cash&securities", cash_and_securities)
        print("debt_bv", debt_bv, "=>", debt_bv_adj)
        print("\n\n")
        print("===== R&D =====")
        print("r_and_d", r_and_d)
        print("amortization_years", r_and_d_amortization_years)
        print("\n===== Operating Leases =====")
        print("leases", leases)
        print("\n===== Segments =====\n")
        if geo_segments_df is None:
            print("10-K not found. Check annual report on company website.")
        else:
            print(geo_segments_df.to_markdown())
        print("\n===== Options =====")
        print("mr_sbc", mr_sbc)
        print("\n\n")

    # Compute get_growth_ttm
    roc_last, reinvestment_last, growth_last, roe_last, reinvestment_eps_last, growth_eps_last = \
        get_growth_ttm(ttm_ebit_after_tax, ttm_net_income_adj, mr_equity_adj, mr_debt_adj, mr_cash_and_securities,
                       reinvestment, ttm_dividends, industry_payout)

    # Get ROE and ROC
    roe, roc = get_roe_roc(equity_bv_adj, debt_bv_adj, cash_and_securities, ebit_after_tax, net_income_adj)

    # Compute Target info
    cagr, target_levered_beta, target_cost_of_equity, target_cost_of_debt, target_cost_of_capital = \
        get_target_info(revenue, ttm_revenue, country_default_spread, tax_rate, final_erp, riskfree,
                        unlevered_beta, damodaran_bond_spread, company_default_spread, target_debt_equity)
    # Normalize info
    revenue_5y, ebit_5y, operating_margin_5y, sales_capital_5y, roc_5y, reinvestment_5y, growth_5y, \
    net_income_5y, roe_5y, reinvestment_eps_5y, growth_eps_5y = \
        get_normalized_info(revenue, ebit_adj, revenue_delta, reinvestment, target_sales_capital,
                        ebit_after_tax, industry_payout, cagr, net_income_adj, roe, dividends, eps_adj, roc)

    # Get dividends info
    eps_5y, payout_5y = get_dividends_info(eps_adj, dividends)

    # Get final info
    survival_prob, debt_equity, \
    levered_beta, cost_of_equity, equity_weight, debt_weight, cost_of_capital = \
        get_final_info(riskfree, cost_of_debt, equity_mkt, debt_mkt, unlevered_beta,
                   tax_rate, final_erp, company_default_spread)

    # Select final data
    mr_receivables = data["mr_receivables"]["value"] / 1000
    mr_inventory = data["mr_inventory"]["value"] / 1000
    mr_other_current_assets = data["mr_other_assets"]["value"] / 1000
    mr_ppe = data["mr_ppe"]["value"] / 1000
    mr_property = data["mr_investment_property"]["value"] / 1000
    mr_equity_investments = data["mr_equity_investments"]["value"] / 1000
    mr_total_liabilities = data["mr_liabilities"]["value"] / 1000

    # Compute liquidation value
    try:
        liquidation_value = calculate_liquidation_value(mr_cash, mr_receivables, mr_inventory, mr_securities,
                                                        mr_other_current_assets, mr_property,
                                                        mr_ppe, mr_equity_investments, mr_total_liabilities, equity_mkt,
                                                        mr_debt, mr_equity, mr_original_min_interest,
                                                        mr_minority_interest, debug=debug)
    except:
        print(traceback.format_exc())
        liquidation_value = 0

    # Compute liquidation per share
    liquidation_per_share = liquidation_value / mr_shares

    if debug:
        print("===== Growth =====\n")
        print("cagr", round(cagr,4))
        print("riskfree", round(riskfree,4))
        print("\n\n")
        print("===== Model Helper Calculation =====\n")
        print("roc_last", round(roc_last,4))
        print("reinvestment_last", round(reinvestment_last,4))
        print("growth_last", round(growth_last,4))
        print("ROC history", roc)
        print("roc_5y", round(roc_5y,4))
        print("Reinvestment history", reinvestment)
        print("reinvestment_5y", round(reinvestment_5y,4))
        print("growth_5y", round(growth_5y,4))
        print("revenue_5y", revenue_5y)
        print("ebit_5y", ebit_5y)
        print("roe_last", round(roe_last,4))
        print("reinvestment_eps_last", round(reinvestment_eps_last,4))
        print("growth_eps_last", round(growth_eps_last,4))
        print("sales_capital_5y", round(sales_capital_5y,4))
        print("roe_5y", round(roe_5y,4))
        print("reinvestment_eps_5y", round(reinvestment_eps_5y,4))
        print("growth_eps_5y", round(growth_eps_5y,4))
        print("eps_5y", round(eps_5y,4))
        print("payout_5y", round(payout_5y,4))
        print("industry_payout", round(industry_payout,4))
        print("target_sales_capital", round(target_sales_capital,4))
        print("\n\n")
        print("===== Recap Info =====\n")
        print("country_default_spread", round(country_default_spread,4))
        print("country_risk_premium", round(country_risk_premium,4))
        print("riskfree", round(riskfree,4))
        print("final_erp", round(final_erp,4))
        print("unlevered_beta", round(unlevered_beta,4))
        print("tax_rate", round(tax_rate,4))
        print("levered_beta", round(levered_beta,4))
        print("cost_of_equity", round(cost_of_equity,4))
        print("cost_of_debt", round(cost_of_debt,4))
        print("equity_weight", round(equity_weight,4))
        print("debt_weight", round(debt_weight,4))
        print("cost_of_capital", round(cost_of_capital,4))
        print("equity_mkt", round(equity_mkt,2))
        print("debt_mkt", round(debt_mkt,2))
        print("debt_equity", round(debt_equity,4))
        print("equity_bv_adj", round(mr_equity_adj,2))
        print("debt_bv_adj", round(mr_debt_adj,2))
        print("ebit_adj", round(ttm_ebit_adj,2))
        print("company_default_spread", round(company_default_spread,4))
        print("survival_prob", round(survival_prob,4))
        print("liquidation value", round(liquidation_value, 2))
        print("\n\n")
        print("===== Other Model inputs =====\n")
        print("operating_margin_5y", round(operating_margin_5y,4))
        print("target_operating_margin", round(target_operating_margin,4))
        print("target_debt_equity", round(target_debt_equity,4))
        print("target_levered_beta", round(target_levered_beta,4))
        print("target_cost_of_equity", round(target_cost_of_equity,4))
        print("target_cost_of_debt", round(target_cost_of_debt,4))
        print("target_cost_of_capital", round(target_cost_of_capital,4))
        print("\n\n")


    # Perform valuations


    dict_values_for_bi = {}

    stock_value_div_ttm_fixed = dividends_valuation(EARNINGS_TTM, GROWTH_FIXED, cagr, growth_eps_5y, growth_5y,
                                                    riskfree, industry_payout, cost_of_equity,
                                                    target_cost_of_equity, growth_eps_last, eps_5y, payout_5y, ttm_eps_adj,
                                                    reinvestment_eps_last, fx_rate, survival_prob, liquidation_per_share, debug=debug, dict_values_for_bi=dict_values_for_bi)
    stock_value_div_norm_fixed = dividends_valuation(EARNINGS_NORM, GROWTH_FIXED, cagr, growth_eps_5y, growth_5y,
                                                     riskfree, industry_payout, cost_of_equity,
                                                     target_cost_of_equity, growth_eps_last, eps_5y, payout_5y, ttm_eps_adj,
                                                    reinvestment_eps_last, fx_rate, survival_prob, liquidation_per_share, debug=debug, dict_values_for_bi=dict_values_for_bi)
    stock_value_div_ttm_ttm = dividends_valuation(EARNINGS_TTM, GROWTH_TTM, cagr, growth_eps_5y, growth_5y, riskfree,
                                                  industry_payout, cost_of_equity, target_cost_of_equity,
                                                  growth_eps_last, eps_5y, payout_5y, ttm_eps_adj,
                                                    reinvestment_eps_last, fx_rate, survival_prob, liquidation_per_share, debug=debug, dict_values_for_bi=dict_values_for_bi)
    stock_value_div_norm_norm = dividends_valuation(EARNINGS_NORM, GROWTH_NORM, cagr, growth_eps_5y, growth_5y, riskfree,
                                                    industry_payout, cost_of_equity,
                                                    target_cost_of_equity, growth_eps_last, eps_5y, payout_5y, ttm_eps_adj,
                                                    reinvestment_eps_last, fx_rate, survival_prob, liquidation_per_share, debug=debug, dict_values_for_bi=dict_values_for_bi)
    stock_value_div_ttm_fixed_recession = dividends_valuation(EARNINGS_TTM, GROWTH_FIXED, cagr, growth_eps_5y, growth_5y,
                                                    riskfree, industry_payout, cost_of_equity,
                                                    target_cost_of_equity, growth_eps_last, eps_5y, payout_5y, ttm_eps_adj,
                                                    reinvestment_eps_last, fx_rate, survival_prob, liquidation_per_share, debug=debug, recession=True, dict_values_for_bi=dict_values_for_bi)
    stock_value_div_norm_fixed_recession = dividends_valuation(EARNINGS_NORM, GROWTH_FIXED, cagr, growth_eps_5y, growth_5y,
                                                     riskfree, industry_payout, cost_of_equity,
                                                     target_cost_of_equity, growth_eps_last, eps_5y, payout_5y, ttm_eps_adj,
                                                    reinvestment_eps_last, fx_rate, survival_prob, liquidation_per_share, debug=debug, recession=True, dict_values_for_bi=dict_values_for_bi)
    stock_value_div_ttm_ttm_recession = dividends_valuation(EARNINGS_TTM, GROWTH_TTM, cagr, growth_eps_5y, growth_5y, riskfree,
                                                  industry_payout, cost_of_equity, target_cost_of_equity,
                                                  growth_eps_last, eps_5y, payout_5y, ttm_eps_adj,
                                                    reinvestment_eps_last, fx_rate, survival_prob, liquidation_per_share, debug=debug, recession=True, dict_values_for_bi=dict_values_for_bi)
    stock_value_div_norm_norm_recession = dividends_valuation(EARNINGS_NORM, GROWTH_NORM, cagr, growth_eps_5y, growth_5y, riskfree,
                                                    industry_payout, cost_of_equity,
                                                    target_cost_of_equity, growth_eps_last, eps_5y, payout_5y, ttm_eps_adj,
                                                    reinvestment_eps_last, fx_rate, survival_prob, liquidation_per_share, debug=debug, recession=True, dict_values_for_bi=dict_values_for_bi)

    stock_value_fcff_ttm_fixed = fcff_valuation(EARNINGS_TTM, GROWTH_FIXED, cagr, riskfree, ttm_revenue, ttm_ebit_adj,
                                                target_operating_margin, mr_tax_benefits, tax_rate, sales_capital_5y, target_sales_capital,
                                                debt_equity, target_debt_equity, unlevered_beta, final_erp, cost_of_debt,
                                                target_cost_of_debt, mr_cash, mr_securities, debt_mkt, mr_minority_interest, survival_prob, mr_shares,
                                                liquidation_value, growth_last, growth_5y, revenue_5y, ebit_5y, fx_rate, mr_property, mr_sbc, debug=debug, dict_values_for_bi=dict_values_for_bi)
    stock_value_fcff_norm_fixed = fcff_valuation(EARNINGS_NORM, GROWTH_FIXED, cagr, riskfree, ttm_revenue, ttm_ebit_adj,
                                                 target_operating_margin, mr_tax_benefits, tax_rate, sales_capital_5y, target_sales_capital,
                                                 debt_equity, target_debt_equity, unlevered_beta, final_erp, cost_of_debt,
                                                 target_cost_of_debt, mr_cash, mr_securities, debt_mkt, mr_minority_interest, survival_prob, mr_shares,
                                                 liquidation_value, growth_last, growth_5y, revenue_5y, ebit_5y, fx_rate, mr_property, mr_sbc, debug=debug, dict_values_for_bi=dict_values_for_bi)
    stock_value_fcff_ttm_ttm = fcff_valuation(EARNINGS_TTM, GROWTH_TTM, cagr, riskfree, ttm_revenue, ttm_ebit_adj,
                                              target_operating_margin, mr_tax_benefits, tax_rate, sales_capital_5y, target_sales_capital,
                                              debt_equity, target_debt_equity, unlevered_beta, final_erp, cost_of_debt,
                                              target_cost_of_debt, mr_cash, mr_securities, debt_mkt, mr_minority_interest, survival_prob, mr_shares,
                                              liquidation_value, growth_last, growth_5y, revenue_5y, ebit_5y, fx_rate, mr_property, mr_sbc, debug=debug, dict_values_for_bi=dict_values_for_bi)
    stock_value_fcff_norm_norm = fcff_valuation(EARNINGS_NORM, GROWTH_NORM, cagr, riskfree, ttm_revenue, ttm_ebit_adj,
                                                target_operating_margin, mr_tax_benefits, tax_rate, sales_capital_5y, target_sales_capital,
                                                debt_equity, target_debt_equity, unlevered_beta, final_erp, cost_of_debt,
                                                target_cost_of_debt, mr_cash, mr_securities, debt_mkt, mr_minority_interest, survival_prob, mr_shares,
                                                liquidation_value, growth_last, growth_5y, revenue_5y, ebit_5y, fx_rate, mr_property, mr_sbc, debug=debug, dict_values_for_bi=dict_values_for_bi)
    stock_value_fcff_ttm_fixed_recession = fcff_valuation(EARNINGS_TTM, GROWTH_FIXED, cagr, riskfree, ttm_revenue, ttm_ebit_adj,
                                                          target_operating_margin, mr_tax_benefits, tax_rate, sales_capital_5y, target_sales_capital,
                                                          debt_equity, target_debt_equity, unlevered_beta, final_erp, cost_of_debt,
                                                          target_cost_of_debt, mr_cash, mr_securities, debt_mkt, mr_minority_interest, survival_prob, mr_shares,
                                                          liquidation_value, growth_last, growth_5y, revenue_5y, ebit_5y, fx_rate, mr_property, mr_sbc, debug=debug, recession=True, dict_values_for_bi=dict_values_for_bi)
    stock_value_fcff_norm_fixed_recession = fcff_valuation(EARNINGS_NORM, GROWTH_FIXED, cagr, riskfree, ttm_revenue, ttm_ebit_adj,
                                                           target_operating_margin, mr_tax_benefits, tax_rate, sales_capital_5y, target_sales_capital,
                                                           debt_equity, target_debt_equity, unlevered_beta, final_erp, cost_of_debt,
                                                           target_cost_of_debt, mr_cash, mr_securities, debt_mkt, mr_minority_interest, survival_prob, mr_shares,
                                                           liquidation_value, growth_last, growth_5y, revenue_5y, ebit_5y, fx_rate, mr_property, mr_sbc, debug=debug, recession=True, dict_values_for_bi=dict_values_for_bi)
    stock_value_fcff_ttm_ttm_recession = fcff_valuation(EARNINGS_TTM, GROWTH_TTM, cagr, riskfree, ttm_revenue, ttm_ebit_adj,
                                                        target_operating_margin, mr_tax_benefits, tax_rate, sales_capital_5y, target_sales_capital,
                                                        debt_equity, target_debt_equity, unlevered_beta, final_erp, cost_of_debt,
                                                        target_cost_of_debt, mr_cash, mr_securities, debt_mkt, mr_minority_interest, survival_prob, mr_shares,
                                                        liquidation_value, growth_last, growth_5y, revenue_5y, ebit_5y, fx_rate, mr_property, mr_sbc, debug=debug, recession=True, dict_values_for_bi=dict_values_for_bi)
    stock_value_fcff_norm_norm_recession = fcff_valuation(EARNINGS_NORM, GROWTH_NORM, cagr, riskfree, ttm_revenue, ttm_ebit_adj,
                                                          target_operating_margin, mr_tax_benefits, tax_rate, sales_capital_5y, target_sales_capital,
                                                          debt_equity, target_debt_equity, unlevered_beta, final_erp, cost_of_debt,
                                                          target_cost_of_debt, mr_cash, mr_securities, debt_mkt, mr_minority_interest, survival_prob, mr_shares,
                                                          liquidation_value, growth_last, growth_5y, revenue_5y, ebit_5y, fx_rate, mr_property, mr_sbc, debug=debug, recession=True, dict_values_for_bi=dict_values_for_bi)

    # Aggregate valuation
    fcff_values_list = [stock_value_fcff_ttm_fixed, stock_value_fcff_norm_fixed, stock_value_fcff_ttm_ttm,
                       stock_value_fcff_norm_norm]
    fcff_recession_values_list = [stock_value_fcff_ttm_fixed_recession, stock_value_fcff_norm_fixed_recession,
                                              stock_value_fcff_ttm_ttm_recession, stock_value_fcff_norm_norm_recession]
    div_values_list = [stock_value_div_ttm_fixed, stock_value_div_norm_fixed, stock_value_div_ttm_ttm,
                       stock_value_div_norm_norm]
    div_recession_values_list = [stock_value_div_ttm_fixed_recession, stock_value_div_norm_fixed_recession,
                                             stock_value_div_ttm_ttm_recession, stock_value_div_norm_norm_recession]

    # Summarize valuations
    fcff_value = summary_valuation(fcff_values_list)
    fcff_recession_value = summary_valuation(fcff_recession_values_list)
    ev_fcff = fcff_value * (1 - recession_probability) + fcff_recession_value * recession_probability
    div_value = summary_valuation(div_values_list)
    div_recession_value = summary_valuation(div_recession_values_list)
    ev_dividends = div_value * (1 - recession_probability) + div_recession_value * recession_probability

    if fx_rate is not None:
        fcff_value *= fx_rate
        div_value *= fx_rate
        liquidation_per_share *= fx_rate

    # Compute valuation delta
    fcff_delta = price_per_share / ev_fcff - 1 if fcff_value > 0 else 10
    div_delta = price_per_share / ev_dividends - 1 if div_value > 0 else 10
    liquidation_delta = price_per_share / liquidation_per_share - 1 if liquidation_per_share > 0 else 10

    # Compute company size
    market_cap_USD = equity_mkt * fx_rate_financial_USD
    if market_cap_USD < 50 * 10 ** 3:
        company_size = "Nano"
    elif market_cap_USD < 300 * 10 ** 3:
        company_size = "Micro"
    elif market_cap_USD < 2 * 10 ** 6:
        company_size = "Small"
    elif market_cap_USD < 10 * 10 ** 6:
        company_size = "Medium"
    elif market_cap_USD < 200 * 10 ** 6:
        company_size = "Large"
    else:
        company_size = "Mega"

    # Compute company complexity
    complexity = company_complexity(doc, industry, company_size)
    # Compute company share diluition
    dilution = company_share_diluition(shares)
    # Retrieve inventory and account receivables
    inventory = get_selected_years(data, "inventory", initial_year-1, final_year)
    receivables = get_selected_years(data, "receivables", initial_year-1, final_year)
    company_type = get_company_type(revenue_growth, mr_debt_adj, equity_mkt, liquidation_value, operating_margin_5y, industry)
    # Retrieve auditor
    auditor = find_auditor(doc)

    if debug:
        print("===== Risk Assessment =====\n")
        print("MKT CAP USD: ", market_cap_USD)
        print("company_size", company_size)
        print("company complexity", complexity)
        print("share dilution", round(dilution, 4))
        print("revenue", revenue)
        print("inventory", inventory)
        print("receivables", receivables)
        print("company_type", company_type)
        print("Auditor", auditor)
        print()

    # Compute status
    status = get_status(fcff_delta, div_delta, liquidation_delta, country, region, company_size, company_type, dilution, complexity,
                        revenue, receivables, inventory, debug)

    if debug:
        print("FCFF values")
        print([round(x, 2) for x in fcff_values_list])
        print("\nFCFF values w/ Recession")
        print([round(x, 2) for x in fcff_recession_values_list])
        print("\n\nDiv values")
        print([round(x, 2) for x in div_values_list])
        print("\nDiv values w/ Recession")
        print([round(x, 2) for x in div_recession_values_list])

        print("\n\n\n")

        print("Price per Share", price_per_share)
        print("FCFF Result", ev_fcff)
        print("FCFF Deviation", fcff_delta)
        print("Dividends Result", ev_dividends)
        print("Dividends Deviation", div_delta)
        print("Status", status)


    # Add qualitative analysis
    if qualitative and doc is not None:

        l = []

        recent_docs = get_recent_docs(cik, doc["filing_date"])
        for d in recent_docs:

            print("##############")
            print(d["form_type"], d["filing_date"], d["_id"])
            print("##############\n")

            if not mongodb.check_document_exists("parsed_documents", d["_id"]):
                parse_document(d)

            parsed_doc = mongodb.get_document("parsed_documents", d["_id"])

            if not mongodb.check_document_exists("items_summary", d["_id"]):
                sections_summary(parsed_doc)

            summary_doc = mongodb.get_document("items_summary", d["_id"])

            for k, v in summary_doc.items():
                if isinstance(v, dict):

                    print(f"=== {k} ===")

                    for info in v["summary"]:

                        print(info)

                        if v["links"] is None:
                            v["links"] = [{"title":"", "link":""}]

                        for link in v["links"]:

                            l.append({
                                "ticker": ticker,
                                "created_at": datetime.datetime.now().date(),
                                "form_type": d["form_type"],
                                "filing_date": d["filing_date"],
                                "url": d["_id"],
                                "section": k,
                                "information": info,
                                "section_link": link["link"],
                                "section_link_title": link["title"]
                            })

                    print()

            print("\n")

        summary_df = pd.DataFrame(l)

    else:
        summary_df = None

    #### DFs for BI ####
    company_info_df = pd.DataFrame([
        {
            "ticker": ticker,
            "company_name": company_name,
            "country": country,
            "region": region,
            "industry": industry,
            "financial_currency": db_financial_curr,
            "quote_currency": db_curr,
            "price_per_share": price_per_share,
            "number_of_shares": mr_shares * 1000,
            "market_cap": equity_mkt * 1000,
            "options_value": mr_sbc * 1000,
            "equity": mr_equity * 1000,
            "equity_adj": mr_equity_adj * 1000,
            "cash": mr_cash * 1000,
            "cash_and_securities": mr_cash_and_securities * 1000,
            "debt": mr_debt * 1000,
            "debt_adj": mr_debt_adj * 1000,
            "minority_interest": mr_original_min_interest * 1000,
            "minority_interest_adj": mr_minority_interest * 1000,
            "survival_rate": survival_prob,
            "liquidation_value": liquidation_value * 1000,
            "created_at": datetime.datetime.now().date()
        }
    ])

    financial_data = []
    for i in range(years):
        financial_data.append({
            "ticker": ticker,
            "created_at": datetime.datetime.now().date(),
            "scenario": "actual",
            "year": initial_year + i,
            "revenue": revenue[i] * 1000,
            "ebit": ebit[i] * 1000,
            "ebit_adj": ebit_adj[i] * 1000,
            "ebit_after_tax": ebit_after_tax[i] * 1000,
            "reinvestment": reinvestment[i] * 1000,
            "fcff": (ebit_after_tax[i] - reinvestment[i]) * 1000,
            "eps": eps[i],
            "eps_adj": eps_adj[i],
            "dividends_per_share": dividends[i],
            "dividends": dividends[i] * mr_shares * 1000
        })
    for scenario in dict_values_for_bi:
        scenario_values = dict_values_for_bi[scenario]
        for i in range(11):
            financial_data.append({
            "ticker": ticker,
            "created_at": datetime.datetime.now().date(),
            "scenario": scenario,
            "year": final_year + i + 1,
            "revenue": scenario_values["revenue"][i] * 1000,
            "ebit": scenario_values["ebit"][i] * 1000,
            "ebit_after_tax": scenario_values["ebit_after_tax"][i] * 1000,
            "reinvestment": scenario_values["reinvestment"][i] * 1000,
            "fcff": scenario_values["FCFF"][i] * 1000,
            "cost_of_capital": scenario_values["cost_of_capital"][i],
            "pv_of_fcff": scenario_values["pv_of_FCFF"][i] * 1000,
            "eps": scenario_values["eps"][i],
            "dividends_per_share": scenario_values["dividends"][i],
            "dividends": scenario_values["dividends"][i] * mr_shares * 1000,
            "cost_of_equity": scenario_values["cost_of_equity"][i],
            "pv_of_dividends_per_share": scenario_values["pv_of_dividends"][i],
        })
    financial_data_df = pd.DataFrame(financial_data)

    if geo_segments_df is None or geo_segments_df.empty:
        geo_segments_df = pd.DataFrame([
            {"value": 1,
             "country": country,
             "country_area": country,
             "region": region
             }
        ])

    return price_per_share, fcff_value, div_value, fcff_delta, div_delta, liquidation_per_share, liquidation_delta, \
           status, company_info_df, financial_data_df, geo_segments_df, summary_df

if __name__ == '__main__':
    cik = cik_from_ticker("BLDR")
    if cik != -1:
        valuation(cik, debug=True, years=6)

from typing import Any, List
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.document_loaders.unstructured import UnstructuredBaseLoader


class UnstructuredStringLoader(UnstructuredBaseLoader):
    """
    Uses unstructured to load a string
    Source of the string, for metadata purposes, can be passed in by the caller
    """

    def __init__(
        self, content: str, source: str = None, mode: str = "single",
        **unstructured_kwargs: Any
    ):
        self.content = content
        self.source = source
        super().__init__(mode=mode, **unstructured_kwargs)

    def _get_elements(self) -> List:
        from unstructured.partition.text import partition_text

        return partition_text(text=self.content, **self.unstructured_kwargs)

    def _get_metadata(self) -> dict:
        return {"source": self.source} if self.source else {}


def split_text_in_chunks(text, chunk_size=20000):
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=100)
    chunks = text_splitter.split_text(text)
    return chunks


def split_doc_in_chunks(doc, chunk_size=20000):
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=100)
    chunks = text_splitter.split_documents(doc)
    return chunks


def doc_summary(docs):
    print(f'You have {len(docs)} document(s)')

    num_words = sum([len(doc.page_content.split(' ')) for doc in docs])

    print(f'You have roughly {num_words} words in your docs')
    print()
    print(f'Preview: \n{docs[0].page_content.split(". ")[0]}')

# if __name__ == '__main__':
#     import mongodb
#     from edgar_utils import company_from_cik
#
#     parser = ConfigParser()
#     _ = parser.read(os.path.join("credentials.cfg"))
#     model = "gpt-3.5-turbo"
#     llm = ChatOpenAI(model_name=model, openai_api_key=parser.get("open_ai", "api_key"))
#
#     url = 'https://www.sec.gov/Archives/edgar/data/8818/000000881823000002/avy-20221231.htm'
#     doc = mongodb.get_document("documents", url)
#     parsed_doc = mongodb.get_document("parsed_documents", url)
#     company = company_from_cik(doc["cik"])
#
#     result = {"_id": doc["_id"],
#               "name": company["name"],
#               "ticker": company["ticker"],
#               "form_type": doc["form_type"],
#               "filing_date": doc["filing_date"]}
#
#     for section_title, section_text in parsed_doc.items():
#
#         # if no section to summarize, skip
#         if section_title == "_id" or len(section_text) == 0:
#             continue
#         # chunks = split_in_chunks(section_text)
#
#         string_loader = UnstructuredStringLoader(section_text)
#         doc = string_loader.load()
#         doc_summary(doc)
#         docs = split_doc_in_chunks(doc)
#         doc_summary(docs)
#
#         chain = load_summarize_chain(llm, chain_type="refine", verbose=True)
#         res = chain.run(docs)
#         print(res)

import copy
import json
import os
import re
import time
import traceback
from datetime import datetime

import Levenshtein as Levenshtein
from bs4 import BeautifulSoup, NavigableString
from unidecode import unidecode

import mongodb
from edgar_utils import company_from_cik, AAPL_CIK, download_submissions_documents, download_all_cik_submissions
import string

from openai_interface import summarize_section

list_10k_items = [
    "business",
    "risk factors",
    "unresolved staff comments",
    "properties",
    "legal proceedings",
    "mine safety disclosures",
    "market for registrants common equity, related stockholder matters and issuer purchases of equity securities",
    "reserved",
    "managements discussion and analysis of financial condition and results of operations",
    "quantitative and qualitative disclosures about market risk",
    "financial statements and supplementary data",
    "changes in and disagreements with accountants on accounting and financial disclosure",
    "controls and procedures",
    "other information",
    "disclosure regarding foreign jurisdictions that prevent inspection",
    "directors, executive officers, and corporate governance",
    "executive compensation",
    "security ownership of certain beneficial owners and management and related stockholder matters",
    "certain relationships and related transactions, and director independence",
    "principal accountant fees and services",
    "exhibits and financial statement schedules",
]
default_10k_sections = {
     1: {'item': 'item 1', 'title': ['business']},
     2: {'item': 'item 1a', 'title': ['risk factor']},
     3: {'item': 'item 1b', 'title': ['unresolved staff']},
     4: {'item': 'item 2', 'title': ['propert']},
     5: {'item': 'item 3', 'title': ['legal proceeding']},
     6: {'item': 'item 4', 'title': ['mine safety disclosure', 'submission of matters to a vote of security holders']},
     7: {'item': 'item 5', 'title': ["market for registrant's common equity, related stockholder matters and issuer purchases of equity securities"]},
     8: {'item': 'item 6', 'title': ['reserved', 'selected financial data']},
     9: {'item': 'item 7', 'title': ["management's discussion and analysis of financial condition and results of operations"]},
     10: {'item': 'item 7a', 'title': ['quantitative and qualitative disclosures about market risk']},
     11: {'item': 'item 8', 'title': ['financial statements and supplementary data']},
     12: {'item': 'item 9', 'title': ['changes in and disagreements with accountants on accounting and financial disclosure']},
     13: {'item': 'item 9a', 'title': ['controls and procedures']},
     14: {'item': 'item 9b', 'title': ['other information']},
     15: {'item': 'item 9c', 'title': ['Disclosure Regarding Foreign Jurisdictions that Prevent Inspections']},
     16: {'item': 'item 10', 'title': ['directors, executive officers and corporate governance','directors and executive officers of the registrant']},
     17: {'item': 'item 11', 'title': ['executive compensation']},
     18: {'item': 'item 12', 'title': ['security ownership of certain beneficial owners and management and related stockholder matters']},
     19: {'item': 'item 13', 'title': ['certain relationships and related transactions']},
     20: {'item': 'item 14', 'title': ['principal accountant fees and services']},
     21: {'item': 'item 15', 'title': ['exhibits, financial statement schedules', 'exhibits and financial statement schedules']},
}
list_10q_items = [
    "financial statement",
    "risk factor",
    "legal proceeding",
    "mine safety disclosure",
    "managements discussion and analysis of financial condition and results of operations",
    "quantitative and qualitative disclosures about market risk",
    "controls and procedures",
    "other information",
    "unregistered sales of equity securities and use of proceeds",
    "defaults upon senior securities",
    "exhibits"
]
default_10q_sections = {
    1: {'item': 'item 1', 'title': ['financial statement']},
    2: {'item': 'item 2', 'title': ["management's discussion and analysis of financial condition and results of operations"]},
    3: {'item': 'item 3', 'title': ['quantitative and qualitative disclosures about market risk']},
    4: {'item': 'item 4', 'title': ['controls and procedures']},
    5: {'item': 'item 1', 'title': ['legal proceeding']},
    6: {'item': 'item 1a', 'title': ['risk factor']},
    7: {'item': 'item 2', 'title': ["unregistered sales of equity securities and use of proceeds"]},
    8: {'item': 'item 3', 'title': ["defaults upon senior securities"]},
    9: {'item': 'item 4', 'title': ["mine safety disclosure"]},
    10: {'item': 'item 5', 'title': ["other information"]},
    11: {'item': 'item 6', 'title': ["exhibits"]},
}
default_8k_sections = {
    1: {'item': 'item 1.01', 'title': ["entry into a material definitive agreement"]},
    2: {'item': 'item 1.02', 'title': ["termination of a material definitive agreement"]},
    3: {'item': 'item 1.03', 'title': ["bankruptcy or receivership"]},
    4: {'item': 'item 1.04', 'title': ["mine safety"]},
    5: {'item': 'item 2.01', 'title': ["completion of acquisition or disposition of asset"]},
    6: {'item': 'item 2.02', 'title': ['results of operations and financial condition']},
    7: {'item': 'item 2.03', 'title': ["creation of a direct financial obligation"]},
    8: {'item': 'item 2.04', 'title': ["triggering events that accelerate or increase a direct financial obligation"]},
    9: {'item': 'item 2.05', 'title': ["costs associated with exit or disposal activities"]},
    10: {'item': 'item 2.06', 'title': ["material impairments"]},
    11: {'item': 'item 3.01', 'title': ["notice of delisting or failure to satisfy a continued listing"]},
    12: {'item': 'item 3.02', 'title': ["unregistered sales of equity securities"]},
    13: {'item': 'item 3.03', 'title': ["material modification to rights of security holders"]},
    14: {'item': 'item 4.01', 'title': ["changes in registrant's certifying accountant"]},
    15: {'item': 'item 4.02', 'title': ["non-reliance on previously issued financial statements"]},
    16: {'item': 'item 5.01', 'title': ["changes in control of registrant"]},
    17: {'item': 'item 5.02', 'title': ['departure of directors or certain officers']},
    18: {'item': 'item 5.03', 'title': ['amendments to articles of incorporation or bylaws']},
    19: {'item': 'item 5.04', 'title': ["temporary suspension of trading under registrant"]},
    20: {'item': 'item 5.05', 'title': ["amendment to registrant's code of ethics"]},
    21: {'item': 'item 5.06', 'title': ["change in shell company status"]},
    22: {'item': 'item 5.07', 'title': ['submission of matters to a vote of security holders']},
    23: {'item': 'item 5.08', 'title': ["shareholder director nominations"]},
    24: {'item': 'item 6.01', 'title': ["abs informational and computational material"]},
    25: {'item': 'item 6.02', 'title': ['change of servicer or trustee']},
    26: {'item': 'item 6.03', 'title': ['change in credit enhancement or other external support']},
    27: {'item': 'item 6.04', 'title': ["failure to make a required distribution"]},
    28: {'item': 'item 6.05', 'title': ["securities act updating disclosure"]},
    29: {'item': 'item 7.01', 'title': ["regulation fd disclosure"]},
    30: {'item': 'item 8.01', 'title': ['other events']},
    31: {'item': 'item 9.01', 'title': ["financial statements and exhibits"]},
}


def string_similarity_percentage(string1, string2):
    """
    Compute the leveshtein distance between the two strings and return the percentage simialrity.
    :param string1:
    :param string2:
    :return: a float representing the percentage of similarity
    """
    distance = Levenshtein.distance(string1.replace(" ", ""), string2.replace(" ", ""))
    max_length = max(len(string1), len(string2))
    similarity_percentage = (1 - (distance / max_length)) * 100
    return similarity_percentage


def clean_section_title(title):
    """
    Clean the title string removing special words and punctuation that makes harder to recognize.
    :param title: a string
    :return: a cleaned string, lowercase
    """
    # lower case
    title = title.lower()
    # remove special html characters
    title = unidecode(title)
    # remove item
    title = title.replace("item ", "")
    # remove '1.' etc
    for idx in range(20, 0, -1):
        for let in ['', 'a', 'b', 'c']:
            title = title.replace(f"{idx}{let}.", "")
    for idx in range(10, 0, -1):
        title = title.replace(f"f-{idx}", "")
    # remove parentesis and strip
    title = re.sub(r'\([^)]*\)', '', title).strip(string.punctuation + string.whitespace)
    return title


def is_title_valid(text):
    """
    Check if title is valid, meaning;
    it does not starts with key words like: item, part, signature, page or is digit and has less than 2 chars
    :param text: a string representing the title
    :return: True if all conditions are satisfied else False
    """
    valid = not (
            text.startswith("item") or
            text.startswith("part") or
            text.startswith("signature") or
            text.startswith("page") or
            text.isdigit() or
            len(text) <= 2)
    return valid


def parse_segments():
    done_ciks = []

    docs = mongodb.get_collection_documents("documents")
    for doc in docs:

        if "aapl" not in doc["_id"]:
            continue

        if doc["form_type"] != "10-K":
            continue

        cik = doc["_id"].split("data/")[1].split("/")[0]

        if cik in done_ciks:
            continue
        else:
            done_ciks.append(cik)

        print(f"######## {doc['_id']} ##########\n")

        page = doc["html"]
        soap = BeautifulSoup(page, features="html.parser")

        ix_resources = soap.find("ix:resources")
        contexts = ix_resources.findAll("xbrli:context")

        axis = [
            "srt:ProductOrServiceAxis",
            "us-gaap:StatementBusinessSegmentsAxis",
            "srt:ConsolidationItemsAxis",
            "srt:StatementGeographicalAxis",
        ]

        for c in contexts:

            context_id = c["id"]
            s = c.find("xbrli:segment")

            if s is not None:

                members = s.find_all("xbrldi:explicitmember")
                if len(members) == 0:
                    continue

                include = True
                for m in members:
                    if m["dimension"] not in axis:
                        include = False
                        break
                if not include:
                    continue

                try:
                    period = c.find("xbrli:enddate").text
                except:
                    period = c.find("xbrli:instant").text
                period = datetime.strptime(period, "%Y-%m-%d").date()

                # dimension = "+".join([x["dimension"] for x in members])
                # value = "+".join([x.text for x in members])

                # if dimension not in result_dict:
                #     result_dict[dimension] = {}
                #
                # if value not in result_dict[dimension] or period > result_dict[dimension][value]["period"]:
                #         result_dict[dimension][value] = {"period":period,"id":context_id}

                element = soap.find("ix:nonfraction", attrs={"contextref": context_id})
                if element is None:
                    continue

                segment = {}
                for m in members:
                    segment[m["dimension"]] = m.text

                print(f"{period} - {segment} => {element.text} ({element['name']})")

        return


def find_possible_axis():
    axis = []

    docs = mongodb.get_collection_documents("documents")
    for doc in docs:

        page = doc["html"]
        soap = BeautifulSoup(page, features="html.parser")

        ix_resources = soap.find("ix:resources")

        if ix_resources is None:
            continue

        contexts = ix_resources.findAll("xbrli:context")

        for c in contexts:
            s = c.find("xbrli:segment")
            if s is not None:
                try:
                    ax = [x["dimension"] for x in s.children]
                    for a in ax:
                        if a not in axis:
                            print(a)
                            axis.append(a)
                except:
                    pass


def identify_table_of_contents(soup, list_items):
    """
    Given a soup object and a list of item, this method look for a table of contents.
    :param soup: soup object of the document
    :param list_items: an array of strings related to sections titles.
    :return: the table of contents PageElement object or None if not found.
    """
    if list_items is None:
        return None
    max_table = 0
    chosen_table = None
    tables = soup.body.findAll("table")
    for t in tables:
        count = 0
        for s in list_items:
            r = t.find(string=re.compile(f'{s}', re.IGNORECASE))
            if r is not None:
                count += 1

        if count > max_table:
            chosen_table = t
            max_table = count
    if max_table > 3:
        return chosen_table
    return None


def get_sections_using_hrefs(soup, table_of_contents):
    """
    Scan the table_of_contents and identify all hrefs, if present.
    The method create a dictionary of sections by finding tag elements referenced inside soup with the specific hrefs.
    :param soup: soup object of the document.
    :param table_of_contents:
    :return: a dictionary with the following structure:
        {1:
            {
                'start_el': tag element where the section starts,
                'idx': an integer index of start element inside soup, used for ordering
                'title': a string representing the section title,
                'title_candidates': a list of title candidates. If there is a single candidate that becomes the title
                'end_el': tag element where the section ends,
                'text': the text of the section
            },
        ...
        }
        Section are ordered based on chid['idx'] value
    :param soup:
    :return: section dictionary
    """
    all_elements = soup.find_all()
    hrefs = {}
    sections = {}
    for tr in table_of_contents.findAll("tr"):
        try:
            aa = tr.find_all("a")
            tr_hrefs = [a['href'][1:] for a in aa]
        except Exception as e:
            continue

        for el in tr.children:
            text = el.text
            text = clean_section_title(text)
            if is_title_valid(text):
                for tr_href in tr_hrefs:
                    if tr_href not in hrefs:
                        h_tag = soup.find(id=tr_href)
                        if h_tag is None:
                            h_tag = soup.find(attrs={"name": tr_href})
                        if h_tag:
                            hrefs[tr_href] = {
                                'start_el': h_tag,
                                "link": tr_href,
                                'idx': all_elements.index(h_tag),
                                'title': None,
                                'title_candidates': {text}}
                    else:
                        hrefs[tr_href]['title_candidates'].add(text)
            else:
                continue

    for h in hrefs:
        hrefs[h]['title_candidates'] = list(hrefs[h]['title_candidates'])
        if len(hrefs[h]['title_candidates']) == 1:
            hrefs[h]['title'] = hrefs[h]['title_candidates'][0]
        else:
            hrefs[h]['title'] = "+++".join(hrefs[h]['title_candidates'])

    temp_s = sorted(hrefs.items(), key=lambda x: x[1]["idx"])
    for i, s in enumerate(temp_s):
        sections[i + 1] = s[1]
        if i > 0:
            sections[i]["end_el"] = sections[i + 1]["start_el"]

    sections = get_sections_text_with_hrefs(soup, sections)
    return sections


def select_best_match(string_to_match, matches, start_index):
    """
    Identifies the best match, in terms of similarity distance between a string_to_match and a list of matches.
    start_index is used to avoid cases where the string_to_match is matched with the first occurence in matches.
    :param string_to_match: a string
    :param matches: a list of regular expresion matches
    :param start_index: a integer representing the index to start from
    :return: a regualr expression match with highest simialrity
    """
    match = None

    if start_index == 0:
        del matches[0]

    if len(matches) == 1:
        match = matches[0]
        if matches[0].start() > start_index:
            match = matches[0]
    elif len(matches) > 1:
        max_similarity = -1
        for i, m in enumerate(matches):
            if m.start() > start_index:
                sim = string_similarity_percentage(string_to_match, m.group().lower().replace("\n", " "))
                if sim > max_similarity:
                    max_similarity = sim
                    match = m
    return match


def get_sections_using_strings(soup, table_of_contents, default_sections):
    """
        Scan the table_of_contents and identify possible section text using strings that match default_sections.
        Retrieve sections strings in soup.body.text.
        :param soup: the soup object
        :param table_of_contents: a PageElement from soup that represent the table of contents
        :param default_sections: a dictionary that contains prefilled data about default sections that could be found in the document
        :return: a dictionary with the following structure, representing the sections:
            {1:
                {
                    'start_index': the start index of the section inside soup.body.text
                    'end_index': the start index of the section inside soup.body.text,
                    'title': a string representing the section title,
                    'end_el': tag element where the section ends
                },
            ...
            }
            Section are ordered based on chid['idx'] value
        """

    # Clean soup.body.text removing consecutive \n and spaces
    body_text = unidecode(soup.body.get_text(separator=" "))
    body_text = re.sub('\n', ' ', body_text)
    body_text = re.sub(' +', ' ', body_text)

    # If there is a table_of_contents look for items strings a check for their validity
    sections = {}
    if table_of_contents:
        num_section = 1
        for tr in table_of_contents.findAll("tr"):
            section = {}
            for el in tr.children:
                text = el.text
                item = unidecode(text.lower()).replace("\n", " ").strip(string.punctuation + string.whitespace)
                # print(text)
                # input("NEXt")
                # remove special html characters
                item = item
                if 'item' in item:
                    section["item"] = item

                text = clean_section_title(text)
                if 'item' in section and is_title_valid(text):
                    section['title'] = text
                    sections[num_section] = section
                    num_section += 1

    # Different behaviour if there is a table_of_contents and sections is already populated.
    if len(sections) == 0:
        # Here we didn't find any usable table_of_contents sections, then we use a prefilled default_sections dictionary
        sections = copy.deepcopy(default_sections)
        start_index = 1
    else:
        # Here we skip first occurrence in text since it also present in table_of_contents
        start_index = 0

    # Loop through all sections to identify a possible item and title for a section.
    # If multiple values are found we select best match based on string similarity.
    for si in sections:
        s = sections[si]
        if 'item' in s:
            match = None
            if isinstance(s['title'], list):
                for t in s['title']:
                    matches = list(re.finditer(fr"{s['item']}. *{t}", body_text, re.IGNORECASE + re.DOTALL))
                    if matches:
                        match = select_best_match(f"{s['item']} {t}", matches, start_index)
                        break
            else:
                matches = list(re.finditer(fr"{s['item']}. *{s['title']}", body_text, re.IGNORECASE + re.DOTALL))
                if matches:
                    match = select_best_match(f"{s['item']} {s['title']}", matches, start_index)

            if match is None:
                matches = list(re.finditer(fr"{s['item']}", body_text, re.IGNORECASE + re.DOTALL))
                if matches:
                    match = select_best_match(f"{s['item']}", matches, start_index)

            if match:
                s['title'] = match.group()
                s["start_index"] = match.start()
                start_index = match.start()
            else:
                s['remove'] = True

    sections_temp = {}
    for si in sections:
        if "remove" not in sections[si]:
            sections_temp[si] = sections[si]

    # Eventually we populate each section in the dictionary with its text taken from body_text
    temp_s = sorted(sections_temp.items(), key=lambda x: x[1]["start_index"])
    sections = {}
    last_section = 0
    for i, s in enumerate(temp_s):
        sections[i + 1] = s[1]
        if i > 0:
            sections[i]["end_index"] = sections[i + 1]["start_index"]
            sections[i]["text"] = body_text[sections[i]["start_index"]:sections[i]["end_index"]]
        last_section = i + 1
    if last_section > 0:
        sections[last_section]["end_index"] = -1
        sections[last_section]["text"] = body_text[sections[last_section]["start_index"]:sections[last_section]["end_index"]]

    return sections


def get_sections_text_with_hrefs(soup, sections):
    """
    This method try to retrieve text from soup object related to a document and its sections
    :param soup: a soup object
    :param sections: a dictionary containing data about sections
    :return:
    """
    next_section = 1
    current_section = None
    text = ""
    last_was_new_line = False
    for el in soup.body.descendants:
        if next_section in sections and el == sections[next_section]['start_el']:
            if current_section is not None:
                sections[current_section]["text"] = text
                text = ""
                last_was_new_line = False

            current_section = next_section
            next_section += 1

        if current_section is not None and isinstance(el, NavigableString):
            if last_was_new_line and el.text == "\n":
                continue
            elif el.text == "\n":
                last_was_new_line = True
            else:
                last_was_new_line = False
            found_text = unidecode(el.get_text(separator=" "))
            if sections[current_section]['title'] is None:
                if found_text in sections[current_section]['title_candidates']:
                    print(f"{bcolors.OKCYAN}"
                          f'new title for {current_section}: {found_text} in {sections[current_section]["title_candidates"]}'
                          f"{bcolors.ENDC}")
                    sections[current_section]['title'] = found_text
            if len(text) > 0 and text[-1] != " " and len(found_text) > 0 and found_text[0] != " ":
                text += "\n"
            text += found_text.replace('\n', ' ')

    if current_section is not None:
        sections[current_section]["text"] = text

    return sections


def parse_document(doc):
    """
    Take a document, SEC filing, parse the content and retrieve the sections.
    Save the result on mongoDB under parsed_documents collection.
    :param doc: document from "documents" collection of mongoDB
    :return:
    """

    url = doc["_id"]
    form_type = doc["form_type"]
    filing_date = doc["filing_date"]
    sections = {}
    cik = doc["cik"]
    html = doc["html"]

    # Supported form type are 10-K, 10-K/A, 10-Q, 10-Q/A, 8-K
    if form_type in ["10-K", "10-K/A"]:
        include_forms = ["10-K", "10-K/A"]
        list_items = list_10k_items
        default_sections = default_10k_sections
    elif form_type == "10-Q":
        include_forms = ["10-Q"]
        list_items = list_10q_items
        default_sections = default_10q_sections
    elif form_type == "8-K":
        include_forms = ["8-K"]
        list_items = None
        default_sections = default_8k_sections
    else:
        print(f"return because form_type {form_type} is not valid")
        return

    if form_type not in include_forms:
        print(f"return because form_type != {form_type}")
        return

    company_info = company_from_cik(cik)

    # no cik in cik_map
    if company_info is None:
        print("return because company info None")
        return

    print(f"form type: \t\t{form_type}")
    print(company_info)

    soup = BeautifulSoup(html, features="html.parser")

    if soup.body is None:
        print("return because soup.body None")
        return

    table_of_contents = identify_table_of_contents(soup, list_items)

    if table_of_contents:
        sections = get_sections_using_hrefs(soup, table_of_contents)

    if len(sections) == 0:
        sections = get_sections_using_strings(soup, table_of_contents, default_sections)

    result = {"_id": url, "cik": cik, "form_type":form_type, "filing_date": filing_date, "sections":{}}

    for s in sections:
        section = sections[s]
        if 'text' in section:
            text = section['text']
            text = re.sub('\n', ' ', text)
            text = re.sub(' +', ' ', text)

            result["sections"][section["title"]] = {"text":text, "link":section["link"] if "link" in section else None}

    try:
        mongodb.upsert_document("parsed_documents", result)
    except:
        traceback.print_exc()
        print(result.keys())
        print(result["sections"].keys())


def find_auditor(doc):
    try:
        soup = BeautifulSoup(doc["html"], features="html.parser")

        # auditor_start_string = 'Report of Independent Registered Public Accounting Firm'.lower()

        # auditor_string = ""
        body = unidecode(soup.body.get_text(separator=" "))
        body = re.sub('\n', ' ', body)
        body = re.sub(' +', ' ', body)

        start_sig = 0
        while start_sig != -1:
            start_sig = body.find('s/', start_sig+1)
            auditor_candidate = body[start_sig: start_sig+200]

            # print(auditor_candidate)
            if 'auditor since' in auditor_candidate.lower():
                pattern = r"s/.+auditor since.*?\d{4}"

                try:
                    match = re.findall(pattern, auditor_candidate)[0]
                    return match.replace("s/", "").strip()
                except:
                    pass
    except Exception as e:
        print(e)
        print("NO AUDITOR FOUND")
        return ""

    # print(auditor_string)

    # if auditor_start_string in body.lower():
    #     start_sig = 0
    #     while start_sig != -1:
    #         start_sig = body.lower().find(auditor_start_string, start_sig)
    #         if start_sig != -1:
    #             start_sig = body.find('s/', start_sig)
    #             end_sig = body.find('.', start_sig)
    #             auditor_candidate = body[start_sig: end_sig]
    #             if 'auditor since' in auditor_candidate.lower():
    #                 auditor_string += body[start_sig: end_sig] + "\n"
    # if auditor_string == "":
    #     auditor_start_string = 'auditor'
    #     if auditor_start_string in body.lower():
    #         start_sig = 0
    #         while start_sig != -1:
    #             start_sig = body.lower().find(auditor_start_string, start_sig + len(auditor_start_string))
    #             if start_sig != -1:
    #                 auditor_string += body[start_sig - 100: start_sig + 100] + "\n"
    #                 print(auditor_string)

    # return auditor_string


class bcolors:
    HEADER = '\033[95m'
    OKBLUE = '\033[94m'
    OKCYAN = '\033[96m'
    OKGREEN = '\033[92m'
    WARNING = '\033[93m'
    FAIL = '\033[91m'
    ENDC = '\033[0m'
    BOLD = '\033[1m'
    UNDERLINE = '\033[4m'


def test():
    test_docs = {
        "docs_with_table_of_contents_and_hrefs": [
            "https://www.sec.gov/Archives/edgar/data/12040/000117494723000017/form10k-29127_bdl.htm",
        ],
        "docs_with_table_of_contents_no_hrefs": [
            "https://www.sec.gov/Archives/edgar/data/10329/000143774923001642/bset20230109_10k.htm",
        ],
        "docs_without_table_of_contents": [
            "https://www.sec.gov/Archives/edgar/data/315374/000155837023000097/hurc-20221031x10k.htm",
            "https://www.sec.gov/Archives/edgar/data/97476/000009747623000007/txn-20221231.htm",
            "https://www.sec.gov/Archives/edgar/data/315213/000031521323000016/rhi-20221231.htm" # item from 10 to 14 are missing in filing
        ]
    }

import re
import sys
import time
from datetime import datetime
from statistics import median

import requests
from bs4 import BeautifulSoup
from dateutil.relativedelta import relativedelta
from forex_python.converter import CurrencyRates, RatesNotAvailableError
from unidecode import unidecode
from urllib3.exceptions import ProtocolError
import numpy as np

import mongodb
from investing_com import get_10y_bond_yield
from postgresql import get_df_from_table
from yahoo_finance import get_current_price_from_yahoo
import math
import pandas as pd

r_and_d_amortization = {
    'Advertising': 2,
    'Aerospace/Defense': 10,
    'Air Transport': 10,
    'Apparel': 3,
    'Auto & Truck': 10,
    'Auto Parts': 5,
    'Bank (Money Center)': 2,
    'Banks (Regional)': 2,
    'Beverage (Alcoholic)': 3,
    'Beverage (Soft)': 3,
    "Broadcasting": 10,
    "Brokerage & Investment Banking": 3,
    'Building Materials': 5,
    'Construction Supplies': 5,
    "Business & Consumer Services": 5,
    'Cable TV': 10,
    'Chemical (Basic)': 10,
    'Chemical (Diversified)': 10,
    'Chemical (Specialty)': 10,
    'Coal & Related Energy': 5,
    'Computer & Peripherals': 5,
    'Computer Services': 3,
    'Diversified': 5,
    "Drugs (Biotechnology)": 10,
    "Drugs (Pharmaceutical)": 10,
    'Education': 3,
    'Electrical Equipment': 10,
    'Electronics (Consumer & Office)': 5,
    'Electronics (General)': 5,
    "Engineering/Construction": 10,
    'Entertainment': 3,
    'Environmental & Waste Services': 5,
    "Farming/Agriculture": 10,
    'Financial Svcs. (Non-bank & Insurance)': 2,
    'Food Processing': 3,
    'Food Wholesalers': 3,
    'Furn/Home Furnishings': 3,
    "Green & Renewable Energy": 10,
    "Healthcare Products": 5,
    "Healthcare Support Services": 3,
    'Heathcare Information and Technology': 3,
    'Homebuilding': 5,
    "Hospitals/Healthcare Facilities": 10,
    'Hotel/Gaming': 3,
    'Household Products': 3,
    "Information Services": 3,
    'Insurance (General)': 3,
    'Insurance (Life)': 3,
    'Insurance (Prop/Cas.)': 3,
    'Investments & Asset Management': 3,
    'Machinery': 10,
    'Metals & Mining': 5,
    'Office Equipment & Services': 5,
    "Oil/Gas (Integrated)": 10,
    "Oil/Gas (Production and Exploration)": 10,
    "Oil/Gas Distribution": 10,
    "Oilfield Svcs/Equip.": 5,
    'Packaging & Container': 5,
    'Paper/Forest Products': 10,
    "Power": 10,
    "Precious Metals": 5,
    'Petroleum (Integrated)': 5,
    'Petroleum (Producing)': 5,
    'Precision Instrument': 5,
    'Publishing & Newspapers': 3,
    'R.E.I.T.': 3,
    "Real Estate (Development)": 5,
    "Real Estate (General/Diversified)": 5,
    "Real Estate (Operations & Services)": 5,
    'Recreation': 5,
    'Reinsurance': 3,
    'Restaurant/Dining': 2,
    'Retail (Special Lines)': 2,
    'Retail (Building Supply)': 2,
    'Retail (General)': 2,
    "Retail (Automotive)": 2,
    "Retail (Distributors)": 2,
    "Retail (Grocery and Food)": 2,
    "Retail (Online)": 2,
    "Rubber& Tires": 5,
    'Semiconductor': 5,
    'Semiconductor Equip': 5,
    'Shipbuilding & Marine': 10,
    'Shoe': 3,
    "Software (Entertainment)": 3,
    "Software (Internet)": 3,
    "Software (System & Application)": 3,
    'Steel': 5,
    "Telecom (Wireless)": 5,
    'Telecom. Equipment': 10,
    'Telecom. Services': 5,
    'Tobacco': 5,
    'Toiletries/Cosmetics': 3,
    'Transportation': 5,
    'Transportation (Railroads)': 5,
    'Trucking': 5,
    'Utility (General)': 10,
    'Utility (Water)': 10

}
industry_complexity = {
    'Advertising': 1,
    'Apparel': 1,
    'Auto & Truck': 1,
    'Auto Parts': 1,
    'Beverage (Alcoholic)': 1,
    'Beverage (Soft)': 1,
    'Building Materials': 1,
    'Construction Supplies': 1,
    'Food Wholesalers': 1,
    'Furn/Home Furnishings': 1,
    'Household Products': 1,
    'Retail (Building Supply)': 1,
    'Retail (General)': 1,
    "Retail (Automotive)": 1,
    "Retail (Distributors)": 1,
    "Retail (Grocery and Food)": 1,
    'Tobacco': 1,
    'Toiletries/Cosmetics': 1,
    'Restaurant/Dining': 1,
    "Rubber& Tires": 1,
    'Shoe': 1,

    "Business & Consumer Services": 2,
    'Cable TV': 2,
    'Air Transport': 2,
    'Coal & Related Energy': 2,
    'Computer & Peripherals': 2,
    'Computers/Peripherals': 2,
    'Education': 2,
    'Electrical Equipment': 2,
    'Entertainment': 2,
    'Homebuilding': 2,
    "Hospitals/Healthcare Facilities": 2,
    'Hotel/Gaming': 2,
    'Food Processing': 2,
    'Office Equipment & Services': 2,
    'Packaging & Container': 2,
    'Paper/Forest Products': 2,

    "Real Estate (Development)": 2,
    "Real Estate (General/Diversified)": 2,
    "Real Estate (Operations & Services)": 2,
    'Recreation': 2,
    'Retail (Special Lines)': 2,
    "Retail (Online)": 2,
    'Publishing & Newspapers': 2,
    'Steel': 2,
    'Electronics (Consumer & Office)': 2,
    'Electronics (General)': 2,
    'R.E.I.T.': 2,
    'Transportation': 2,
    'Transportation (Railroads)': 2,
    'Trucking': 2,

    "Broadcasting": 3,
    'Metals & Mining': 3,
    "Precious Metals": 3,
    'Aerospace/Defense': 3,
    'Chemical (Basic)': 3,
    'Chemical (Diversified)': 3,
    'Computer Services': 3,
    "Engineering/Construction": 3,
    "Farming/Agriculture": 3,
    'Heathcare Information and Technology': 3,
    "Information Services": 3,
    'Insurance (General)': 3,
    'Insurance (Life)': 3,
    'Insurance (Prop/Cas.)': 3,
    'Investments & Asset Management': 3,
    'Machinery': 3,
    'Utility (General)': 3,
    'Utility (Water)': 3,
    "Software (Entertainment)": 3,
    "Software (Internet)": 3,
    "Software (System & Application)": 3,
    'Reinsurance': 3,
    'Semiconductor': 3,
    "Power": 3,
    'Telecom. Services': 3,
    'Shipbuilding & Marine': 3,
    "Telecom (Wireless)": 3,
    'Telecom. Equipment': 3,
    "Healthcare Products": 3,
    "Healthcare Support Services": 3,


    'Bank (Money Center)': 4,
    'Banks (Regional)': 4,
    "Brokerage & Investment Banking": 4,
    'Financial Svcs. (Non-bank & Insurance)': 4,
    'Environmental & Waste Services': 4,
    "Green & Renewable Energy": 4,
    "Oil/Gas (Integrated)": 4,
    "Oil/Gas (Production and Exploration)": 4,
    "Oil/Gas Distribution": 4,
    "Oilfield Svcs/Equip.": 4,
    'Petroleum (Integrated)': 4,
    'Petroleum (Producing)': 4,
    'Precision Instrument': 4,
    'Semiconductor Equip': 4,


    'Chemical (Specialty)': 5,
    'Diversified': 5,
    "Drugs (Biotechnology)": 5,
    "Drugs (Pharmaceutical)": 5,

}
industry_cyclical = {
    'Advertising': True,
    'Apparel': False,
    'Auto & Truck': True,
    'Auto Parts': True,
    'Beverage (Alcoholic)': False,
    'Beverage (Soft)': False,
    'Building Materials': True,
    'Construction Supplies': True,
    'Food Wholesalers': False,
    'Furn/Home Furnishings': True,
    'Household Products': False,
    'Retail (Building Supply)': True,
    'Retail (General)': False,
    "Retail (Automotive)": True,
    "Retail (Distributors)": True,
    "Retail (Grocery and Food)": False,
    'Tobacco': False,
    'Toiletries/Cosmetics': False,
    'Restaurant/Dining': True,
    "Rubber& Tires": True,
    'Shoe': False,

    "Business & Consumer Services": True,
    'Cable TV': False,
    'Air Transport': True,
    'Coal & Related Energy': True,
    'Computer & Peripherals': True,
    'Computers/Peripherals': True,
    'Education': False,
    'Electrical Equipment': True,
    'Entertainment': True,
    'Homebuilding': True,
    "Hospitals/Healthcare Facilities": False,
    'Hotel/Gaming': True,
    'Food Processing': False,
    'Office Equipment & Services': True,
    'Packaging & Container': True,
    'Paper/Forest Products': True,

    "Real Estate (Development)": True,
    "Real Estate (General/Diversified)": True,
    "Real Estate (Operations & Services)": True,
    'Recreation': True,
    'Retail (Special Lines)': True,
    "Retail (Online)": True,
    'Publishing & Newspapers': False,
    'Steel': True,
    'Electronics (Consumer & Office)': True,
    'Electronics (General)': True,
    'R.E.I.T.': True,
    'Transportation': True,
    'Transportation (Railroads)': True,
    'Trucking': True,

    "Broadcasting": False,
    'Metals & Mining': True,
    "Precious Metals": True,
    'Aerospace/Defense': False,
    'Chemical (Basic)': True,
    'Chemical (Diversified)': True,
    'Computer Services': True,
    "Engineering/Construction": True,
    "Farming/Agriculture": False,
    'Heathcare Information and Technology': False,
    "Information Services": True,
    'Insurance (General)': False,
    'Insurance (Life)': False,
    'Insurance (Prop/Cas.)': False,
    'Investments & Asset Management': True,
    'Machinery': True,
    'Utility (General)': False,
    'Utility (Water)': False,
    "Software (Entertainment)": True,
    "Software (Internet)": True,
    "Software (System & Application)": True,
    'Reinsurance': True,
    'Semiconductor': True,
    "Power": False,
    'Telecom. Services': False,

    'Bank (Money Center)': False,
    'Banks (Regional)': False,
    "Brokerage & Investment Banking": True,
    'Financial Svcs. (Non-bank & Insurance)': True,
    'Environmental & Waste Services': False,
    "Green & Renewable Energy": False,
    "Healthcare Products": False,
    "Healthcare Support Services": False,
    "Oil/Gas (Integrated)": True,
    "Oil/Gas (Production and Exploration)": True,
    "Oil/Gas Distribution": True,
    "Oilfield Svcs/Equip.": True,
    'Petroleum (Integrated)': True,
    'Petroleum (Producing)': True,
    'Precision Instrument': True,
    'Semiconductor Equip': True,
    'Shipbuilding & Marine': True,
    "Telecom (Wireless)": False,
    'Telecom. Equipment': False,

    'Chemical (Specialty)': False,
    'Diversified': False,
    "Drugs (Biotechnology)": False,
    "Drugs (Pharmaceutical)": False,

}

def company_complexity(doc, industry, company_size):
    base = industry_complexity[industry]

    # no 10-K
    if doc is None:
        length_modifier = 1

    else:
        soup = BeautifulSoup(doc["html"], 'html.parser')
        body_text = unidecode(soup.body.get_text(separator=" "))
        body_text = re.sub('\n', ' ', body_text)
        body_text = re.sub(' +', ' ', body_text)
        length = len(body_text)

        if length > 1.4 * 10 ** 6:
            length_modifier = 4
        elif length > 1.1 * 10 ** 6:
            length_modifier = 3
        elif length > 8 * 10 ** 5:
            length_modifier = 2
        elif length > 5 * 10 ** 5:
            length_modifier = 1
        else:
            length_modifier = 0

    if company_size == "Large" or company_size == "Mega":
        company_size_modifier = 1
    else:
        company_size_modifier = 0

    return min(5, max(1, base + length_modifier + company_size_modifier))

def company_share_diluition(shares):

    first_idx = 0
    for i, s in enumerate(shares):
        if s > 0:
            first_idx = i
            break

    l = shares[first_idx:]
    years = len(l) - 1
    return (l[-1] / l[0]) ** (1 / years) - 1

def get_company_type(revenue_growth, mr_debt_adj, equity_mkt, liquidation_value, operating_margin_5y, industry):

    fast_grower = False
    stalward = False
    slow_grower = False
    declining = False
    turn_around = False
    asset_play = False
    cyclical = industry_cyclical[industry]

    avg_revenue_growth = np.mean(revenue_growth)
    if avg_revenue_growth > 0.15:
        fast_grower = True
    elif avg_revenue_growth > 0.07:
        stalward = True
    elif avg_revenue_growth > -0.02:
        slow_grower = True
    else:
        declining = True

    # high debt + liquidation > debt + low margins
    if mr_debt_adj > equity_mkt * 2 and liquidation_value > mr_debt_adj and operating_margin_5y < 0.05:
        turn_around = True

    if liquidation_value > equity_mkt:
        asset_play = True

    return {
        "fast_grower": fast_grower,
        "stalward": stalward,
        "slow_grower": slow_grower,
        "declining": declining,
        "turn_around": turn_around,
        "asset_play": asset_play,
        "cyclical": cyclical
    }

def convert_currencies(db_curr, db_financial_curr, currency=None, financial_currency=None):

    if (currency is not None and financial_currency is not None):
        financial_currency = financial_currency.replace("Currency in ", "")

    else:
        currency = db_curr
        financial_currency = db_financial_curr

    if (financial_currency == "GBP" and ("GBp" in currency or "0.01" in currency)) \
            or (financial_currency == "ZAR" and ("ZAC" in currency or "ZAc" in currency or "0.01" in currency)) \
            or (financial_currency == "ILS" and ("ILA" in currency or "0.01" in currency)):
        fx_rate = 100

    else:

        c = CurrencyRates()

        rates = None
        num_retry = 0
        max_retry = 5
        while (rates is None and num_retry < max_retry):
            if num_retry > 0:
                print("# retry = " + str(num_retry) + ", get forex rates")

            if num_retry > 0:
                time.sleep(0.5 * num_retry)
            try:
                rates = c.get_rates(financial_currency)
            except requests.exceptions.ConnectionError:
                pass
            except ProtocolError:
                pass
            except RatesNotAvailableError:
                break

            num_retry += 1

        multiplier = 1
        if "GBp" in currency:
            currency = "GBP"
            multiplier = 100
        if "ZAc" in currency or "ZAC" in currency:
            currency = "ZAR"
            multiplier = 100
        if "ILA" in currency:
            currency = "ILS"
            multiplier = 100

        if rates is not None and currency in rates:
            fx_rate = rates[currency]
        else:
            if financial_currency == "USD":
                fx_rate = get_current_price_from_yahoo(f"{currency}=X")
            else:
                fx_rate = get_current_price_from_yahoo(f"{financial_currency}{currency}=X")
            # print("FX Yahoo", fx_rate)
            # raise Exception("mine")

        fx_rate *= multiplier

    # if debug:
    #     print("CONVERT ", financial_currency, "=>", currency, ": x", fx_rate)

    return fx_rate

def capitalize_rd(r_and_d, r_and_d_amortization_years, tax_rate, years):

    # print("DEBUG R&D")
    # print(r_and_d)
    # print(r_and_d_amortization_years)
    # print(tax_rate)
    # print(years)

    # last element does not amortize this year

    r_and_d = r_and_d[-r_and_d_amortization_years-1:]
    while len(r_and_d) < years:
        r_and_d.insert(0, 0)

    r_and_d_amortization_cy = [sum(i * 1 / r_and_d_amortization_years for i in r_and_d[:-1])]

    # first element is fully amortized after this year
    r_and_d_unamortized = [sum(i[0] * i[1] for i in
                               zip(r_and_d[-r_and_d_amortization_years:],
                                   np.linspace(1 / r_and_d_amortization_years, 1, r_and_d_amortization_years)))]

    ebit_r_and_d_adj = [r_and_d[-1] - r_and_d_amortization_cy[0]]
    tax_benefit = [ebit_r_and_d_adj[0] * tax_rate]

    # print("r_and_d after inserting 0", r_and_d)

    r_and_d_growth = []
    for i in range(len(r_and_d) - 1):
        try:
            r_and_d_growth.append(r_and_d[i + 1] / r_and_d[i] - 1)
        except:
            r_and_d_growth.append(0)

    # first element is the growth between most recent year and the year before that
    r_and_d_growth.reverse()

    # print(r_and_d)
    # print(r_and_d_growth)

    for g in r_and_d_growth:
        if g == -1:
            tax_benefit.append(0)
            ebit_r_and_d_adj.append(0)
            r_and_d_unamortized.append(0)
            r_and_d_amortization_cy.append(0)
        else:
            tax_benefit.append(tax_benefit[-1] / (1 + g))
            ebit_r_and_d_adj.append(ebit_r_and_d_adj[-1] / (1 + g))
            r_and_d_unamortized.append(r_and_d_unamortized[-1] / (1 + g))
            r_and_d_amortization_cy.append(r_and_d_amortization_cy[-1] / (1 + g))

    # reverse order
    for l in [ebit_r_and_d_adj, tax_benefit, r_and_d_unamortized, r_and_d_amortization_cy]:
        l.reverse()

    # print("r_and_d", r_and_d)
    # print("r_and_d_amortization_years", r_and_d_amortization_years)
    # print("r_and_d_growth", r_and_d_growth)
    # print("r_and_d_amortization_cy", r_and_d_amortization_cy)
    # print("r_and_d_unamortized", r_and_d_unamortized)
    # print("ebit_r_and_d_adj", ebit_r_and_d_adj)
    # print("tax_benefit", tax_benefit)

    return ebit_r_and_d_adj, tax_benefit, r_and_d_unamortized, r_and_d_amortization_cy

def get_spread_from_dscr(interest_coverage_ratio, damodaran_bond_spread):
    spread = damodaran_bond_spread[(interest_coverage_ratio >= damodaran_bond_spread["greater_than"]) &
                                   (interest_coverage_ratio < damodaran_bond_spread["less_than"])].iloc[0]
    return float(spread["spread"])

def debtize_op_leases(ttm_interest_expense, ttm_ebit_adj, damodaran_bond_spread, riskfree, country_default_spread,
                      leases, last_year_leases, tax_rate, revenue_growth):

    int_exp_op_adj = 0
    ttm_ebit_op_adj = 0
    debt_adj = [0]
    interest_coverage_ratio = 12.5
    company_default_spread = -1
    visited_icr = []
    done = False

    # CYCLE
    while not done:

        helper_interest_expense_adj = ttm_interest_expense + int_exp_op_adj
        helper_ebit_adj = ttm_ebit_adj + ttm_ebit_op_adj

        try:
            if helper_interest_expense_adj > 0:
                interest_coverage_ratio = min(99999, helper_ebit_adj / helper_interest_expense_adj)
        except:
            interest_coverage_ratio = 12.5

        spread = get_spread_from_dscr(interest_coverage_ratio, damodaran_bond_spread)

        if spread == company_default_spread:
            done = True

        else:

            if interest_coverage_ratio in visited_icr:
                done = True
            visited_icr.append(interest_coverage_ratio)

            company_default_spread = spread
            cost_of_debt = riskfree + country_default_spread + company_default_spread
            pv_leases = []
            for i in range(1, len(leases)):
                pv_leases.append(leases[i] / (1 + cost_of_debt) ** i)

            debt_adj = sum(pv_leases)
            if last_year_leases > 0:
                op_leases_depreciation = (debt_adj + leases[0]) / (last_year_leases + 1)
            else:
                op_leases_depreciation = 0

            # update helper_ebit
            ttm_ebit_op_adj = leases[0] - op_leases_depreciation

            # update helper_interest
            int_exp_op_adj = leases[0] * (1 - 1 / (1 + cost_of_debt))

        # print(interest_coverage_ratio, spread, company_default_spread, cost_of_debt)

    ebit_op_adj = [ttm_ebit_op_adj]
    tax_benefit_op = [ebit_op_adj[0] * tax_rate]
    debt_adj = [debt_adj]

    # first element is the growth between most year and the year before that
    revenue_growth.reverse()

    for g in revenue_growth:
        tax_benefit_op.append(tax_benefit_op[-1] / (1 + g))
        ebit_op_adj.append(ebit_op_adj[-1] / (1 + g))
        debt_adj.append(debt_adj[-1] / (1 + g))

    # restore revenue_growth
    revenue_growth.reverse()

    # reverse order
    for l in [tax_benefit_op, ebit_op_adj, debt_adj]:
        l.reverse()

    # print("leases", leases)
    # print("cost of debt", cost_of_debt)
    # print("pv_leases", pv_leases)
    # print("ttm_ebit_op_adj", ttm_ebit_op_adj)
    # print("tax_benefit_op", tax_benefit_op)
    # print("debt_adj", debt_adj)
    # print("years dep", last_year_leases)
    # print("depreciation", op_leases_depreciation)
    # print("helper_ebit_adj", helper_ebit_adj)
    # print("helper_interest_expense_adj", helper_interest_expense_adj)

    return ebit_op_adj, int_exp_op_adj, debt_adj, tax_benefit_op, company_default_spread

def get_growth_ttm(ttm_ebit_after_tax, ttm_net_income_adj, mr_equity_adj, mr_debt_adj, mr_cash_and_securities,
                 reinvestment, ttm_dividends, industry_payout):

    if (mr_debt_adj + mr_equity_adj - mr_cash_and_securities) > 0:
        # print("ROC LAST")
        # print(ttm_ebit_after_tax)
        # print(mr_debt_adj)
        # print(mr_equity_adj)
        # print(mr_cash_and_securities)
        roc_last = ttm_ebit_after_tax / (mr_debt_adj + mr_equity_adj - mr_cash_and_securities)
    else:
        roc_last = 0

    if ttm_ebit_after_tax > 0:
        reinvestment_last = reinvestment[-1] / ttm_ebit_after_tax
    else:
        reinvestment_last = 0

    if reinvestment_last < 0:
        reinvestment_last = 1 - industry_payout

    growth_last = roc_last * reinvestment_last

    if mr_equity_adj > 0:
        roe_last = ttm_net_income_adj / mr_equity_adj
    else:
        roe_last = 0

    if ttm_net_income_adj > 0:
        reinvestment_eps_last = 1 - ttm_dividends / ttm_net_income_adj
    else:
        reinvestment_eps_last = 0

    growth_eps_last = roe_last * reinvestment_eps_last

    return roc_last, reinvestment_last, growth_last, roe_last, reinvestment_eps_last, growth_eps_last

def get_roe_roc(equity_bv_adj, debt_bv_adj, cash_and_securities, ebit_after_tax, net_income_adj):
    roc = []
    roe = []
    avg_equity = sum(equity_bv_adj) / len(equity_bv_adj)
    for i in range(len(equity_bv_adj)):

        invested_capital = debt_bv_adj[i] + equity_bv_adj[i] - cash_and_securities[i]
        if invested_capital <= 0:
            roc.append(0)
        else:
            try:
                roc.append(ebit_after_tax[i] / invested_capital)
            except:
                roc.append(0)

        if equity_bv_adj[i] > 0:
            eq = equity_bv_adj[i]
        else:
            eq = avg_equity
        try:
            roe.append(net_income_adj[i] / eq)
        except:
            roe.append(0)
    return roe, roc

def get_target_info(revenue, ttm_revenue, country_default_spread, tax_rate, final_erp, riskfree,
                    unlevered_beta, damodaran_bond_spread, company_default_spread, target_debt_equity):

    cagr = None
    if abs(ttm_revenue/revenue[-1] - 1) > 0.0001:
        ttm = True
    else:
        ttm = False

    if ttm:
        rev_list = revenue + [ttm_revenue]
    else:
        rev_list = revenue

    first_index = -1

    if rev_list[0] > 0:
        first_index = 0
        first_revenue = rev_list[0]
    elif rev_list[1] > 0:
        first_index = 1
        first_revenue = rev_list[1]
    elif rev_list[2] > 0:
        first_index = 2
        first_revenue = rev_list[2]
    else:
        cagr = 0
        print("error CAGR 0 - no revenue first 3 years")

    if first_index >= 0:
        for i in rev_list[first_index:]:
            if i <= 0:
                cagr = 0


    if cagr is None:

        years_diff = -1
        for i in rev_list:
            if i > 0:
                years_diff += 1

        # Simple CAGR
        simple_cagr = (rev_list[-1] / first_revenue) ** (1/(years_diff)) - 1
        capped_simple_cagr = max(min(simple_cagr,0.3),-0.2)

        # CAGR from start
        cagr_from_start_list = []
        for i in range(1, years_diff+1):
            cagr_from_start_list.append((rev_list[first_index+i] / first_revenue) ** (1/i) - 1)

        abs_cagr_from_start = [abs(x) for x in cagr_from_start_list]
        cagr_from_start_sorted = [x for _, x in sorted(zip(abs_cagr_from_start, cagr_from_start_list), reverse=True)]

        value_sum, weight_sum = (0,0)
        for idx, value in enumerate(cagr_from_start_sorted):
            weight = 2**idx
            weight_sum += weight
            value_sum += value * weight

        cagr_from_start = value_sum / weight_sum
        capped_cagr_from_start = max(min(cagr_from_start,0.3),-0.2)

        # CAGR from end
        cagr_from_end_list = []
        for i in range(years_diff):
            cagr_from_end_list.append((rev_list[-1] / rev_list[first_index+i]) ** (1 / (years_diff-i)) - 1)

        abs_cagr_from_end = [abs(x) for x in cagr_from_end_list]
        cagr_from_end_sorted = [x for _, x in sorted(zip(abs_cagr_from_end, cagr_from_end_list), reverse=True)]

        value_sum, weight_sum = (0, 0)
        for idx, value in enumerate(cagr_from_end_sorted):
            weight = 2 ** idx
            weight_sum += weight
            value_sum += value * weight

        cagr_from_end = value_sum / weight_sum
        capped_cagr_from_end = max(min(cagr_from_end, 0.3), -0.2)

        # print("rev_list", rev_list)
        # print("first_revenue", first_revenue)
        # print("simple_cagr", simple_cagr)
        # print("capped_simple_cagr", capped_simple_cagr)
        # print("cagr_from_start_list", cagr_from_start_list)
        # print("cagr_from_start", cagr_from_start)
        # print("capped_cagr_from_start", capped_cagr_from_start)
        # print("cagr_from_end_list", cagr_from_end_list)
        # print("cagr_from_end", cagr_from_end)
        # print("capped_cagr_from_end", capped_cagr_from_end)

        cagr_3_values = [capped_simple_cagr, capped_cagr_from_start, capped_cagr_from_end]
        cagr_3_values.sort(reverse=True)

        value_sum, weight_sum = (0, 0)
        for idx, value in enumerate(cagr_3_values):
            weight = 2 ** idx
            weight_sum += weight
            value_sum += value * weight

        cagr = value_sum / weight_sum

    spread_list = list(damodaran_bond_spread["spread"].unique())
    spread_list = [float(x) for x in spread_list]
    spread_list.sort()

    debt_improvement_offset = 2
    idx = spread_list.index(company_default_spread)
    idx -= debt_improvement_offset
    if idx < 0:
        idx = 0
    target_company_default_spread = float(spread_list[idx])

    target_levered_beta = unlevered_beta * (1+ (1-tax_rate) * target_debt_equity)
    target_cost_of_equity = riskfree + final_erp * target_levered_beta
    target_cost_of_debt = riskfree + country_default_spread + target_company_default_spread
    target_cost_of_capital = target_cost_of_debt * (1-tax_rate) * target_debt_equity / (target_debt_equity + 1) + \
                             target_cost_of_equity * 1 / (1 + target_debt_equity)

    return cagr, target_levered_beta, target_cost_of_equity, target_cost_of_debt, target_cost_of_capital

def currency_bond_yield(currency, alpha_3_code, country_stats):

    currency_10y_bond, mother_country = get_10y_bond_yield(currency)

    if currency_10y_bond is not None:

        filter_df = country_stats[country_stats["country"] == mother_country.replace(" ", "")].iloc[0]
        country_default_spread = float(filter_df["adjusted_default_spread"])

        #10y yield currency - default risk mother currency
        riskfree = currency_10y_bond - country_default_spread

    else:

        if alpha_3_code is None:
            return -1

        us_10y_bond, _ = get_10y_bond_yield("USD")

        filter_df = country_stats[country_stats["country"] == "UnitedStates"].iloc[0]
        us_cds = float(filter_df["adjusted_default_spread"])

        riskfree_us = us_10y_bond - us_cds

        current_year_date = datetime.now().date().replace(day=1) - relativedelta(months=2)
        last_year_date = current_year_date - relativedelta(years=1)

        cpi_data = get_df_from_table("oecd_financial", f"where location IN ('{alpha_3_code}','USA') "
                                                       f"and indicator='CPI' "
                                                       f"and date in ('{last_year_date.strftime('%Y-%m-%d')}',"
                                                       f"'{current_year_date.strftime('%Y-%m-%d')}')")

        inflation_us = cpi_data[cpi_data["location"] == "USA"]
        inflation_us = inflation_us[inflation_us["date"] == current_year_date]["value"].iloc[0] / \
                       inflation_us[inflation_us["date"] == last_year_date]["value"].iloc[0] - 1

        inflation_country = cpi_data[cpi_data["location"] == alpha_3_code]
        if inflation_country.empty:
            inflation_country = inflation_us
        else:
            inflation_country = inflation_country[inflation_country["date"] == current_year_date]["value"].iloc[0] / \
                                inflation_country[inflation_country["date"] == last_year_date]["value"].iloc[0] - 1

        riskfree = riskfree_us * float(inflation_country) / float(inflation_us)
        print("10y bond yield not found - inflation_country", inflation_country, "inflation_us", inflation_us)

    return riskfree

def get_normalized_info(revenue, ebit_adj, revenue_delta, reinvestment, target_sales_capital,
                        ebit_after_tax, industry_payout, cagr, net_income_adj, roe, dividends, eps_adj, roc):

    weights = [2**x for x in range(len(revenue))]
    sum_weights = sum(weights)
    revenue_5y = sum(i[0] * i[1] for i in zip(revenue, weights)) / sum_weights
    ebit_5y = sum(i[0] * i[1] for i in zip(ebit_adj, weights)) / sum_weights

    try:
        operating_margin_5y = ebit_5y / revenue_5y
    except:
        operating_margin_5y = 0

    # print("## SALES CAPITAL ##")
    # print(revenue_delta)
    # print(sum(revenue_delta))
    # print(reinvestment)
    # print(sum(reinvestment))
    # print(sum(revenue_delta) / sum(reinvestment))
    # print("target", target_sales_capital)

    try:
        sales_capital_5y = sum(revenue_delta) / sum(reinvestment)
        if sales_capital_5y <= 0:
            sales_capital_5y = target_sales_capital
    except:
        sales_capital_5y = target_sales_capital

    roc_5y = sum(i[0] * i[1] for i in zip(roc, weights)) / sum_weights

    try:
        reinvestment_5y = sum(reinvestment) / sum(ebit_after_tax)
        if reinvestment_5y <= 0:
            reinvestment_5y = 1 - industry_payout
    except:
        reinvestment_5y = 1 - industry_payout

    try:
        growth_5y = roc_5y * reinvestment_5y
    except:
        growth_5y = cagr

    net_income_5y = sum(i[0] * i[1] for i in zip(net_income_adj, weights)) / sum_weights
    roe_5y = sum(i[0] * i[1] for i in zip(roe, weights)) / sum_weights

    try:
        reinvestment_eps_5y = 1 - sum(dividends) / sum(eps_adj)
    except:
        reinvestment_eps_5y = reinvestment_5y
    growth_eps_5y = roe_5y * reinvestment_eps_5y

    return revenue_5y, ebit_5y, operating_margin_5y, sales_capital_5y, roc_5y, reinvestment_5y, growth_5y, \
           net_income_5y, roe_5y, reinvestment_eps_5y, growth_eps_5y

def get_dividends_info(eps_adj, dividends):

    weights = [2 ** x for x in range(len(eps_adj))]
    sum_weights = sum(weights)

    eps_5y = sum(i[0] * i[1] for i in zip(eps_adj, weights)) / sum_weights
    try:
        payout_5y = sum(dividends) / sum(eps_adj)
    except:
        payout_5y = 0
    if payout_5y < 0:
        payout_5y = 0
    return eps_5y, payout_5y

def get_final_info(riskfree, cost_of_debt, equity_mkt, debt_mkt, unlevered_beta,
                   tax_rate, final_erp, company_default_spread):

    survival_prob = (1 - company_default_spread) ** 10

    try:
        debt_equity = debt_mkt / equity_mkt
    except:
        debt_equity = 0.5

    levered_beta = unlevered_beta * (1 + (1 - tax_rate) * debt_equity)

    cost_of_equity = riskfree + levered_beta * final_erp
    try:
        equity_weight = equity_mkt / (equity_mkt + debt_mkt)
    except:
        equity_weight = 0.5
    debt_weight = 1 - equity_weight
    cost_of_capital = cost_of_equity * equity_weight + cost_of_debt * (1 - tax_rate) * debt_weight

    return survival_prob, debt_equity, \
           levered_beta, cost_of_equity, equity_weight, debt_weight, cost_of_capital

def calculate_liquidation_value(cash, receivables, inventory, securities, other_current_assets, mr_property, ppe,
                                equity_investments, total_liabilities, equity_mkt, mr_debt, mr_equity, mr_original_min_interest,
                                minority_interest, debug=True):

    percent_minority_interest = minority_interest / equity_mkt
    # market_liquidation = equity_mkt + debt_mkt - mr_debt
    # if market_liquidation < 0:
    #     market_liquidation = 0

    damodaran_liquidation = cash + securities + mr_property + (other_current_assets + inventory + receivables + ppe) * 0.75 + \
                            equity_investments * 0.5 - total_liabilities
    if damodaran_liquidation < 0:
        damodaran_liquidation = 0
    net_net_wc_liquidation = cash + receivables + inventory + securities + mr_property + other_current_assets - total_liabilities
    if net_net_wc_liquidation < 0:
        net_net_wc_liquidation = 0

    _sorted = sorted([damodaran_liquidation, net_net_wc_liquidation], reverse=True)

    value_sum, weight_sum = (0, 0)
    for idx, value in enumerate(_sorted):
        weight = 2 ** idx
        weight_sum += weight
        value_sum += value * weight

    liquidation_value = value_sum / weight_sum * (1-percent_minority_interest)

    if debug:
        print("===== Liquidation Value =====\n")
        print("cash", cash)
        print("securities", securities)
        print("receivables", receivables)
        print("inventory", inventory)
        print("other_current_assets_ms", other_current_assets)
        print("property", mr_property)
        print("ppe", ppe)
        print("equity_investments", equity_investments)
        print()
        print("total_liabilities", total_liabilities)
        # print("percent_minority_interest", percent_minority_interest)
        # print("equity_mkt", equity_mkt)
        # print("debt_mkt", debt_mkt)
        print("debt_bv", mr_debt)
        print("equity_bv", mr_equity)
        print("minority_interest", mr_original_min_interest, "=>", minority_interest)
        # print("market_liquidation", market_liquidation)
        print("damodaran_liquidation", damodaran_liquidation)
        print("net_net_wc_liquidation", net_net_wc_liquidation)
        print("liquidation_value", liquidation_value)
        print("\n\n")

    return liquidation_value

def get_industry_parameter(df, industry, region, parameter, debug=True):

    region_waterfall = {
        "Europe": "US",
        "US": "Global",
        "Japan": "Global",
        "China": "emerg",
        "India": "emerg",
        "emerg": "Global",
        "Rest": "US"
    }

    value = None

    try:
        series = df[df["region"] == region].iloc[0]
        value = series[parameter]
    except:
        pass

    # print(industry, region, parameter, value)

    if value is None or math.isnan(value):
        if region in region_waterfall:
            if debug:
                print("value not found for ", industry, region, parameter)
                print("searching now in region ", region_waterfall[region])
            return get_industry_parameter(df, industry, region_waterfall[region], parameter)
        else:
            print("*** ERROR DAMODARAN_INDUSTRY_DATA: ", industry, region, parameter)
            if parameter == "sales_capital":
                value = 1
            elif parameter == "cash_return":
                value = 0
            elif parameter == "pbv":
                value = 1
            elif parameter == "unlevered_beta":
                value = 1
            elif parameter == "opmargin_adjusted":
                value = 0.05
            elif parameter == "debt_equity":
                value = 1
            else:
                value = 0
            print("using default value ", value)
            return value
    else:
        return float(value)

def get_industry_data(industry, region, geo_segments_df, revenue, ebit_adj, revenue_delta, reinvestment, equity_mkt, debt_mkt,
                          equity_bv_adj, debt_bv_adj, mr_equity_adj, mr_debt_adj, min_std=0.1, max_std=1):

    # TAKE 1/3 value from last year
    # 2/3 value from this year

    columns = ["industry_name","region","sales_capital","cash_return","unlevered_beta","opmargin_adjusted","debt_equity","pbv"]
    df_last_year = get_df_from_table("damodaran_industry_data", f"where industry_name = '{industry}' and "
                                                                f"created_at = (SELECT MAX(created_at) "
                                                                f"FROM damodaran_industry_data "
                                                                f"WHERE created_at < date_trunc('year',now()))")[columns]
    df = get_df_from_table("damodaran_industry_data", f"where industry_name = '{industry}'", most_recent=True)[columns]

    # print(df_last_year)
    # print(df)
    df = pd.merge(df, df_last_year, left_on=["industry_name", 'region'],
                  right_on=["industry_name", 'region'], how="left")

    for x in [i for i in columns if i not in ["industry_name","region"]]:
        # if there is no last year value take the most recent
        df[x+"_y"] = df[x+"_y"].fillna(df[x+"_x"])
        # TAKE 1/3 value from last year
        # 2/3 value from this year
        df[x] = df[x+"_y"] * 1/3 + df[x+"_x"] * 2/3

    industry_sales_capital = 0
    industry_payout = 0
    pbv = 0
    unlevered_beta = 0
    industry_operating_margin = 0
    industry_debt_equity = 0

    debug=True

    if geo_segments_df is None or geo_segments_df.empty:
        industry_sales_capital = get_industry_parameter(df, industry, region, "sales_capital", debug=debug)
        industry_payout = min(1, get_industry_parameter(df, industry, region, "cash_return", debug=debug))
        pbv = get_industry_parameter(df, industry, region, "pbv", debug=debug)
        unlevered_beta = get_industry_parameter(df, industry, region, "unlevered_beta", debug=debug)
        industry_operating_margin = get_industry_parameter(df, industry, region, "opmargin_adjusted", debug=debug)
        industry_debt_equity = get_industry_parameter(df, industry, region, "debt_equity", debug=debug)

    else:
        for _, row in geo_segments_df.iterrows():
            percent = row["value"]
            r = row["region"]

            tsc = get_industry_parameter(df, industry, r, "sales_capital", debug=debug)
            ip = min(1, get_industry_parameter(df, industry, r, "cash_return", debug=debug))
            p = get_industry_parameter(df, industry, r, "pbv", debug=debug)
            ub = get_industry_parameter(df, industry, r, "unlevered_beta", debug=debug)
            tom = get_industry_parameter(df, industry, r, "opmargin_adjusted", debug=debug)
            tde = get_industry_parameter(df, industry, r, "debt_equity", debug=debug)

            industry_sales_capital += tsc * percent
            industry_payout = min(1, industry_payout + ip * percent)
            pbv += p * percent
            unlevered_beta += ub * percent
            industry_operating_margin += tom * percent
            industry_debt_equity += tde * percent

    operating_margin = []
    debt_equity = []
    sales_capital = []

    try:
        sales_capital_5y = sum(revenue_delta) / sum(reinvestment)
        if sales_capital_5y <= 0:
            sales_capital_5y = industry_sales_capital
    except:
        sales_capital_5y = industry_sales_capital

    for i in range(len(revenue)):
        if revenue[i] > 0:
            operating_margin.append(ebit_adj[i] / revenue[i])
        else:
            operating_margin.append(0)

        num = (debt_bv_adj[i] * (debt_mkt/mr_debt_adj)) if mr_debt_adj > 0 else debt_bv_adj[i]
        den = (equity_bv_adj[i] * (equity_mkt/mr_equity_adj)) if mr_equity_adj > 0 else equity_bv_adj[i]
        if den > 0 and num / den > 0:
            debt_equity.append(num/den)
        else:
            debt_equity.append(0)

        try:
            if revenue_delta[i] / reinvestment[i] > 0:
                sales_capital.append(revenue_delta[i] / reinvestment[i])
            else:
                sales_capital.append(sales_capital_5y)
        except:
            sales_capital.append(sales_capital_5y)

    weights = [x+1 for x in range(len(revenue))]
    sum_weights = sum(weights)

    # print(debt_bv_adj)
    # print(debt_mkt)
    # print(mr_debt_adj)
    # print(equity_bv_adj)
    # print(equity_mkt)
    # print(mr_equity_adj)
    # print(debt_equity)

    om_company = sum(i[0] * i[1] for i in zip(operating_margin, weights)) / sum_weights
    de_company = sum(i[0] * i[1] for i in zip(debt_equity, weights)) / sum_weights
    sc_company = sum(i[0] * i[1] for i in zip(sales_capital, weights)) / sum_weights

    std_om_company = np.std(operating_margin)
    std_de_company = np.std(debt_equity)
    std_sc_company = np.std(sales_capital)

    if om_company != 0:
        om_industry_weight = max(0, min(1, ((std_om_company / om_company) - min_std) / (max_std - min_std)))
    else:
        om_industry_weight = 1

    if de_company != 0:
        de_industry_weight = max(0, min(1, ((std_de_company / de_company) - min_std) / (max_std - min_std)))
    else:
        de_industry_weight = 1

    if sc_company != 0:
        sc_industry_weight = max(0, min(1, ((std_sc_company / sc_company) - min_std) / (max_std - min_std)))
    else:
        sc_industry_weight = 1

    target_sales_capital = sc_industry_weight * industry_sales_capital + (1 - sc_industry_weight) * sc_company
    target_debt_equity = de_industry_weight * industry_debt_equity + (1 - de_industry_weight) * de_company
    target_operating_margin = om_industry_weight * industry_operating_margin + (1 - om_industry_weight) * om_company

    # print("DEBUG TARGETS")
    # print(sales_capital)
    # print(operating_margin)
    # print(debt_equity)
    # print("sc_company",sc_company,"industry_sales_capital",industry_sales_capital,"std_sc_company",std_sc_company,"sc_industry_weight",sc_industry_weight,"target_sales_capital",target_sales_capital)
    # print("om_company",om_company,"industry_operating_margin",industry_operating_margin,"std_om_company",std_om_company,"om_industry_weight",om_industry_weight,"target_operating_margin",target_operating_margin)
    # print("de_company",de_company,"industry_debt_equity",industry_debt_equity,"std_de_company",std_de_company,"de_industry_weight",de_industry_weight,"target_debt_equity",target_debt_equity)

    return target_sales_capital, industry_payout, pbv, unlevered_beta, target_operating_margin, target_debt_equity

EARNINGS_TTM = "EARNINGS_TTM"
EARNINGS_NORM = "EARNINGS_NORM"
GROWTH_FIXED = "GROWTH_FIXED"
GROWTH_TTM = "GROWTH_TTM"
GROWTH_NORM = "GROWTH_NORM"

def dividends_valuation(earnings_type, growth_type, cagr, growth_eps_5y, growth_5y, riskfree,
                        industry_payout, cost_of_equity, target_cost_of_equity,
                        growth_eps_last, eps_5y, payout_5y, ttm_eps_adj, reinvestment_eps_last, fx_rate, survival_prob,
                        liquidation_per_share, debug=True, recession=False, dict_values_for_bi=None):

    final_growth = riskfree

    if growth_5y != 0:
        final_growth = riskfree * growth_eps_5y / growth_5y
        if final_growth <= 0:
            final_growth = riskfree
        else:
            final_growth = riskfree * max(min(growth_eps_5y / growth_5y, 2), 0.5)

    if growth_type == GROWTH_FIXED:
        if growth_5y == 0:
            initial_growth = cagr
        else:
            initial_growth = cagr * growth_eps_5y / growth_5y

            if initial_growth <= 0:
                initial_growth = cagr
            else:
                initial_growth = cagr * max(min(growth_eps_5y / growth_5y, 2),0.5)

    elif growth_type == GROWTH_TTM:
        initial_growth = growth_eps_last
    else:
        initial_growth = growth_eps_5y

    growth_history = np.linspace(initial_growth, final_growth, 11)

    if recession:
        growth_history[3:6] = 0

    if earnings_type == EARNINGS_TTM:
        initial_eps = ttm_eps_adj
    else:
        initial_eps = eps_5y

    eps_history = []

    for i in range(len(growth_history)):
        if initial_eps < 0 and  i < 6:
            if i == 0:
                eps_history.append(initial_eps + abs(initial_eps) / 5)
            else:
                eps_history.append(eps_history[i-1] + abs(initial_eps) / 5)
        else:
            if i == 0:
                eps_history.append(initial_eps * (1+growth_history[i]))
            else:
                eps_history.append(eps_history[i-1] * (1+growth_history[i]))

    if earnings_type == EARNINGS_TTM:
        try:
            initial_payout = 1 - reinvestment_eps_last
        except:
            initial_payout = payout_5y
    else:
        initial_payout = payout_5y

    final_payout = industry_payout
    payout_history = np.linspace(initial_payout, final_payout, 12)
    payout_history = payout_history[1:]

    if recession:
        payout_history[3:6] = 0

    dps_history = []
    for i in range(len(eps_history)):
        dps_history.append(eps_history[i] * payout_history[i])

    initial_coe = cost_of_equity
    final_coe = target_cost_of_equity
    cost_of_equity_history = np.linspace(initial_coe, final_coe, 12)
    cost_of_equity_history = cost_of_equity_history[1:]

    cumulative_coe = []
    for i in range(len(cost_of_equity_history)):
        if i == 0:
            cumulative_coe.append(1+cost_of_equity_history[i])
        else:
            cumulative_coe.append(cumulative_coe[i-1] * (1+cost_of_equity_history[i]))

    present_value = []
    for i in range(len(cumulative_coe)-1):
        present_value.append(dps_history[i] / cumulative_coe[i])

    terminal_value = eps_history[-1] * payout_history[-1] / \
                     (cost_of_equity_history[-1] - growth_history[-1])
    terminal_pv = terminal_value / cumulative_coe[-1]

    stock_value_price_curr = sum(present_value) + terminal_pv

    if fx_rate is not None:
        stock_value = stock_value_price_curr * fx_rate
    else:
        stock_value = stock_value_price_curr

    stock_value = stock_value * survival_prob + liquidation_per_share * (1 - survival_prob)

    if dict_values_for_bi is not None:
        dict_values_for_bi[f"{earnings_type} + {growth_type} + recession:{recession}"] = {
            "eps": eps_history,
            "dividends": dps_history,
            "cost_of_equity": cost_of_equity_history,
            "pv_of_dividends": present_value + [terminal_pv]
        }

    if debug:

        for i in [growth_history, eps_history, payout_history, dps_history, cost_of_equity_history, cumulative_coe, present_value]:
            for idx, j in enumerate(i):
                i[idx] = round(j,4)

        print(f"===== Dividends Valuation - {earnings_type} + {growth_type} + recession:{recession} =====\n")
        print("expected_growth", growth_history)
        print("earnings_per_share", eps_history)
        print("payout_ratio", payout_history)
        print("dividends_per_share", dps_history)
        print("cost_of_equity", cost_of_equity_history)
        print("cumulative_cost_equity", cumulative_coe)
        print("present_value",present_value)
        print("terminal_value",round(terminal_value,2))
        print("PV of terminal_value", round(terminal_pv,2))
        print("stock value (price curr)", round(stock_value_price_curr,2))
        print("stock value (fin curr)", round(stock_value, 2))
        print("\n\n")

    return stock_value

def fcff_valuation(earnings_type, growth_type, cagr, riskfree, ttm_revenue, ttm_ebit_adj, target_operating_margin, tax_benefits,
                   tax_rate, sales_capital_5y, target_sales_capital, debt_equity, target_debt_equity, unlevered_beta,
                   final_erp, cost_of_debt, target_cost_of_debt, mr_cash, mr_securities, debt_mkt, minority_interest, survival_prob,
                   share_issued, ko_proceeds, growth_last, growth_5y, revenue_5y, ebit_5y, fx_rate, mr_property, mr_sbc, debug=True,
                   recession=False, dict_values_for_bi=None):

    # earnings ttm + growth fixed

    if growth_type == GROWTH_FIXED:
        initial_growth = cagr
    elif growth_type == GROWTH_TTM:
        initial_growth = growth_last
    else:
        initial_growth = growth_5y
    final_growth = riskfree

    growth_history = np.linspace(initial_growth, final_growth, 11)

    if recession:
        growth_history[3] = -0.1
        growth_history[4] = -0.2
        growth_history[5] = 0.4

    if earnings_type == EARNINGS_TTM:
        initial_revenue = ttm_revenue
    else:
        initial_revenue = revenue_5y

    revenue_history = []
    for i in range(len(growth_history)):
        if i == 0:
            revenue_history.append(initial_revenue * (1+growth_history[i]))
        else:
            revenue_history.append(revenue_history[i-1] * (1+growth_history[i]))

    if earnings_type == EARNINGS_TTM:
        if ttm_revenue == 0:
            initial_margin = 0
        else:
            initial_margin = ttm_ebit_adj / ttm_revenue
    else:
        if revenue_5y == 0:
            initial_margin = 0
        else:
            initial_margin = ebit_5y / revenue_5y

    final_margin = target_operating_margin
    margin_history = np.linspace(initial_margin, final_margin, 12)[1:]

    if initial_margin < 0:
        check_margin = initial_margin / 5

        if check_margin * 4 > margin_history[0]:

            for i in range(len(margin_history)):
                if i < 5:
                    margin_history[i] = initial_margin - check_margin * (i+1)
                else:
                    margin_history[i] = final_margin / 6 * (i-4)

    if recession:
        margin_history[3] *= 0.5
        margin_history[4] *= 0.25
        margin_history[5] *= 0.5

    ebit_history = []
    for i in range(len(revenue_history)):
        ebit_history.append(revenue_history[i] * margin_history[i])

    residual_tax_benefits = tax_benefits
    tax_history = []
    ebit_after_tax_history = []
    for i in range(len(ebit_history)):
        e = ebit_history[i]
        if e < 0:
            tax_history.append(0)
        else:
            tax_history.append(max(0, (e * tax_rate) - residual_tax_benefits))
            residual_tax_benefits = max(0, residual_tax_benefits - (e * tax_rate))

        ebit_after_tax_history.append(ebit_history[i] - tax_history[i])

    initial_sales_capital = sales_capital_5y
    final_sales_capital = target_sales_capital
    sales_capital_history = np.linspace(initial_sales_capital, final_sales_capital, 12)[1:]

    reinvestment_history = []
    fcff_history = []
    for i in range(len(revenue_history)):
        if i == 0:
            delta_revenue = revenue_history[i] - initial_revenue
        else:
            delta_revenue = revenue_history[i] - revenue_history[i-1]
        reinvestment_history.append(delta_revenue / sales_capital_history[i])

        if recession and i in [3,4]:
            reinvestment_history[i] = reinvestment_history[i-1]

        fcff_history.append(ebit_after_tax_history[i] - reinvestment_history[i])

    # initial_debt_ratio = debt_weight
    # final_debt_ratio = target_debt_equity / (1+target_debt_equity)
    # debt_ratio_history = np.linspace(initial_debt_ratio, final_debt_ratio, 12)[1:]

    debt_equity_history = np.linspace(debt_equity, target_debt_equity, 12)[1:]

    initial_cost_of_debt = cost_of_debt
    final_cost_of_debt = target_cost_of_debt
    cost_of_debt_history = np.linspace(initial_cost_of_debt, final_cost_of_debt, 12)[1:]

    debt_ratio_history = []
    beta_history = []
    cost_of_equity_history = []
    cost_of_capital_history = []
    cumulative_wacc_history = []
    present_value_history = []
    for i in range(len(debt_equity_history)):
        debt_ratio_history.append(debt_equity_history[i] / (debt_equity_history[i] + 1))
        # debt_equity_history.append(debt_ratio_history[i] / (1-debt_ratio_history[i]))
        beta_history.append(unlevered_beta * (1+(1-tax_rate)*debt_equity_history[i]))
        cost_of_equity_history.append(riskfree + final_erp * beta_history[i])
        cost_of_capital_history.append(cost_of_equity_history[i] * (1-debt_ratio_history[i])
                                       + cost_of_debt_history[i] * (1-tax_rate) * debt_ratio_history[i])

        if i == 0:
            cumulative_wacc_history.append(1 + cost_of_capital_history[i])
        else:
            cumulative_wacc_history.append(cumulative_wacc_history[i-1] * (1 + cost_of_capital_history[i]))

        present_value_history.append(fcff_history[i] / cumulative_wacc_history[i])

    terminal_value = fcff_history[-1] / (cost_of_capital_history[-1] - growth_history[-1])
    terminal_value_pv = terminal_value / cumulative_wacc_history[-1]

    firm_value = sum(present_value_history[:-1]) + terminal_value_pv + mr_cash + mr_securities + mr_property

    equity_value = firm_value - debt_mkt - minority_interest - mr_sbc

    try:
        stock_value_price_curr = (equity_value * survival_prob + ko_proceeds * (1-survival_prob)) / share_issued
    except:
        stock_value_price_curr = 0

    if fx_rate is not None:
        stock_value = stock_value_price_curr * fx_rate
    else:
        stock_value = stock_value_price_curr

    if dict_values_for_bi is not None:
        dict_values_for_bi[f"{earnings_type} + {growth_type} + recession:{recession}"].update({
            "revenue": revenue_history,
            "ebit": ebit_history,
            "ebit_after_tax": ebit_after_tax_history,
            "reinvestment": reinvestment_history,
            "FCFF": fcff_history,
            "cost_of_capital": cost_of_capital_history,
            "pv_of_FCFF": present_value_history[:-1] + [terminal_value_pv]
        })

    if debug:

        # for i in [growth_history, revenue_history, margin_history, ebit_history, tax_history, ebit_after_tax_history,
        #           sales_capital_history, reinvestment_history, fcff_history, debt_ratio_history, cost_of_debt_history,
        #           cost_of_equity_history, cost_of_capital_history, cumulative_wacc_history, present_value_history]:
        #     for idx, j in enumerate(i):
        #         i[idx] = round(j,4)

        print(f"===== FCFF Valuation - {earnings_type} + {growth_type} recession:{recession} =====\n")
        print("expected_growth", growth_history)
        print("revenue", revenue_history)
        print("margin", margin_history)
        print("ebit", ebit_history)
        print("tax_history", tax_history)
        print("ebit_after_tax", ebit_after_tax_history)
        print("sales_capital", sales_capital_history)
        print("reinvestment", reinvestment_history)
        print("FCFF", fcff_history)
        print("debt ratio", debt_ratio_history)
        print("debt2equity", debt_equity_history)
        print("beta", beta_history)
        print("cost_of_equity", cost_of_equity_history)
        print("cost_of_debt", cost_of_debt_history)
        print("cost_of_capital", cost_of_capital_history)
        print("cumulative WACC", cumulative_wacc_history)
        print("present value", present_value_history)
        print("terminal value", round(terminal_value,2))
        print("PV of FCFF during growth", sum(present_value_history[:-1]))
        print("PV of terminal value", round(terminal_value_pv,2))
        print("Value of operating assets", sum(present_value_history[:-1])+terminal_value_pv)
        print("Value of cash and property", mr_cash + mr_securities + mr_property)
        print("firm value", round(firm_value,2))
        print("debt outstanding", round(debt_mkt,2))
        print("equity value", round(equity_value,2))
        print("stock value (price curr)", round(stock_value_price_curr,2))
        print("stock value (fin curr)", round(stock_value, 2))
        print("\n\n")

    return stock_value

def summary_valuation(valuations):

    sorted = valuations.copy()
    sorted = [0 if math.isnan(x) else x for x in sorted]
    sorted.sort()

    count_negative = 0
    for val in sorted:
        if val < 0:
            count_negative += 1

    if count_negative > 1:
        result = 0
    elif count_negative > 0:
        result = sorted[1]
    else:
        second_highest = sorted[2]
        third_highest = sorted[1]
        if third_highest == 0 or second_highest / third_highest > 10:
            result = 0
        else:
            max_val = max(sorted)
            min_val = min(sorted)
            if min_val == 0 or max_val / min_val > 3:
                result = sorted[1]
            else:
                result = median(sorted)

    return result

def get_status(fcff_delta, div_delta, liquidation_delta, country, region, company_size, company_type, dilution, complexity,
               revenue, receivables=None, inventory=None, debug=False):

    STATUS_OK = "OK"
    STATUS_NI = "NI"
    STATUS_KO = "KO"

    max_base = 0.2
    if liquidation_delta < -0.5:
        t = -max_base
    elif liquidation_delta < 0:
        t = liquidation_delta / 0.5 * max_base
    elif liquidation_delta >= 10:
        t = max_base
    else:
        t = np.log10(liquidation_delta + 1) * max_base

    if debug:
        print("base threshold", t, "(liquidation delta:", liquidation_delta, ")")

    if country == "United States":
        t += 0
    elif region in ["US","EU","Japan","Rest"]:
        t += 0.05
    else:
        t += 0.1

    if debug:
        print("country/region threshold", t, "(country:", country, ", region:", region, ")")

    if company_size == "Mega":
        t += 0
    elif company_size == "Large":
        t += 0.05
    elif company_size == "Medium":
        t += 0.12
    elif company_size == "Small":
        t += 0.18
    elif company_size == "Micro":
        t += 0.25
    else:
        t += 0.3

    if debug:
        print("company size threshold", t, "(company size:", company_size, ")")

    elif complexity == 2:
        t += 0.03
    elif complexity == 3:
        t += 0.06
    elif complexity == 4:
        t += 0.1
    elif complexity == 5:
        t += 0.2

    if debug:
        print("company complexity threshold", t, "(complexity:", complexity, ")")

    if dilution > 0.1:
        t += 0.05
    elif dilution > 0.02:
        t += 0.02

    if debug:
        print("dilution threshold", t, "(dilution:", dilution, ")")

    first_idx = 0
    for i, r in enumerate(revenue):
        if r > 0:
            first_idx = i
            break

    ratio, score = calculate_divergence(revenue[first_idx], inventory[first_idx], revenue[-1], inventory[-1])
    if score is None:
        if ratio < 10:
            t += 0.02
        elif ratio < 5:
            t += 0.05
    else:
        if score > 0.8 or score < -0.4:
            t += 0.05
        elif score > 0.4 or score < -0.2:
            t += 0.02

    if debug:
        print("inventory divergence threshold", t, "(inventory:", inventory, ", revenue:", revenue, ")")

    ratio, score = calculate_divergence(revenue[first_idx], receivables[first_idx], revenue[-1], receivables[-1])
    if score is None:
        if ratio < 10:
            t += 0.02
        elif ratio < 5:
            t += 0.05
    else:
        if score > 0.8 or score < -0.4:
            t += 0.05
        elif score > 0.4 or score < -0.2:
            t += 0.02

    if debug:
        print("receivables divergence threshold", t, "(receivables:", receivables, ", revenue:", revenue, ")")

    if company_type["declining"]:
        t += 0.05
    if company_type["turn_around"]:
        t += 0.05
    if company_type["cyclical"]:
        t += 0.05

    if debug:
        print("company type threshold", t, "(company_type:", company_type, ")")

    if fcff_delta < -t:
        if div_delta < -t:
            status = STATUS_OK
        else:
            status = STATUS_NI
    elif fcff_delta < 0:
        if div_delta < 0:
            status = STATUS_NI
        else:
            status = STATUS_KO
    else:
        if div_delta < -t:
            status = STATUS_NI
        else:
            status = STATUS_KO

    if debug:
        print("status", status)
        print()

    return status


import math


def calculate_divergence(initial_a, initial_b, final_a, final_b):

    try:
        ratio = max(initial_a, final_a) / max(initial_b, final_b)
    except:
        return 0, 0

    try:
        growth_a = final_a / initial_a
        growth_b = final_b / initial_b
        growth_ratio = growth_b / growth_a - 1
    except:
        return ratio, None

    # print(initial_a, "=>", final_a, "|", initial_b, "=>", final_b, "ratio:", ratio, "growth_ratio:", growth_ratio, "result:", growth_ratio/ratio)

    return ratio, growth_ratio / ratio

    # A growing
    # < -0.5 = risky
    # > 0.4 = risky
    # > 2 = bad

    # A declining
    # < -2 = bad
    # < 0.4 risky


if __name__ == '__main__':

    initial_a = 100

    final_a = [0]
    initial_b = [100]
    final_b = [
        [50, 0]
    ]

    for f_a in final_a:
        for idx, i_b in enumerate(initial_b):
            for f_b in final_b[idx]:
                divergence = calculate_divergence(initial_a, i_b, f_a, f_b)
            print()
        print("=====")


import time
from datetime import datetime
from ssl import SSLError

import yfinance as yf
import requests
from bs4 import BeautifulSoup
from dateutil.relativedelta import relativedelta
from requests import ReadTimeout
from requests.exceptions import ChunkedEncodingError
from urllib3.exceptions import MaxRetryError, ReadTimeoutError
import traceback

packet_stream_proxy = "http://easymap_buyer:34Qgo0O03zOhrx8h@proxy.packetstream.io:31112"
bright_data_proxy = 'http://brd-customer-hl_f8b1a708-zone-finance:u7iz73qdf9wv@brd.superproxy.io:22225'
proxy = bright_data_proxy

def request_yahoo_url(url):
    hed = {'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8',
           'Accept-Encoding': 'gzip, deflate, br', 'Accept-Language': 'it-IT,it;q=0.9,en-US;q=0.8,en;q=0.7',
           'Upgrade-Insecure-Requests': '1',
           'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_14_1) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/12.0.1 Safari/605.1.15',
           'Cache-Control': 'PUBLIC'}
    cookies = {
        "EuConsent": "CPUNe08PUNe08AOACBITB-CoAP_AAH_AACiQIJNe_X__bX9n-_59__t0eY1f9_r3v-QzjhfNt-8F2L_W_L0H_2E7NB36pq4KuR4ku3bBIQFtHMnUTUmxaolVrzHsak2MpyNKJ7LkmnsZe2dYGHtPn9lD-YKZ7_7___f73z___9_-39z3_9f___d9_-__-vjfV_993________9nd____BBIAkw1LyALsSxwJNo0qhRAjCsJCoBQAUUAwtEVgAwOCnZWAT6ghYAITUBGBECDEFGDAIAAAIAkIiAkALBAIgCIBAACAFCAhAARMAgsALAwCAAUA0LEAKAAQJCDI4KjlMCAiRaKCWysQSgr2NMIAyywAoFEZFQgIlCCBYGQkLBzHAEgJYAYaADAAEEEhEAGAAIIJCoAMAAQQSA",
        "OTH": "v=1&d=eyJraWQiOiIwMTY0MGY5MDNhMjRlMWMxZjA5N2ViZGEyZDA5YjE5NmM5ZGUzZWQ5IiwiYWxnIjoiUlMyNTYifQ.eyJjdSI6eyJndWlkIjoiWVZURENIQVJDVFFUSVM3WDVBN0g0NzZYVDQiLCJwZXJzaXN0ZW50Ijp0cnVlLCJzaWQiOiJNZm1Bc291aHZTbzIifX0.Qz8bX4q6yUmgNqoxVogtnln1kNlA5oc9hhMFm_baVHvl2_gnK5almd6r-u_Wx4W9c9uhi2g9dvovheQr6DXlkGlG7Bw7OJubPeSGqy4asxOWAO4VNpUppmdK9kVuwOQIbnpg5skXXuGykmWRnUrtZH4resNBrOJhXgfUehIROpQ",
        "GUC": "AQAABgFiBrFi6kIhUQUB",
        "maex": "%7B%22v2%22%3A%7B%22106c4e0d%22%3A%7B%22lst%22%3A1644604741%2C%22ic%22%3A56%7D%7D%7D",
        "UIDR": "1599641610",
        "cmp": "v=22&t=1644604104&j=1",
        "PRF": "t%3DAAPL%252BABTG.MI%252BFB%252B%255ESOX%252BMSFT%252BSLV%252BKO%252BGOOG%252BVT%252BREET%252BBNDW%252BFCT.MI%252BISP.MI%252BCRES.MI%252BTRASTOR.AT",
        "A1S": "d=AQABBJDkvF8CEEnmdkmy3hsZxUP4oHXu3MoFEgAABgGxBmLqYudVb2UB9iMAAAcIf7Z8XzRwxloID4-gDUXX3Q7JnS7c59zqFwkBBwoBMA&S=AQAAAujaKZu-E9Ike-e7u6WnYmk&j=WORLD",
        "B": "5lhjg6hfnpdjv&b=4&d=Jw.N6YdtYFmKelHVCZg9&s=cc&i=j6ANRdfdDsmdLtzn3OoX",
    }
    proxies = {
        "http": proxy,
        "https": proxy,
    }
    response = None
    num_retry = 0
    max_retry = 5

    #return requests.get(url, headers=hed, proxies=proxies)

    while ((response is None or response.status_code == 403) and num_retry < max_retry):

        if num_retry > 0:
            print("# retry = "+str(num_retry)+", response = "+str(response)+", url = "+url)

        if num_retry > 0:
            time.sleep(0.5 * num_retry)
        try:
            response = requests.get(url, headers=hed, proxies=proxies, timeout=20)
        except requests.exceptions.SSLError:
            pass
        except requests.exceptions.ConnectionError:
            pass
        except ReadTimeout:
            pass
        except ChunkedEncodingError:
            pass
        except MaxRetryError:
            pass
        except ReadTimeoutError:
            pass
        except:
            print(traceback.format_exc())

        num_retry += 1

    return response

def get_premarket_price_yahoo(ticker):

    url = f"https://finance.yahoo.com/quote/{ticker}"
    response = request_yahoo_url(url)
    # network_size = len(response.content)

    if response is not None:
        if response.status_code == 200:
            soup = BeautifulSoup(response.text, 'lxml')
            quote_header = soup.find("div", id="quote-header-info")

            if quote_header is None:
                return None#, network_size

            premarket = quote_header.select_one('fin-streamer[data-field="preMarketPrice"]')
            if premarket is not None:
                return float(premarket.text.replace(",",""))#, network_size

            regular = quote_header.select_one('fin-streamer[data-field="regularMarketPrice"]')
            if regular is not None:
                return float(regular.text.replace(",",""))#, network_size

    return None#, network_size

def get_current_price_from_yahoo(ticker, created_at=None):

    if created_at is None:
        price = get_premarket_price_yahoo(ticker)
        if price is not None:
            return price
        else:
            created_at = datetime.now().date()

    t = yf.Ticker(ticker)

    todays_data = None
    max_retry = 5
    retry = 0
    while todays_data is None and retry < max_retry:
        try:
            todays_data = t.history(start=created_at - relativedelta(days=5), end=created_at,
                                    interval="1m")

        except:
            print(f"{ticker} conn err - retry")
        retry += 1
    try:
        return todays_data['Close'][-1]
    except IndexError:
        return None
    except TypeError:
        return None


#### fundamentalytics

import requests
import json
import os.path
import pandas as pd

header = {'User-Agent': 'Fundamentalytics'}

def download_tickers(filename:str='all_tickers.json'):
    url = 'https://www.sec.gov/files/company_tickers.json'
    response = requests.get(url, headers=header)
    with open(filename, 'w') as f:
        json.dump(response.json(), f)

def load_tickers(filename:str='all_tickers.json'):
    if not os.path.isfile(filename):
        download_tickers()
    with open(filename, 'r') as f:
        all_tickers = json.load(f)
    return all_tickers

def get_all_companies():
    raw = load_tickers()
    all_companies = []
    for _, data in raw.items():
        all_companies.append(str(data['ticker'] + ' ' +  data['title'] + ' ' + str(data['cik_str'])))
    return all_companies

def get_company_facts(cik: str):
    if len(cik) < 10:
        cik = (10 - len(cik)) * '0' + cik
    
    url = f'https://data.sec.gov/api/xbrl/companyfacts/CIK{cik}.json'
    response = requests.get(url, headers=header).json()
    
    result = {
        'info': {
            'cik': response['cik'],
            'entityName': response['entityName']
        },
        'facts': {
        }
    }

    facts = response['facts']['dei'] | response['facts']['us-gaap']
    
    for k, v in facts.items():
        if k in ['WeightedAverageNumberOfSharesOutstandingBasic', 'WeightedAverageNumberOfDilutedSharesOutstanding',
                 'Revenues', 'RevenueFromContractWithCustomerExcludingAssessedTax', 'OperatingIncomeLoss',
                 'IncomeLossFromContinuingOperationsBeforeIncomeTaxesExtraordinaryItemsNoncontrollingInterest',
                 'IncomeTaxExpenseBenefit', 'NetIncomeLoss', 'CostOfGoodsAndServicesSold', 'CashAndCashEquivalentsAtCarryingValue',
                 'CashCashEquivalentsAndShortTermInvestments', 'ResearchAndDevelopmentExpense', 'InventoryNet', 'AssetsCurrent',
                 'StockholdersEquity', 'Assets', 'LiabilitiesCurrent', 'EarningsPerShareBasic','EarningsPerShareDiluted', 'Liabilities']:
            
            df = pd.DataFrame(v['units'][list(v['units'].keys())[0]])

            temp_dict = {
                'label': v['label'],
                'description': v['description'],
                'unit': list(v['units'].keys())[0],
                'data': df
            }
            result['facts'][k] = temp_dict
    del response
    return result



### bulk data
Bulk Data Transfer
The SEC offers a bulk data download option for all of the data in both the Frame and Company Facts APIs. This data is updated nightly and can be accessed at http://www.sec.gov/Archives/edgar/daily-index/xbrl/companyfacts.zip. While all available data is provided (~14.5k files), only those companies in the S&P 500 are needed for this project. Due to the number of files that need to be selected and copied, it's necessary to create a script to automatically select the correct files.

The sp500_ciks.csv data was sourced from the "S&P 500 component stocks" table on the List of S&P 500 companies Wikipedia page.

import shutil
import pandas as pd
df = pd.read_csv('data/sp500_ciks.csv', dtype=str)
df.head()
Symbol	Security	SEC filings	GICS Sector	GICS Sub-Industry	Headquarters Location	Date first added	CIK	Founded
0	MMM	3M	reports	Industrials	Industrial Conglomerates	Saint Paul, Minnesota	1976-08-09	0000066740	1902
1	ABT	Abbott Laboratories	reports	Health Care	Health Care Equipment	North Chicago, Illinois	1964-03-31	0000001800	1888
2	ABBV	AbbVie	reports	Health Care	Pharmaceuticals	North Chicago, Illinois	2012-12-31	0001551152	2013 (1888)
3	ABMD	Abiomed	reports	Health Care	Health Care Equipment	Danvers, Massachusetts	2018-05-31	0000815094	1981
4	ACN	Accenture	reports	Information Technology	IT Consulting & Other Services	Dublin, Ireland	2011-07-06	0001467373	1989
src = 'E:/Downloads/sec_bulk_data'
dst = 'data/sec_bulk_data/'

for cik in df.CIK:
    try:
        shutil.copy(src + f'/CIK{cik}.json', dst)
    except FileNotFoundError as e:
        print('File not found for CIK:', cik, f'({df[df.CIK == cik].Security.values})')
File not found for CIK: 0001132979 (['First Republic Bank'])
I manually confirmed that the CIK for First Republic Bank is correct as shown above. However, the SEC's API shows that the data is missing when trying to access it through the website (https://data.sec.gov/api/xbrl/companyfacts/CIK0001132979.json). Therefore, it makes sense that the data would also be missing from the bulk data download. 499 / 500 companies were successfully found and copied.